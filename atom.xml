<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://p.guipulp.top/</id>
    <title>PeterPan</title>
    <updated>2019-09-12T04:58:28.538Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://p.guipulp.top/"/>
    <link rel="self" href="https://p.guipulp.top//atom.xml"/>
    <subtitle>贵有恒，何必三更起五更睡；最无益，只怕一日暴十寒。</subtitle>
    <logo>https://p.guipulp.top//images/avatar.png</logo>
    <icon>https://p.guipulp.top//favicon.ico</icon>
    <rights>All rights reserved 2019, PeterPan</rights>
    <entry>
        <title type="html"><![CDATA[使用kubeadm 新加入节点（原始token过期后）]]></title>
        <id>https://p.guipulp.top//post/2019091201</id>
        <link href="https://p.guipulp.top//post/2019091201">
        </link>
        <updated>2019-09-12T04:47:00.000Z</updated>
        <content type="html"><![CDATA[<h2 id="kubeadm-join">kubeadm join</h2>
<blockquote>
<p>kubeadm init 安装完成后你会得到以下的输出，使用join指令可以新增节点到集群,此token 有效期为24小时</p>
</blockquote>
<pre><code>You should now deploy a pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join 18.16.202.35:6443 --token zr8n5j.yfkanjio0lfsupc0 --discovery-token-ca-cert-hash sha256:380b775b7f9ea362d45e4400be92adc4f71d86793ba6aae091ddb53c489d218c
</code></pre>
<h2 id="kubeadm-token">kubeadm token</h2>
<blockquote>
<p>在新节点没有拿到证书以前，新节点和api server的通信是通过token和ca的签名完成的，具体的步骤如下</p>
</blockquote>
<pre><code># 生成token
[root@node1 flannel]# kubeadm  token create
kiyfhw.xiacqbch8o8fa8qj
[root@node1 flannel]# kubeadm  token list
TOKEN                     TTL         EXPIRES                     USAGES                   DESCRIPTION   EXTRA GROUPS
gvvqwk.hn56nlsgsv11mik6   &lt;invalid&gt;   2018-10-25T14:16:06+08:00   authentication,signing   &lt;none&gt;        system:bootstrappers:kubeadm:default-node-token
kiyfhw.xiacqbch8o8fa8qj   23h         2018-10-27T06:39:24+08:00   authentication,signing   &lt;none&gt;        system:bootstrappers:kubeadm:default-node-token
</code></pre>
<pre><code># 生成ca的sha256 hash值
[root@node1 flannel]# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
5417eb1b68bd4e7a4c82aded83abc55ec91bd601e45734d6aba85de8b1ebb057
</code></pre>
<pre><code># 组装join命令
kubeadm join 18.16.202.35:6443 --token kiyfhw.xiacqbch8o8fa8qj --discovery-token-ca-cert-hash sha256:5417eb1b68bd4e7a4c82aded83abc55ec91bd601e45734d6aba85de8b1ebb057
</code></pre>
<pre><code># 一步完成以上步骤
kubeadm token create --print-join-command
</code></pre>
<pre><code># 手动生成token，完成命令打印
token=$(kubeadm token generate)
kubeadm token create $token --print-join-command --ttl=0
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes1.15.3离线安装]]></title>
        <id>https://p.guipulp.top//post/2019090901</id>
        <link href="https://p.guipulp.top//post/2019090901">
        </link>
        <updated>2019-09-09T10:09:51.000Z</updated>
        <content type="html"><![CDATA[<h1 id="kubeadm-k8s集群离线安装">kubeadm k8s集群离线安装</h1>
<h2 id="下载软件包">下载软件包</h2>
<pre><code>yum install --downloadonly --downloaddir=./  kubeadm-1.15.3-0 kubelet-1.15.3-0 kubectl-1.15.3-0 docker-ce-18.06.3.ce-3.el7 keepalived ipset ipvsadm   
</code></pre>
<h2 id="下载镜像并打包">下载镜像并打包</h2>
<pre><code>kubeadm  config images pull --image-repository=registry.cn-hangzhou.aliyuncs.com/google_conta iners

for x in $(docker images |awk  '{print $1&quot;:&quot;$2}'|grep -v REP);do docker save -o $(echo $x|awk -F '/' '{print $3}').tar $x;done

</code></pre>
<h2 id="恢复镜像">恢复镜像</h2>
<pre><code>cd images;for x in `ls`;do docker load -i $x;done
</code></pre>
<h2 id="安装脚本">安装脚本</h2>
<p>软件包下载地址：</p>
<pre><code>链接:https://pan.baidu.com/s/1Vw0geL4Pn9_cq0rLxw7dRQ  密码:3v4g
</code></pre>
<pre><code># hostname
#hostnamectl set-hostname $HOSTNAME

# disable swap
swapoff -a

# disable selinux
sed -i &quot;s/SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config
setenforce 0

# stop and disable firewalld
systemctl stop firewalld ; systemctl disable firewalld

# add ipvs modules
cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4

# change node kernel
cat &lt;&lt;EOF &gt;  /etc/sysctl.d/ceph.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system

# install rpm
cd /root/local/rpm ; yum localinstall ./*

# load images
systemctl start docker 
systemctl enable docker kubelet
cd /root/local/images ; for x in `ls`;do docker load -i $x;done

# install kubernetes
# kubeadm init --apiserver-advertise-address 172.16.10.101 --pod-network-cidr=10.244.0.0/16 --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers
echo &quot;172.16.10.101 api.k8s.com&quot; &gt;&gt; /etc/hosts
kubeadm init --config /root/local/kubeadm.conf

# install client env
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# install addons
cd /root/local/yaml; kubectl apply -f .

# remove taint
#kubectl  taint node lab1 node-role.kubernetes.io/master:NoSchedule-
kubectl  get nodes |grep master |awk '{print $1}' |xargs -i kubectl  taint node {} node-role.kubernetes.io/master:NoSchedule-
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[etcd数据备份和恢复]]></title>
        <id>https://p.guipulp.top//post/2019090503</id>
        <link href="https://p.guipulp.top//post/2019090503">
        </link>
        <updated>2019-09-05T03:46:23.000Z</updated>
        <content type="html"><![CDATA[<h2 id="对于etcd-api-v3数据备份与恢复方法">对于etcd api v3数据备份与恢复方法</h2>
<pre><code> # export ETCDCTL_API=3
</code></pre>
<pre><code> # etcdctl --endpoints localhost:2379 snapshot save snapshot.db （备份）
</code></pre>
<pre><code> # etcdctl snapshot restore snapshot.db --name m3 --data-dir=/home/etcd_data （还原）
</code></pre>
<blockquote>
<p>恢复后的文件需要修改权限为 etcd:etcd<br>
–name:重新指定一个数据目录，可以不指定，默认为 default.etcd<br>
–data-dir：指定数据目录<br>
建议使用时不指定 name 但指定 data-dir，并将 data-dir 对应于 etcd 服务中配置的 data-dir</p>
</blockquote>
<h2 id="实践方法">实践方法</h2>
<p><strong>单机备份</strong></p>
<pre><code>[root@k8s-master1 ~]# etcdctl --endpoints 127.0.0.1:2379 snapshot save snashot.db
Snapshot saved at snashot.db
[root@k8s-master1 ~]# ll
-rw-r--r--   1 root root 3756064 Apr 18 10:38 snashot.db
[root@k8s-master1 ~]#
</code></pre>
<p><strong>集群备份</strong></p>
<pre><code>[root@k8s-master1 ~]# etcdctl --endpoints=&quot;https://192.168.32.129:2379,https://192.168.32.130:2379,192.168.32.128:2379&quot; --cacert=/etc/kubernetes/cert/ca.pem --key=/etc/etcd/cert/etcd-key.pem --cert=/etc/etcd/cert/etcd.pem  snapshot save snashot1.db
Snapshot saved at snashot1.db
[root@k8s-master1 ~]#
[root@k8s-master1 ~]# ll
-rw-r--r--   1 root root 3756064 Apr 18 10:53 snashot1.db
-rw-r--r--   1 root root 3756064 Apr 18 10:38 snashot.db
</code></pre>
<p><strong>数据恢复</strong></p>
<p>做下面的操作,请慎重,有可能造成集群崩溃数据丢失.请在实验环境测试.</p>
<p>执行命令:systemctl stop etcd<br>
所有节点的etcd服务全部停止.</p>
<p>执行命令:rm -rf /var/lib/etcd/<br>
所有节点删除etcd的数据</p>
<p><strong>恢复v3的数据</strong></p>
<pre><code>[root@k8s-master1 ~]#  etcdctl --name=k8s-master1 --endpoints=&quot;https://192.168.32.128:2379&quot; --cacert=/etc/kubernetes/cert/ca.pem --key=/etc/etcd/cert/etcd-key.pem --cert=/etc/etcd/cert/etcd.pem --initial-cluster-token=etcd-cluster-1 --initial-advertise-peer-urls=https://192.168.32.128:2380 --initial-cluster=k8s-master1=https://192.168.32.128:2380,k8s-master2=https://192.168.32.129:2380,k8s-master3=https://192.168.32.130:2380 --data-dir=/var/lib/etcd snapshot restore snashot1.db
2019-04-18 13:43:42.570882 I | mvcc: restore compact to 148651
2019-04-18 13:43:42.584194 I | etcdserver/membership: added member 4c99f52323a3e391 [https://192.168.32.129:2380] to cluster 2a0978507970d828
2019-04-18 13:43:42.584224 I | etcdserver/membership: added member 5a74b01f28ece933 [https://192.168.32.128:2380] to cluster 2a0978507970d828
2019-04-18 13:43:42.584234 I | etcdserver/membership: added member b29b94ace458096d [https://192.168.32.130:2380] to cluster 2a0978507970d828
</code></pre>
<pre><code>[root@k8s-master2 ~]# etcdctl --name=k8s-master2 --endpoints=&quot;https://192.168.32.129:2379&quot; --cacert=/etc/kubernetes/cert/ca.pem --key=/etc/etcd/cert/etcd-key.pem --cert=/etc/etcd/cert/etcd.pem --initial-cluster-token=etcd-cluster-1 --initial-advertise-peer-urls=https://192.168.32.129:2380 --initial-cluster=k8s-master1=https://192.168.32.128:2380,k8s-master2=https://192.168.32.129:2380,k8s-master3=https://192.168.32.130:2380 --data-dir=/var/lib/etcd snapshot restore snashot1.db
2019-04-18 13:43:56.313096 I | mvcc: restore compact to 148651
2019-04-18 13:43:56.324779 I | etcdserver/membership: added member 4c99f52323a3e391 [https://192.168.32.129:2380] to cluster 2a0978507970d828
2019-04-18 13:43:56.324806 I | etcdserver/membership: added member 5a74b01f28ece933 [https://192.168.32.128:2380] to cluster 2a0978507970d828
2019-04-18 13:43:56.324819 I | etcdserver/membership: added member b29b94ace458096d [https://192.168.32.130:2380] to cluster 2a0978507970d828
</code></pre>
<pre><code>[root@k8s-master3 ~]# etcdctl --name=k8s-master3 --endpoints=&quot;https://192.168.32.130:2379&quot; --cacert=/etc/kubernetes/cert/ca.pem --key=/etc/etcd/cert/etcd-key.pem --cert=/etc/etcd/cert/etcd.pem --initial-cluster-token=etcd-cluster-1 --initial-advertise-peer-urls=https://192.168.32.130:2380 --initial-cluster=k8s-master1=https://192.168.32.128:2380,k8s-master2=https://192.168.32.129:2380,k8s-master3=https://192.168.32.130:2380 --data-dir=/var/lib/etcd snapshot restore snashot1.db
2019-04-18 13:44:10.643115 I | mvcc: restore compact to 148651
2019-04-18 13:44:10.649920 I | etcdserver/membership: added member 4c99f52323a3e391 [https://192.168.32.129:2380] to cluster 2a0978507970d828
2019-04-18 13:44:10.649957 I | etcdserver/membership: added member 5a74b01f28ece933 [https://192.168.32.128:2380] to cluster 2a0978507970d828
2019-04-18 13:44:10.649973 I | etcdserver/membership: added member b29b94ace458096d [https://192.168.32.130:2380] to cluster 2a0978507970d828
</code></pre>
<p>服务起不来</p>
<pre><code>[root@k8s-master1 ~]# tail -n 30 /var/log/messages
Apr 18 13:46:41 k8s-master1 systemd: Starting Etcd Server...
Apr 18 13:46:41 k8s-master1 etcd: etcd Version: 3.3.7
Apr 18 13:46:41 k8s-master1 etcd: Git SHA: 56536de55
Apr 18 13:46:41 k8s-master1 etcd: Go Version: go1.9.6
Apr 18 13:46:41 k8s-master1 etcd: Go OS/Arch: linux/amd64
Apr 18 13:46:41 k8s-master1 etcd: setting maximum number of CPUs to 1, total number of available CPUs is 1
Apr 18 13:46:41 k8s-master1 etcd: error listing data dir: /var/lib/etcd
Apr 18 13:46:41 k8s-master1 systemd: etcd.service: main process exited, code=exited, status=1/FAILURE
Apr 18 13:46:41 k8s-master1 systemd: Failed to start Etcd Server.
Apr 18 13:46:41 k8s-master1 systemd: Unit etcd.service entered failed state.
Apr 18 13:46:41 k8s-master1 systemd: etcd.service failed.
Apr 18 13:46:41 k8s-master1 flanneld: timed out
Apr 18 13:46:41 k8s-master1 flanneld: E0418 13:46:41.858283   63943 main.go:349] Couldn't fetch network config: client: etcd cluster is unavailable or misconfigured; error #0: EOF
Apr 18 13:46:41 k8s-master1 flanneld: ; error #1: EOF
Apr 18 13:46:41 k8s-master1 flanneld: ; error #2: EOF
[root@k8s-master1 ~]#
</code></pre>
<p>修改数据目录权限,默认是root:root<br>
chown -R etcd:etcd /var/lib/etcd<br>
恢复正常.</p>
<pre><code>[root@k8s-master1 ~]# etcdctl member list
4c99f52323a3e391, started, k8s-master2, https://192.168.32.129:2380, https://192.168.32.129:2379
5a74b01f28ece933, started, k8s-master1, https://192.168.32.128:2380, https://192.168.32.128:2379
b29b94ace458096d, started, k8s-master3, https://192.168.32.130:2380, https://192.168.32.130:2379
[root@k8s-master1 ~]#
</code></pre>
<p>可以看到v3的数据恢复成功.</p>
<h2 id="参考地址">参考地址</h2>
<p>https://blog.csdn.net/liukuan73/article/details/78986652</p>
<p>https://blog.51cto.com/goome/2380854</p>
<p>https://yq.aliyun.com/articles/336781</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Centos7下的日志切割]]></title>
        <id>https://p.guipulp.top//post/2019090502</id>
        <link href="https://p.guipulp.top//post/2019090502">
        </link>
        <updated>2019-09-05T03:27:49.000Z</updated>
        <content type="html"><![CDATA[<h2 id="logrotate">logrotate</h2>
<p>/etc/logrotate.conf 是 Logrotate 工具的一个配置文件，这个工具用来自动切割系统日志，Logrotate 是基于 cron 来运行的，如下：</p>
<pre><code>[root@localhost ~]$ cat /etc/cron.daily/logrotate    # 每天运行
#!/bin/sh

/usr/sbin/logrotate /etc/logrotate.conf &gt;/dev/null 2&gt;&amp;1
EXITVALUE=$?
if [ $EXITVALUE != 0 ]; then
    /usr/bin/logger -t logrotate &quot;ALERT exited abnormally with [$EXITVALUE]&quot;
fi
exit 0
</code></pre>
<p>实际运行时，Logrotate 会调用配置文件 /etc/logrotate.conf ，默认的配置如下：</p>
<pre><code>[root@localhost ~]$ cat /etc/logrotate.conf 

weekly                      # 每周切割一次
rotate 4                    # 只保留四份文件
create                      # 切割后会创建一个新的文件
dateext                     # 指定切割文件的后缀名，这里以日期为后缀名
include /etc/logrotate.d    # 包含其他配置文件的目录

/var/log/wtmp {             # 对哪个文件进行切割
    monthly                 # 每个月切割一次
    create 0664 root utmp   # 指定创建的新文件的权限，属主，属组
        minsize 1M          # 文件容量超过这个值时才进行切割
    rotate 1                # 只保留一份文件
}

/var/log/btmp {
    missingok
    monthly
    create 0600 root utmp
    rotate 1
}
</code></pre>
<h2 id="参考链接">参考链接</h2>
<p>https://blog.51cto.com/linuxblind/1269458</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes里面的GC]]></title>
        <id>https://p.guipulp.top//post/2019090501</id>
        <link href="https://p.guipulp.top//post/2019090501">
        </link>
        <updated>2019-09-05T03:02:08.000Z</updated>
        <content type="html"><![CDATA[<h2 id="什么是gc">什么是GC</h2>
<p>GC 是 Garbage Collector 的简称。从功能层面上来说，它和编程语言当中的「GC」 基本上是一样的。它清理 Kubernetes 中「符合特定条件」的 Resource Object。</p>
<p>Kubelet的GC功能将清理未使用的image和container。Kubelet每分钟对container执行一次GC，每5分钟对image执行一次GC。不建议使用外部垃圾收集工具，因为这些工具可能破坏Kubelet。</p>
<h2 id="kubernetes里面的基本常识">kubernetes里面的基本常识</h2>
<ul>
<li>在 k8s 中，你可以认为万物皆资源，很多逻辑的操作对象都是 Resource Object。</li>
<li>Kubernetes 在不同的 Resource Objects 中维护一定的「从属关系」。内置的 Resource Objects 一般会默认在一个 Resource Object 和它的创建者之间建立一个「从属关系」。</li>
<li>你也可以利用<code>ObjectMeta.OwnerReferences</code>自由的去给两个 Resource Object 建立关系，前提是被建立关系的两个对象必须在一个 Namespace 下。</li>
<li>K8s 实现了一种「Cascading deletion」（级联删除）的机制，它利用已经建立的「从属关系」进行资源对象的清理工作。例如，当一个 dependent 资源的 owner 已经被删除或者不存在的时候，从某种角度就可以判定，这个 dependent 的对象已经是异常（无人管辖）的了，需要进行清理。而 「cascading deletion」则是被 k8s 中的一个 controller 组件实现的：<code>Garbage Collector</code></li>
<li>k8s 是通过 <code>Garbage Collector</code> 和 <code>ownerReference</code> 一起配合实现了「垃圾回收」的功能。</li>
</ul>
<h2 id="kubernetes的gc组成">kubernetes的gc组成</h2>
<p><img src="http://img.blog.csdn.net/20161225115013410?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvV2FsdG9uV2FuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="kubernetes GC architecture in v1.3"></p>
<p>一个 Garbage Collector 通常由三部分实现：</p>
<ul>
<li>
<p>Scanner： 它负责收集目前系统中已存在的 Resource，并且周期性的将这些资源对象放入一个队列中，等待处理（检测是否要对某一个Resource Object 进行 GC 操作）</p>
</li>
<li>
<p>Garbage Processor: Garbage Processor 由两部分组成</p>
</li>
<li>
<ul>
<li>
<p>Dirty Queue： Scanner 会将周期性扫描到的 Resource Object 放入这个队列中等待处理</p>
</li>
<li>
<p>Worker：worker 负责从这个队列中取出元素进行处理</p>
</li>
<li>
<ul>
<li>
<p>检查 Object 的 metaData 部分，查看<code>ownerReference</code>字段是否为空</p>
</li>
<li>
<ul>
<li>
<p>如果为空，则本次处理结束</p>
</li>
<li>
<p>如果不为空，检测<code>ownerReference</code>字段内标识的 Owner Resource Object是否存在</p>
</li>
<li>
<ul>
<li>存在：则本次处理结束</li>
<li>不存在：删除这个 Object</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Propagator： Propagator 由三个部分构成</p>
</li>
<li>
<ul>
<li>
<p>EventQueue：负责存储 k8s 中资源对象的事件（Eg：ADD，UPDATE，DELETE）</p>
</li>
<li>
<p>DAG(有向无环图)：负责存储 k8s 中所有资源对象的「owner-dependent」 关系</p>
</li>
<li>
<p>Worker：从 EventQueue 中，取出资源对象的事件，根据事件的类型会采取以下两种操作</p>
</li>
<li>
<ul>
<li>ADD/UPDATE: 将该事件对应的资源对象加入 DAG，且如果该对象有 owner 且 owner 不在 DAG 中，将它同时加入 Garbage Processor 的 Dirty Queue 中</li>
<li>DELETE：将该事件对应的资源对象从 DAG 中删除，并且将其「管辖」的对象（只向下寻找一级，如删除 Deployment，那么只操作 ReplicaSet ）加入 Garbage Processor 的 Dirty Queue 中</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>其实，在有了 Scanner 和 Garbage Processor 之后，Garbage Collector 就已经能够实现「垃圾回收」的功能了。但是有一个明显的问题：Scanner 的扫描频率设置多少好呢？太长了，k8s 内部就会积累过多的「废弃资源」；太短了，尤其是在集群内部资源对象较多的时候，频繁的拉取信息对 API-Server 也是一个不小的压力。</p>
<p>k8s 作为一个分布式的服务编排系统，其内部执行任何一项逻辑或者行为，都依赖一种机制：「事件驱动」。说的简单点，k8s 中一些看起来「自动」的行为，其实都是由一些神秘的「力量」在驱动着。而这个「力量」就是我们所说的「Event」。任意一个 Resource Object 发生变动的时候（新建，更新，删除），都会触发一个 k8s 的事件（Event），这个事件在 k8s 的内部是公开的，也就是说，我们可以在任意一个地方监听这些事件。</p>
<p>总的来说，无论是「事件的监听机制」还是「周期性访问 API-Server 批量获取 Resource Object 信息」，其目的都是为了能够掌握 Resource Object 的最新信息。两者是各有优势的：</p>
<ol>
<li>批量拉取：一次性拉取所有的 Resource Object，全面</li>
<li>监听 Resource 的 Event：实时性强， 且对 API—SERVER 不会造成太大的压力</li>
</ol>
<p>综上所述，在实现 Garbage Collector 的过程中，k8s 向其添加了一个「增强型」的组件：Propagator</p>
<p>在有了 Propagator 的加入之后，我们完全可以仅在 GC 开始运行的时候，让 Scanner 扫描一下系统中所有的 Object，然后将这些信息传递给 Propagator 和 Dirty Queue。只要 DAG 一建立起来之后，那么 Scanner 其实就没有再工作的必要了。「事件驱动」的机制提供了一种增量的方式让 GC 来监控 k8s 集群内部的资源对象变化情况。</p>
<h2 id="参考地址">参考地址</h2>
<p>https://mp.weixin.qq.com/s/6b5jdDkvmtywvcRa4MMjQA</p>
<p>https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go</p>
<p>https://yq.aliyun.com/articles/679728</p>
<p>https://zhuanlan.zhihu.com/p/50101300</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[linux cgroups简介]]></title>
        <id>https://p.guipulp.top//post/2019090202</id>
        <link href="https://p.guipulp.top//post/2019090202">
        </link>
        <updated>2019-09-02T07:54:27.000Z</updated>
        <content type="html"><![CDATA[<p>https://blog.csdn.net/ahilll/article/details/82109008<br>
https://blog.csdn.net/chenleiking/article/details/87988851</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes 清理孤儿POD]]></title>
        <id>https://p.guipulp.top//post/2019090201</id>
        <link href="https://p.guipulp.top//post/2019090201">
        </link>
        <updated>2019-09-02T07:53:25.000Z</updated>
        <content type="html"><![CDATA[<h3 id="孤儿pod的产生">孤儿pod的产生</h3>
<p>节点OOM以后或者节点异常崩溃的情况下，pod未能被正常的清理而导致的孤儿进程。</p>
<p><strong>提示如下</strong></p>
<pre><code>Orphaned pod found - but volume paths are still present on disk
</code></pre>
<p><strong>进入k8s的pod目录</strong></p>
<pre><code>cd /var/lib/kubelet/pods/
</code></pre>
<h3 id="解决问题">解决问题</h3>
<p>过滤日志中的孤儿pod，删除pod。</p>
<pre><code class="language-js">#!/bin/bash
num=$(grep &quot;errors similar to this. Turn up verbosity to see them.&quot;  /var/log/messages |tail -1 | awk '{print $12}' |sed 's/&quot;//g')
echo $num

while [ $num ]
do
   [ -d &quot;/var/lib/kubelet/pods/${num}&quot; ] &amp;&amp; rm -rf /var/lib/kubelet/pods/${num}

  sleep 2s
  num=$(grep &quot;errors similar to this. Turn up verbosity to see them.&quot;  /var/log/messages |tail -1 | awk '{print $12}' |sed 's/&quot;//g')
  [ -d &quot;/var/lib/kubelet/pods/${num}&quot; ] || num=

  echo &quot;$num remaining&quot;

done
</code></pre>
<p>但是这个方法有一定的<code>危险性</code>，还不确认是否有<code>数据丢失</code>的风险，如果可以确认，再执行。</p>
<p>如果已经挂载了PVC等相关存储，先执行umount再执行删除。</p>
<p>再去查看日志，就会发现syslog不会再刷类似的日志了。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes资源预留]]></title>
        <id>https://p.guipulp.top//post/2019083002</id>
        <link href="https://p.guipulp.top//post/2019083002">
        </link>
        <updated>2019-08-30T09:25:37.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>下面内容还处于测试阶段，生产上是否能保证集群稳定暂时还不清楚。😁😁</p>
</blockquote>
<h2 id="事故">事故</h2>
<p>今天我们的开发环境由于java应用内存抢占原因导致k8s集群worker节点全部宕机，主要原因是程序和资源没进行限制规划，且kubelet也没配置资源预留，那host上所有资源都是可以给pod调配使用的，这样就引起集群雪崩效应，比如集群内有一台上跑的pod没做resource limt导致占用资源过大导致将宿主机压死了，此时这个节点在kubernetes内就是一个no ready的状态了，kubernetes会将这台host上所有的pod在其他节点上重建，也就意味着那个有问题的pod重新跑在其他正常的节点上，将另外正常的节点压跨。循怀下去直到集群内所有主机都挂了，这就是集群雪崩效应。</p>
<h2 id="解决办法">解决办法</h2>
<p>在kubernetes中可以通过给kubelet配置参数预留资源给系统进程和kubernetes进程保证它们稳定运行。目前能实现到cpu、memory、ephemeral-storage层面的资源预留。<br>
重点提两点<br>
cpu：cpu是配置cpu shares实际上对应的是cpu的优先级，简单来说，这个在cpu繁忙时，它能有更高优先级获取更多cpu资源。</p>
<p>ephemeral-storage是kubernetes1.8开始引入的一个资源限制的对象，kubernetes 1.10版本中kubelet默认已经打开的了,主要是用于对本地临时存储使用空间大小的限制，如对pod的empty dir、/var/lib/kubelet、日志、容器可读写层的使用大小的限制。</p>
<h2 id="配置">配置</h2>
<h3 id="基本概念">基本概念</h3>
<p>在讲配置之前我们先了解几个概念：</p>
<p>Node capacity：节点总共的资源<br>
kube-reserved：给kubernetes进程预留的资源<br>
system-reserved：给操作系统预留的资源<br>
eviction-threshold：kubelet eviction的阀值<br>
allocatable：留给pod使用的资源</p>
<pre><code>node_allocatable=Node_capacity-(kube-reserved+system-reserved+hard-eviction)
</code></pre>
<pre><code>      Node Capacity
---------------------------
|     kube-reserved       |
|-------------------------|
|     system-reserved     |
|-------------------------|
|    eviction-threshold   |
|-------------------------|
|                         |
|      allocatable        |
|   (available for pods)  |
|                         |
|                         |
---------------------------
</code></pre>
<p>Kubernetes 节点上的 <code>Allocatable</code> 被定义为 pod 可用计算资源量。调度器不会超额申请 <code>Allocatable</code>。目前支持 <code>CPU</code>, <code>memory</code> 和 <code>storage</code> 这几个参数。</p>
<p>Node Allocatable 暴露为 API 中 <code>v1.Node</code> 对象的一部分，也是 CLI 中 <code>kubectl describe node</code> 的一部分。</p>
<p>在 <code>kubelet</code> 中，可以为两类系统守护进程预留资源。</p>
<h3 id="使用kubelet参数进行限制">使用kubelet参数进行限制</h3>
<p>此方法适用于老版本的kubernetes集群，在新版本(1.11之前)中已经不适用了。</p>
<p>https://k8smeetup.github.io/docs/tasks/administer-cluster/reserve-compute-resources/</p>
<p>https://www.bladewan.com/2018/01/26/k8s_resource_resver/</p>
<h3 id="使用kubelet-config进行限制">使用kubelet config进行限制</h3>
<p>kubelet 较新的版本都采用kubelet config对集群的kubelet进行配置，此处采用静态配置方式，当然也可以使用动态配置方式。</p>
<pre><code># 编辑文档kubelet config 文件
vim /var/lib/kubelet/config
</code></pre>
<p><strong>配置资源预留</strong></p>
<pre><code># 找到enforceNodeAllocatable 注释掉
#enforceNodeAllocatable:
#- pods

# 添加以下内容，系统和kubelet均预留CPU500m内存500Mi磁盘5G
systemReserved:
  cpu: &quot;500m&quot;
  memory: &quot;500Mi&quot;
  ephemeral-storage: &quot;5Gi&quot;
kubeReserved:
  cpu: &quot;500m&quot;
  memory: &quot;500Mi&quot;
  ephemeral-storage: &quot;5Gi&quot;
systemReservedCgroup: /system.slice
kubeReservedCgroup: /kubelet.service
EnforceNodeAllocatable:
- pods
- kube-reserved
- system-reserved

</code></pre>
<p><strong>配置软驱逐（默认为硬驱逐）</strong></p>
<p>软驱逐有资源驱逐等待时间，硬驱逐为立刻驱逐。</p>
<pre><code># evictionHard 注释掉，并在后面新增以下内容

#evictionHard:
EvictionSoft:
  imagefs.available: 15%
  memory.available: 10%
  nodefs.available: 10%
  nodefs.inodesFree: 5%
EvictionSoftGracePeriod:
  memory.available: &quot;5m&quot;
  nodefs.available: &quot;2m&quot;
  nodefs.inodesFree: &quot;2m&quot;
  imagefs.available: &quot;2m&quot;
</code></pre>
<pre><code># 如果你使用的cgroup driver是croup还需要进行以下操作
# cpuset和hugetlb subsystem是默认没有初始化system.slice手动创建,
mkdir -p /sys/fs/cgroup/hugetlb/system.slice/kubelet.service/
mkdir -p /sys/fs/cgroup/cpuset/system.slice/kubelet.service/

# 配置在kubelet中避免重启失效
ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/cpuset/system.slice/kubelet.service
ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/hugetlb/system.slice/kubelet.service
</code></pre>
<p><strong>重启kubelet</strong></p>
<pre><code>service kubelet restart
</code></pre>
<h2 id="验证">验证</h2>
<pre><code>[root@m3 pki]# kubectl  describe node m1
Name:               m1
Roles:              master
Labels:             beta.kubernetes.io/arch=amd64
....
CreationTimestamp:  Mon, 26 Aug 2019 20:35:35 -0400
...
Addresses:
  InternalIP:  172.27.100.13
  Hostname:    m1
Capacity:
 cpu:                4
 ephemeral-storage:  36678148Ki
 hugepages-1Gi:      0
 hugepages-2Mi:      0
 memory:             8010576Ki
 pods:               110
Allocatable:
 cpu:                3
 ephemeral-storage:  23065162901
 hugepages-1Gi:      0
 hugepages-2Mi:      0
 memory:             6884176Ki
 pods:               110
</code></pre>
<ul>
<li>可以看到预留后，可用CPU为3，不预留为4，内存是一样的计算方式，此处预留了1G（500Mi+500Mi）</li>
</ul>
<h2 id="参考文档">参考文档</h2>
<p>https://github.com/rootsongjc/qa/issues/3</p>
<p>https://cloud.tencent.com/developer/article/1097002</p>
<p>https://blog.csdn.net/ahilll/article/details/82109008</p>
<p>http://dockone.io/article/4797</p>
<p>https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/</p>
<p>https://kubernetes.io/zh/docs/tasks/administer-cluster/kubelet-config-file/</p>
<p>https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go</p>
<p>https://www.bladewan.com/2018/01/26/k8s_resource_resver/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[修改kubernetes证书过期时间的骚操作]]></title>
        <id>https://p.guipulp.top//post/2019083001</id>
        <link href="https://p.guipulp.top//post/2019083001">
        </link>
        <updated>2019-08-30T08:50:14.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>简单验证（随机修改系统时间）是有效的（10年内），具体用起来怎样，自己去测试吧。</p>
</blockquote>
<p>最近公司早期部署的kubernetes的集群证书过期，研究了下kubernetes的证书时间，大致上是说修改kubeadm的源代码，自己编译kubeadm，实现证书99年。</p>
<p>kubernetes默认CA证书10年，很少有企业的一个服务能跑10年不更换，因此想到以下方法进行了以下实验。测试暂时没发现什么问题。</p>
<p>如果有多master，需要在每个master上进行以下操作。</p>
<pre><code>[root@m2 pki]# date -s 8/30/2028
2028年 08月 30日 星期三 00:00:00 EDT
[root@m2 pki]# kubectl get pods
Unable to connect to the server: x509: certificate has expired or is not yet valid
[root@m2 pki]# kubectl get pods
Unable to connect to the server: x509: certificate has expired or is not yet valid
[root@m2 pki]# kubeadm alpha certs renew all
certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed
certificate for serving the Kubernetes API renewed
certificate the apiserver uses to access etcd renewed
certificate for the API server to connect to kubelet renewed
certificate embedded in the kubeconfig file for the controller manager to use renewed
certificate for liveness probes to healtcheck etcd renewed
certificate for etcd nodes to communicate with each other renewed
certificate for serving etcd renewed
certificate for the front proxy client renewed
certificate embedded in the kubeconfig file for the scheduler manager to use renewed
[root@m2 pki]# ntpdate 0.cn.pool.ntp.org
30 Aug 04:51:35 ntpdate[9983]: step time server 203.107.6.88 offset -284065763.328437 sec
[root@m2 pki]# cp /etc/kubernetes/admin.conf ~/.kube/config
cp：是否覆盖&quot;/root/.kube/config&quot;？ y
[root@m2 pki]# openssl x509 -in apiserver.crt -noout -text
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number: 6495439138002669247 (0x5a246f7b50d18ebf)
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: CN=kubernetes
        Validity
            Not Before: Aug 27 00:34:59 2019 GMT
            Not After : Aug 30 04:00:30 2029 GMT
        Subject: CN=kube-apiserver
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (2048 bit)
                Modulus:
                    00:d5:c5:1a:60:e3:dd:5c:68:c6:a0:38:df:06:54:
                    7b:87:73:3f:6d:1b:bc:65:4e:8e:1a:e0:a7:13:a9:
                    df:a6:67:65:13:cb:c6:e7:c8:5e:60:3e:14:7d:c4:
                    e0:91:d5:de:8b:91:bd:9c:59:2b:35:87:62:87:c9:
                    97:c4:f6:c6:41:b4:6a:80:25:34:1a:d2:b4:ad:e0:
                    bc:d0:af:18:35:68:b9:46:7b:1e:af:da:24:ce:79:
                    94:75:77:3d:ae:67:ac:b9:08:5a:74:87:fd:c0:33:
                    ec:d9:d5:ba:71:ee:23:9e:39:69:fa:2f:76:3f:e1:
                    80:a4:89:a3:39:40:f9:ef:a6:f5:4c:27:3a:7c:60:
                    aa:83:ce:cf:48:2a:e9:6c:15:88:21:8b:fb:53:f8:
                    05:15:bf:35:10:9d:4e:63:f9:09:ae:9c:45:b2:75:
                    4f:8f:4e:a9:9f:bc:f8:c0:9f:29:bc:5d:a3:8f:a9:
                    c7:d1:ea:85:d4:79:32:e1:e3:b7:2f:08:eb:47:99:
                    af:db:b4:5d:a7:d9:c0:7f:29:63:ec:f4:bb:ba:9b:
                    16:f4:c1:11:28:37:7c:c8:f9:13:7b:38:86:83:ba:
                    f7:e8:dd:fd:05:9e:7c:46:4a:1e:d9:9e:8e:0c:ad:
                    86:09:70:da:40:46:e7:e8:5c:82:26:08:3a:4e:ad:
                    8a:a3
                Exponent: 65537 (0x10001)
        X509v3 extensions:
            X509v3 Key Usage: critical
                Digital Signature, Key Encipherment
            X509v3 Extended Key Usage:
                TLS Web Server Authentication
            X509v3 Subject Alternative Name:
                DNS:m2, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, IP Address:10.96.0.1, IP Address:172.27.100.14, IP Address:172.27.100.211, IP Address:172.29.100.211
    Signature Algorithm: sha256WithRSAEncryption
         ab:31:c7:2a:a2:52:40:e2:b5:c6:a4:fc:d7:f5:35:2c:c9:28:
         8d:ba:6a:1d:c3:06:d0:85:4e:bf:43:0c:27:a2:2f:13:62:97:
         89:cc:4e:0c:17:86:11:ef:21:2a:01:9d:1c:25:f2:48:c5:31:
         95:63:41:2c:57:2a:1c:1f:40:00:2f:1a:b1:90:7b:3d:6a:2b:
         7c:6f:33:9a:c8:09:44:4d:a4:ff:fa:06:de:77:66:e1:0f:59:
         29:bd:76:d1:52:b9:63:81:b4:ae:27:0f:ed:54:93:b1:21:8b:
         bf:4f:16:9e:b2:4d:dd:2f:ac:81:e8:86:38:c8:2d:d9:38:1b:
         ac:56:23:5e:8c:dc:96:ae:24:3f:dd:7f:ce:ba:97:b5:e7:07:
         05:df:f2:fd:ba:51:6d:94:13:c5:2d:a8:75:32:21:ad:e9:07:
         13:04:64:e9:c5:f9:3c:46:39:9a:16:59:3b:ff:91:af:7b:fb:
         14:6d:2e:66:2d:52:0d:1b:45:ba:e4:0c:42:23:1c:2c:ea:53:
         6a:20:88:87:24:b1:55:39:d0:93:7b:44:46:bc:46:16:37:d6:
         ff:e6:2f:dd:05:53:03:bc:d6:ba:32:1c:8a:7c:74:4b:cc:4d:
         3b:20:b6:80:a0:a8:28:a5:da:fb:0b:3a:43:c3:b7:79:86:a8:
         2e:cc:91:48

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[iptables的四表五链]]></title>
        <id>https://p.guipulp.top//post/2019082802</id>
        <link href="https://p.guipulp.top//post/2019082802">
        </link>
        <updated>2019-08-28T04:44:56.000Z</updated>
        <content type="html"><![CDATA[<h2 id="netfilter介绍">netfilter介绍</h2>
<ul>
<li>它是iptables的主要的工作模块，位于内核中，在网络层的五个位置（也就是防火墙四表五链中的五链）注册了一些钩子函数，用来抓取数据包；</li>
<li>把数据包的信息拿出来匹配各个链位置在对应表中的规则；</li>
<li>匹配之后，进行相应的处理ACCEPT、DROP等等；</li>
</ul>
<h3 id="netfilter和iptables的关系图">netfilter和iptables的关系图</h3>
<p><img src="https://img.mubu.com/document_image/e0653c59-e432-40d9-a8bd-38558599a4da-1235096.jpg" alt="img"></p>
<h2 id="四表五链">四表五链</h2>
<ul>
<li>链就是位置：共有五个，进路由(PREROUTING)、进系统(INPUT) 、转发(FORWARD)、出系统(OUTPUT)、出路由(POSTROUTING)；</li>
<li>表就是存储的规则；数据包到了该链处，会去对应表中查询设置的规则，然后决定是否放行、丢弃、转发还是修改等等操作。</li>
</ul>
<h3 id="四表">四表</h3>
<ul>
<li>filter表——过滤数据包</li>
<li>Nat表——用于网络地址转换（IP、端口）</li>
<li>Mangle表——修改数据包的服务类型、TTL、并且可以配置路由实现QOS</li>
<li>Raw表——决定数据包是否被状态跟踪机制处理</li>
</ul>
<h3 id="五链">五链</h3>
<ul>
<li>INPUT链——进来的数据包应用此规则链中的策略</li>
<li>OUTPUT链——外出的数据包应用此规则链中的策略</li>
<li>FORWARD链——转发数据包时应用此规则链中的策略</li>
<li>PREROUTING链——对数据包作路由选择前应用此链中的规则（所有的数据包进来的时侯都先由这个链处理）</li>
<li>POSTROUTING链——对数据包作路由选择后应用此链中的规则（所有的数据包出来的时侯都先由这个链处理）</li>
</ul>
<h3 id="链表关系图">链表关系图</h3>
<p><img src="https://img2018.cnblogs.com/blog/1479220/201809/1479220-20180926150504012-1967719887.png" alt="image-20190828125115128"></p>
<h3 id="数据处理流程">数据处理流程</h3>
<p>我们结合下面的图举个例子：<br>
假如有数据包从Network IN要通过iptables，数据包流向如下：<br>
1.Network IN数据包到达服务器的网络接口<br>
2.进入raw表的 PREROUTING 链，这个链的作用是决定数据包是否被状态跟踪。<br>
进入 mangle 表的 PREROUTING 链，在此可以修改数据包，比如 TOS 等。<br>
进入 nat 表的 PREROUTING 链，可以在此做DNAT(目标地址转换)<br>
3.决定路由，查看目标地址是交给本地主机还是转发给其它主机。</p>
<p>4.到这里分两种情况，一种情况是数据包要转发给其它主机（一般情况下它是在担任网关服务器），数据包会依次经过：<br>
5.进入 mangle 表的 FORWARD 链，<br>
进入 filter 表的 FORWARD 链，在这里我们可以对所有转发的数据包进行过滤。<br>
6.进入 mangle 表的 POSTROUTING 链<br>
进入 nat 表的 POSTROUTING 链，在这里一般都是用来做 SNAT （源地址转换）<br>
7.数据包流出网络接口，发往network out。</p>
<p>8.另一种情况，数据包的目标地址就是发给本地主机的，它会依次穿过：<br>
9.进入 mangle 表的 INPUT 链，<br>
进入 filter 表的 INPUT 链，在这里我们可以对流入的所有数据包进行过滤，<br>
数据包交给本地主机的应用程序进行处理。<br>
10.应用程序处理完毕后发送的数据包进行路由发送决定。<br>
11.进入 raw 表的 OUTPUT 链。<br>
进入 mangle 表的 OUTPUT 链，<br>
进入 nat 表的 OUTPUT 链，<br>
进入 filter 表的 OUTPUT 链。<br>
12.进入 mangle 表的 POSTROUTING 链，<br>
进入 nat 表的 POSTROUTING 链。<br>
13.进入出去的网络接口，发送往network out。</p>
<p>所以，如果我们要过滤控制包的进出，只需要把input chain和forward chain这两个关口把控住就好了，这也是我们需要把input chain和forward chain默认设置为drop的原因。</p>
<ul>
<li>
<p>图一</p>
<p><img src="https://img.mubu.com/document_image/c1b9ef44-0c8d-4818-a18b-83b527e8d0e4-1235096.jpg" alt="img"></p>
</li>
<li>
<p>图二</p>
<p><img src="https://img.mubu.com/document_image/bcb89f30-1a57-4967-9a41-3cb6e2c8a5bc-1235096.jpg" alt="img"></p>
</li>
</ul>
<h2 id="iptables操作">iptables操作</h2>
<pre><code>iptables [-t 表名] 选项 [链名] [条件] [-j 控制类型]

-P 设置默认策略:iptables
 
-P INPUT (DROP|ACCEPT)
 
-F 清空规则链
 
-L 查看规则链
 
-A 在规则链的末尾加入新规则
 
-I num 在规则链的头部加入新规则
 
-D num 删除某一条规则
 
-s 匹配来源地址IP/MASK，加叹号&quot;!&quot;表示除这个IP外。
 
-d 匹配目标地址
 
-i 网卡名称 匹配从这块网卡流入的数据
 
-o 网卡名称 匹配从这块网卡流出的数据
 
-p 匹配协议,如tcp,udp,icmp
 
--dport num 匹配目标端口号
 
--sport num 匹配来源端口号
</code></pre>
<h3 id="常用iptables的语句">常用iptables的语句</h3>
<pre><code>1. 删除已有规则
在开始创建iptables规则之前，你也许需要删除已有规则。命令如下：
iptables -F
(or)
iptables –flush
查看已有规则
iptables -nL
 
 
2.设置链的默认策略
链的默认政策设置为”ACCEPT”（接受），若要将INPUT,FORWARD,OUTPUT链设置成”DROP”（拒绝），命令如下：
iptables -P INPUT DROP
iptables -P FORWARD DROP
iptables -P OUTPUT DROP
当INPUT链和OUTPUT链都设置成DROP时，对于每一个防火墙规则，我们都应该定义两个规则。例如：一个传入另一个传出。在下面所有的例子中，由于我们已将DROP设置成INPUT链和OUTPUT链的默认策略，每种情况我们都将制定两条规则。
当然，如果你相信你的内部用户,则可以省略上面的最后一行。例如：默认不丢弃所有出站的数据包。在这种情况下,对于每一个防火墙规则要求,你只需要制定一个规则——只对进站的数据包制定规则。
 
 
3. 阻止指定IP地址
例：丢弃来自IP地址x.x.x.x的包
BLOCK_THIS_IP=&quot;x.x.x.x&quot;
iptables -A INPUT -s &quot;$BLOCK_THIS_IP&quot; -j DROP
注：当你在log里发现来自某ip地址的异常记录，可以通过此命令暂时阻止该地址的访问以做更深入分析
例：阻止来自IP地址x.x.x.x eth0 tcp的包
iptables -A INPUT -i eth0 -s &quot;$BLOCK_THIS_IP&quot; -j DROP
iptables -A INPUT -i eth0 -p tcp -s &quot;$BLOCK_THIS_IP&quot; -j DROP
 
 
4. 允许所有SSH的连接请求
例：允许所有来自外部的SSH连接请求，即只允许进入eth0接口，并且目标端口为22的数据包
iptables -A INPUT -i eth0 -p tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT
 
 
5. 仅允许来自指定网络的SSH连接请求
例：仅允许来自于192.168.100.0/24域的用户的ssh连接请求
iptables -A INPUT -i eth0 -p tcp -s 192.168.100.0/24 --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT
 
 
6.允许http和https的连接请求
例：允许所有来自web - http的连接请求
iptables -A INPUT -i eth0 -p tcp --dport 80 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 80 -m state --state ESTABLISHED -j ACCEPT
例：允许所有来自web - https的连接请求
iptables -A INPUT -i eth0 -p tcp --dport 443 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 443 -m state --state ESTABLISHED -j ACCEPT
 
 
7. 使用multiport 将多个规则结合在一起
允许多个端口从外界连入，除了为每个端口都写一条独立的规则外，我们可以用multiport将其组合成一条规则。如下所示：
例：允许所有ssh,http,https的流量访问
iptables -A INPUT -i eth0 -p tcp -m multiport --dports 22,80,443 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp -m multiport --sports 22,80,443 -m state --state ESTABLISHED -j ACCEPT
 
 
8. 允许从本地发起的SSH请求
iptables -A OUTPUT -o eth0 -p tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A INPUT -i eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT
请注意,这与允许ssh连入的规则略有不同。本例在OUTPUT链上，我们允许NEW和ESTABLISHED状态。在INPUT链上，我们只允许ESTABLISHED状态。ssh连入的规则与之相反。
 
 
9. 仅允许从本地发起到一个指定的网络域的SSH请求
例：仅允许从内部连接到网域192.168.100.0/24
iptables -A OUTPUT -o eth0 -p tcp -d 192.168.100.0/24 --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A INPUT -i eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT
 
 
10. 允许从本地发起的HTTPS连接请求
下面的规则允许输出安全的网络流量。如果你想允许用户访问互联网，这是非常有必要的。在服务器上，这些规则能让你使用wget从外部下载一些文件
iptables -A OUTPUT -o eth0 -p tcp --dport 443 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A INPUT -i eth0 -p tcp --sport 443 -m state --state ESTABLISHED -j ACCEPT
注：对于HTTP web流量的外联请求，只需要将上述命令中的端口从443改成80即可。
 
 
11. 负载平衡传入的网络流量
使用iptables可以实现传入web流量的负载均衡，我们可以传入web流量负载平衡使用iptables防火墙规则。
例：使用iptables nth将HTTPS流量负载平衡至三个不同的ip地址。
iptables -A PREROUTING -i eth0 -p tcp --dport 443 -m state --state NEW -m nth --counter 0 --every 3 --packet 0 -j DNAT --to-destination 192.168.1.101:443
iptables -A PREROUTING -i eth0 -p tcp --dport 443 -m state --state NEW -m nth --counter 0 --every 3 --packet 1 -j DNAT --to-destination 192.168.1.102:443
iptables -A PREROUTING -i eth0 -p tcp --dport 443 -m state --state NEW -m nth --counter 0 --every 3 --packet 2 -j DNAT --to-destination 192.168.1.103:443
 
 
12. 允许外部主机ping内部主机
iptables -A INPUT -p icmp --icmp-type echo-request -j ACCEPT
iptables -A OUTPUT -p icmp --icmp-type echo-reply -j ACCEPT
 
 
13. 允许内部主机ping外部主机
iptables -A OUTPUT -p icmp --icmp-type echo-request -j ACCEPT
iptables -A INPUT -p icmp --icmp-type echo-reply -j ACCEPT
 
 
14. 允许回环访问 例：在服务器上允许127.0.0.1回环访问。
iptables -A INPUT -i lo -j ACCEPT
iptables -A OUTPUT -o lo -j ACCEPT
 
 
15. 允许内部网络域外部网络的通信
防火墙服务器上的其中一个网卡连接到外部，另一个网卡连接到内部服务器，使用以下规则允许内部网络与外部网络的通信。此例中，eth1连接到外部网络(互联网)，eth0连接到内部网络(例如:192.168.1.x)。
iptables -A FORWARD -i eth0 -o eth1 -j ACCEPT
 
 
16. 允许出站的DNS连接
iptables -A OUTPUT -p udp -o eth0 --dport 53 -j ACCEPT
iptables -A INPUT -p udp -i eth0 --sport 53 -j ACCEPT
 
 
17. 允许NIS连接
如果你使用NIS管理用户帐户，你需要允许NIS连接。如果你不允许NIS相关的ypbind连接请求，即使SSH连接请求已被允许，用户仍然无法登录。NIS的端口是动态的，先使用命令rpcinfo –p来知道端口号，此例中为853和850端口。
rpcinfo -p | grep ypbind
例：允许来自111端口以及ypbind使用端口的连接请求
iptables -A INPUT -p tcp --dport 111 -j ACCEPT
iptables -A INPUT -p udp --dport 111 -j ACCEPT
iptables -A INPUT -p tcp --dport 853 -j ACCEPT
iptables -A INPUT -p udp --dport 853 -j ACCEPT
iptables -A INPUT -p tcp --dport 850 -j ACCEPT
iptables -A INPUT -p udp --dport 850 -j ACCEPT
注：当你重启ypbind之后端口将不同，上述命令将无效。有两种解决方案：
1）使用你NIS的静态IP
2）编写shell脚本通过“rpcinfo - p”命令自动获取动态端口号,并在上述iptables规则中使用。
 
 
18. 允许来自指定网络的rsync连接请求
例：允许来自网络192.168.101.0/24的rsync连接请求
iptables -A INPUT -i eth0 -p tcp -s 192.168.101.0/24 --dport 873 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 873 -m state --state ESTABLISHED -j ACCEPT
 
 
19. 允许来自指定网络的MySQL连接请求
很多情况下，MySQL数据库与web服务跑在同一台服务器上。有时候我们仅希望DBA和开发人员从内部网络（192.168.100.0/24）直接登录数据库，可尝试以下命令：
iptables -A INPUT -i eth0 -p tcp -s 192.168.100.0/24 --dport 3306 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 3306 -m state --state ESTABLISHED -j ACCEPT
 
 
20. 允许Sendmail, Postfix邮件服务
Sendmail和postfix都使用了25端口，因此我们只需要允许来自25端口的连接请求即可。
iptables -A INPUT -i eth0 -p tcp --dport 25 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 25 -m state --state ESTABLISHED -j ACCEPT
 
 
21. 允许IMAP和IMAPS 例：允许IMAP/IMAP2流量，端口为143
iptables -A INPUT -i eth0 -p tcp --dport 143 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 143 -m state --state ESTABLISHED -j ACCEPT
例：允许IMAPS流量，端口为993
iptables -A INPUT -i eth0 -p tcp --dport 993 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 993 -m state --state ESTABLISHED -j ACCEPT
 
 
22. 允许POP3和POP3S 例：允许POP3访问
iptables -A INPUT -i eth0 -p tcp --dport 110 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 110 -m state --state ESTABLISHED -j ACCEPT
例：允许POP3S访问
iptables -A INPUT -i eth0 -p tcp --dport 995 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 995 -m state --state ESTABLISHED -j ACCEPT
 
 
23. 防止DoS攻击
iptables -A INPUT -p tcp --dport 80 -m limit --limit 25/minute --limit-burst 100 -j ACCEPT
上述例子中： -m limit: 启用limit扩展 –limit 25/minute: 允许最多每分钟25个连接（根据需求更改）。 –limit-burst 100: 只有当连接达到limit-burst水平(此例为100)时才启用上述limit/minute限制。
 
 
24. 端口转发 例：将来自422端口的流量全部转到22端口。
这意味着我们既能通过422端口又能通过22端口进行ssh连接。启用DNAT转发。
iptables -t nat -A PREROUTING -p tcp -d 192.168.102.37 --dport 422 -j DNAT --to 192.168.102.37:22
除此之外，还需要允许连接到422端口的请求
iptables -A INPUT -i eth0 -p tcp --dport 422 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 422 -m state --state ESTABLISHED -j ACCEPT
 
 
25. 记录丢弃的数据表 第一步：新建名为LOGGING的链
iptables -N LOGGING
第二步：将所有来自INPUT链中的数据包跳转到LOGGING链中
iptables -A INPUT -j LOGGING
第三步：为这些包自定义个前缀，命名为”IPTables Packet Dropped”
iptables -A LOGGING -m limit --limit 2/min -j LOG --log-prefix &quot;IPTables Packet Dropped: &quot; --log-level 7
第四步：丢弃这些数据包
iptables -A LOGGING -j DROP
 
 
26. ip映射(NAT)
假设有一家ISP提供园区Internet接入服务，为了方便管理，该ISP分配给园区用户的IP地址都是伪IP，但是部分用户要求建立自己的WWW服务器对外发布信息。
我们可以再防火墙的外部网卡上绑定多个合法IP地址，然后通过ip映射使发给其中某一 个IP地址的包转发至内部某一用户的WWW服务器上，然后再将该内部WWW服务器响应包伪装成该合法IP发出的包。
 
我们假设以下情景:
该ISP分配给A单位www服务器的ip为:
伪ip:192.168.1.100
真实ip:202.110.123.100
该ISP分配给B单位www服务器的ip为:
伪ip:192.168.1.200
真实ip:202.110.123.200
linux防火墙的ip地址分别为:
内网接口eth1:192.168.1.1
外网接口eth0:202.110.123.1
然后我们将分配给A、B单位的真实ip绑定到防火墙的外网接口，以root权限执行以下命令:
ifconfig eth0 add 202.110.123.100 netmask 255.255.255.0
ifconfig eth0 add 202.110.123.200 netmask 255.255.255.0
 
首先，对防火墙接收到的目的ip为202.110.123.100和202.110.123.200的所有数据包进行目的NAT(DNAT):
iptables -A PREROUTING -i eth0 -d 202.110.123.100 -j DNAT --to 192.168.1.100
iptables -A PREROUTING -i eth0 -d 202.110.123.200 -j DNAT --to 192.168.1.200
 
其次，对防火墙接收到的源ip地址为192.168.1.100和192.168.1.200的数据包进行源NAT(SNAT):
iptables -A POSTROUTING -o eth0 -s 192.168.1.100 -j SNAT --to 202.110.123.100
iptables -A POSTROUTING -o eth0 -s 192.168.1.200 -j SNAT --to 202.110.123.200
 
这样，所有目的ip为202.110.123.100和202.110.123.200的数据包都将分别被转发给192.168.1.100和192.168.1.200;而所有来自192.168.1.100和192.168.1.200的数据包都将分 别被伪装成由202.110.123.100和202.110.123.200，从而也就实现了ip映射。
</code></pre>
<h2 id="参考文档">参考文档</h2>
<p>https://www.cnblogs.com/zejin2008/p/5919550.html<br>
https://blog.csdn.net/skc361/article/details/20481521<br>
https://www.cnblogs.com/zhujingzhi/p/9706664.html</p>
]]></content>
    </entry>
</feed>