<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://p.guipulp.top/</id>
    <title>PeterPan</title>
    <updated>2019-08-28T05:03:39.715Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://p.guipulp.top/"/>
    <link rel="self" href="https://p.guipulp.top//atom.xml"/>
    <subtitle>贵有恒，何必三更起五更睡；最无益，只怕一日暴十寒。</subtitle>
    <logo>https://p.guipulp.top//images/avatar.png</logo>
    <icon>https://p.guipulp.top//favicon.ico</icon>
    <rights>All rights reserved 2019, PeterPan</rights>
    <entry>
        <title type="html"><![CDATA[iptables的四表五链]]></title>
        <id>https://p.guipulp.top//post/2019082802</id>
        <link href="https://p.guipulp.top//post/2019082802">
        </link>
        <updated>2019-08-28T04:44:56.000Z</updated>
        <content type="html"><![CDATA[<h2 id="netfilter介绍">netfilter介绍</h2>
<ul>
<li>它是iptables的主要的工作模块，位于内核中，在网络层的五个位置（也就是防火墙四表五链中的五链）注册了一些钩子函数，用来抓取数据包；</li>
<li>把数据包的信息拿出来匹配各个链位置在对应表中的规则；</li>
<li>匹配之后，进行相应的处理ACCEPT、DROP等等；</li>
</ul>
<h3 id="netfilter和iptables的关系图">netfilter和iptables的关系图</h3>
<p><img src="https://img.mubu.com/document_image/e0653c59-e432-40d9-a8bd-38558599a4da-1235096.jpg" alt="img"></p>
<h2 id="四表五链">四表五链</h2>
<ul>
<li>链就是位置：共有五个，进路由(PREROUTING)、进系统(INPUT) 、转发(FORWARD)、出系统(OUTPUT)、出路由(POSTROUTING)；</li>
<li>表就是存储的规则；数据包到了该链处，会去对应表中查询设置的规则，然后决定是否放行、丢弃、转发还是修改等等操作。</li>
</ul>
<h3 id="四表">四表</h3>
<ul>
<li>filter表——过滤数据包</li>
<li>Nat表——用于网络地址转换（IP、端口）</li>
<li>Mangle表——修改数据包的服务类型、TTL、并且可以配置路由实现QOS</li>
<li>Raw表——决定数据包是否被状态跟踪机制处理</li>
</ul>
<h3 id="五链">五链</h3>
<ul>
<li>INPUT链——进来的数据包应用此规则链中的策略</li>
<li>OUTPUT链——外出的数据包应用此规则链中的策略</li>
<li>FORWARD链——转发数据包时应用此规则链中的策略</li>
<li>PREROUTING链——对数据包作路由选择前应用此链中的规则（所有的数据包进来的时侯都先由这个链处理）</li>
<li>POSTROUTING链——对数据包作路由选择后应用此链中的规则（所有的数据包出来的时侯都先由这个链处理）</li>
</ul>
<h3 id="链表关系图">链表关系图</h3>
<p><img src="https://img2018.cnblogs.com/blog/1479220/201809/1479220-20180926150504012-1967719887.png" alt="image-20190828125115128"></p>
<h3 id="数据处理流程">数据处理流程</h3>
<p>我们结合下面的图举个例子：<br>
假如有数据包从Network IN要通过iptables，数据包流向如下：<br>
1.Network IN数据包到达服务器的网络接口<br>
2.进入raw表的 PREROUTING 链，这个链的作用是决定数据包是否被状态跟踪。<br>
进入 mangle 表的 PREROUTING 链，在此可以修改数据包，比如 TOS 等。<br>
进入 nat 表的 PREROUTING 链，可以在此做DNAT(目标地址转换)<br>
3.决定路由，查看目标地址是交给本地主机还是转发给其它主机。</p>
<p>4.到这里分两种情况，一种情况是数据包要转发给其它主机（一般情况下它是在担任网关服务器），数据包会依次经过：<br>
5.进入 mangle 表的 FORWARD 链，<br>
进入 filter 表的 FORWARD 链，在这里我们可以对所有转发的数据包进行过滤。<br>
6.进入 mangle 表的 POSTROUTING 链<br>
进入 nat 表的 POSTROUTING 链，在这里一般都是用来做 SNAT （源地址转换）<br>
7.数据包流出网络接口，发往network out。</p>
<p>8.另一种情况，数据包的目标地址就是发给本地主机的，它会依次穿过：<br>
9.进入 mangle 表的 INPUT 链，<br>
进入 filter 表的 INPUT 链，在这里我们可以对流入的所有数据包进行过滤，<br>
数据包交给本地主机的应用程序进行处理。<br>
10.应用程序处理完毕后发送的数据包进行路由发送决定。<br>
11.进入 raw 表的 OUTPUT 链。<br>
进入 mangle 表的 OUTPUT 链，<br>
进入 nat 表的 OUTPUT 链，<br>
进入 filter 表的 OUTPUT 链。<br>
12.进入 mangle 表的 POSTROUTING 链，<br>
进入 nat 表的 POSTROUTING 链。<br>
13.进入出去的网络接口，发送往network out。</p>
<p>所以，如果我们要过滤控制包的进出，只需要把input chain和forward chain这两个关口把控住就好了，这也是我们需要把input chain和forward chain默认设置为drop的原因。</p>
<ul>
<li>
<p>图一</p>
<p><img src="https://img.mubu.com/document_image/c1b9ef44-0c8d-4818-a18b-83b527e8d0e4-1235096.jpg" alt="img"></p>
</li>
<li>
<p>图二</p>
<p><img src="https://img.mubu.com/document_image/bcb89f30-1a57-4967-9a41-3cb6e2c8a5bc-1235096.jpg" alt="img"></p>
</li>
</ul>
<h2 id="iptables操作">iptables操作</h2>
<pre><code>iptables [-t 表名] 选项 [链名] [条件] [-j 控制类型]

-P 设置默认策略:iptables
 
-P INPUT (DROP|ACCEPT)
 
-F 清空规则链
 
-L 查看规则链
 
-A 在规则链的末尾加入新规则
 
-I num 在规则链的头部加入新规则
 
-D num 删除某一条规则
 
-s 匹配来源地址IP/MASK，加叹号&quot;!&quot;表示除这个IP外。
 
-d 匹配目标地址
 
-i 网卡名称 匹配从这块网卡流入的数据
 
-o 网卡名称 匹配从这块网卡流出的数据
 
-p 匹配协议,如tcp,udp,icmp
 
--dport num 匹配目标端口号
 
--sport num 匹配来源端口号
</code></pre>
<h3 id="常用iptables的语句">常用iptables的语句</h3>
<pre><code>1. 删除已有规则
在开始创建iptables规则之前，你也许需要删除已有规则。命令如下：
iptables -F
(or)
iptables –flush
查看已有规则
iptables -nL
 
 
2.设置链的默认策略
链的默认政策设置为”ACCEPT”（接受），若要将INPUT,FORWARD,OUTPUT链设置成”DROP”（拒绝），命令如下：
iptables -P INPUT DROP
iptables -P FORWARD DROP
iptables -P OUTPUT DROP
当INPUT链和OUTPUT链都设置成DROP时，对于每一个防火墙规则，我们都应该定义两个规则。例如：一个传入另一个传出。在下面所有的例子中，由于我们已将DROP设置成INPUT链和OUTPUT链的默认策略，每种情况我们都将制定两条规则。
当然，如果你相信你的内部用户,则可以省略上面的最后一行。例如：默认不丢弃所有出站的数据包。在这种情况下,对于每一个防火墙规则要求,你只需要制定一个规则——只对进站的数据包制定规则。
 
 
3. 阻止指定IP地址
例：丢弃来自IP地址x.x.x.x的包
BLOCK_THIS_IP=&quot;x.x.x.x&quot;
iptables -A INPUT -s &quot;$BLOCK_THIS_IP&quot; -j DROP
注：当你在log里发现来自某ip地址的异常记录，可以通过此命令暂时阻止该地址的访问以做更深入分析
例：阻止来自IP地址x.x.x.x eth0 tcp的包
iptables -A INPUT -i eth0 -s &quot;$BLOCK_THIS_IP&quot; -j DROP
iptables -A INPUT -i eth0 -p tcp -s &quot;$BLOCK_THIS_IP&quot; -j DROP
 
 
4. 允许所有SSH的连接请求
例：允许所有来自外部的SSH连接请求，即只允许进入eth0接口，并且目标端口为22的数据包
iptables -A INPUT -i eth0 -p tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT
 
 
5. 仅允许来自指定网络的SSH连接请求
例：仅允许来自于192.168.100.0/24域的用户的ssh连接请求
iptables -A INPUT -i eth0 -p tcp -s 192.168.100.0/24 --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT
 
 
6.允许http和https的连接请求
例：允许所有来自web - http的连接请求
iptables -A INPUT -i eth0 -p tcp --dport 80 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 80 -m state --state ESTABLISHED -j ACCEPT
例：允许所有来自web - https的连接请求
iptables -A INPUT -i eth0 -p tcp --dport 443 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 443 -m state --state ESTABLISHED -j ACCEPT
 
 
7. 使用multiport 将多个规则结合在一起
允许多个端口从外界连入，除了为每个端口都写一条独立的规则外，我们可以用multiport将其组合成一条规则。如下所示：
例：允许所有ssh,http,https的流量访问
iptables -A INPUT -i eth0 -p tcp -m multiport --dports 22,80,443 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp -m multiport --sports 22,80,443 -m state --state ESTABLISHED -j ACCEPT
 
 
8. 允许从本地发起的SSH请求
iptables -A OUTPUT -o eth0 -p tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A INPUT -i eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT
请注意,这与允许ssh连入的规则略有不同。本例在OUTPUT链上，我们允许NEW和ESTABLISHED状态。在INPUT链上，我们只允许ESTABLISHED状态。ssh连入的规则与之相反。
 
 
9. 仅允许从本地发起到一个指定的网络域的SSH请求
例：仅允许从内部连接到网域192.168.100.0/24
iptables -A OUTPUT -o eth0 -p tcp -d 192.168.100.0/24 --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A INPUT -i eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT
 
 
10. 允许从本地发起的HTTPS连接请求
下面的规则允许输出安全的网络流量。如果你想允许用户访问互联网，这是非常有必要的。在服务器上，这些规则能让你使用wget从外部下载一些文件
iptables -A OUTPUT -o eth0 -p tcp --dport 443 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A INPUT -i eth0 -p tcp --sport 443 -m state --state ESTABLISHED -j ACCEPT
注：对于HTTP web流量的外联请求，只需要将上述命令中的端口从443改成80即可。
 
 
11. 负载平衡传入的网络流量
使用iptables可以实现传入web流量的负载均衡，我们可以传入web流量负载平衡使用iptables防火墙规则。
例：使用iptables nth将HTTPS流量负载平衡至三个不同的ip地址。
iptables -A PREROUTING -i eth0 -p tcp --dport 443 -m state --state NEW -m nth --counter 0 --every 3 --packet 0 -j DNAT --to-destination 192.168.1.101:443
iptables -A PREROUTING -i eth0 -p tcp --dport 443 -m state --state NEW -m nth --counter 0 --every 3 --packet 1 -j DNAT --to-destination 192.168.1.102:443
iptables -A PREROUTING -i eth0 -p tcp --dport 443 -m state --state NEW -m nth --counter 0 --every 3 --packet 2 -j DNAT --to-destination 192.168.1.103:443
 
 
12. 允许外部主机ping内部主机
iptables -A INPUT -p icmp --icmp-type echo-request -j ACCEPT
iptables -A OUTPUT -p icmp --icmp-type echo-reply -j ACCEPT
 
 
13. 允许内部主机ping外部主机
iptables -A OUTPUT -p icmp --icmp-type echo-request -j ACCEPT
iptables -A INPUT -p icmp --icmp-type echo-reply -j ACCEPT
 
 
14. 允许回环访问 例：在服务器上允许127.0.0.1回环访问。
iptables -A INPUT -i lo -j ACCEPT
iptables -A OUTPUT -o lo -j ACCEPT
 
 
15. 允许内部网络域外部网络的通信
防火墙服务器上的其中一个网卡连接到外部，另一个网卡连接到内部服务器，使用以下规则允许内部网络与外部网络的通信。此例中，eth1连接到外部网络(互联网)，eth0连接到内部网络(例如:192.168.1.x)。
iptables -A FORWARD -i eth0 -o eth1 -j ACCEPT
 
 
16. 允许出站的DNS连接
iptables -A OUTPUT -p udp -o eth0 --dport 53 -j ACCEPT
iptables -A INPUT -p udp -i eth0 --sport 53 -j ACCEPT
 
 
17. 允许NIS连接
如果你使用NIS管理用户帐户，你需要允许NIS连接。如果你不允许NIS相关的ypbind连接请求，即使SSH连接请求已被允许，用户仍然无法登录。NIS的端口是动态的，先使用命令rpcinfo –p来知道端口号，此例中为853和850端口。
rpcinfo -p | grep ypbind
例：允许来自111端口以及ypbind使用端口的连接请求
iptables -A INPUT -p tcp --dport 111 -j ACCEPT
iptables -A INPUT -p udp --dport 111 -j ACCEPT
iptables -A INPUT -p tcp --dport 853 -j ACCEPT
iptables -A INPUT -p udp --dport 853 -j ACCEPT
iptables -A INPUT -p tcp --dport 850 -j ACCEPT
iptables -A INPUT -p udp --dport 850 -j ACCEPT
注：当你重启ypbind之后端口将不同，上述命令将无效。有两种解决方案：
1）使用你NIS的静态IP
2）编写shell脚本通过“rpcinfo - p”命令自动获取动态端口号,并在上述iptables规则中使用。
 
 
18. 允许来自指定网络的rsync连接请求
例：允许来自网络192.168.101.0/24的rsync连接请求
iptables -A INPUT -i eth0 -p tcp -s 192.168.101.0/24 --dport 873 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 873 -m state --state ESTABLISHED -j ACCEPT
 
 
19. 允许来自指定网络的MySQL连接请求
很多情况下，MySQL数据库与web服务跑在同一台服务器上。有时候我们仅希望DBA和开发人员从内部网络（192.168.100.0/24）直接登录数据库，可尝试以下命令：
iptables -A INPUT -i eth0 -p tcp -s 192.168.100.0/24 --dport 3306 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 3306 -m state --state ESTABLISHED -j ACCEPT
 
 
20. 允许Sendmail, Postfix邮件服务
Sendmail和postfix都使用了25端口，因此我们只需要允许来自25端口的连接请求即可。
iptables -A INPUT -i eth0 -p tcp --dport 25 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 25 -m state --state ESTABLISHED -j ACCEPT
 
 
21. 允许IMAP和IMAPS 例：允许IMAP/IMAP2流量，端口为143
iptables -A INPUT -i eth0 -p tcp --dport 143 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 143 -m state --state ESTABLISHED -j ACCEPT
例：允许IMAPS流量，端口为993
iptables -A INPUT -i eth0 -p tcp --dport 993 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 993 -m state --state ESTABLISHED -j ACCEPT
 
 
22. 允许POP3和POP3S 例：允许POP3访问
iptables -A INPUT -i eth0 -p tcp --dport 110 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 110 -m state --state ESTABLISHED -j ACCEPT
例：允许POP3S访问
iptables -A INPUT -i eth0 -p tcp --dport 995 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 995 -m state --state ESTABLISHED -j ACCEPT
 
 
23. 防止DoS攻击
iptables -A INPUT -p tcp --dport 80 -m limit --limit 25/minute --limit-burst 100 -j ACCEPT
上述例子中： -m limit: 启用limit扩展 –limit 25/minute: 允许最多每分钟25个连接（根据需求更改）。 –limit-burst 100: 只有当连接达到limit-burst水平(此例为100)时才启用上述limit/minute限制。
 
 
24. 端口转发 例：将来自422端口的流量全部转到22端口。
这意味着我们既能通过422端口又能通过22端口进行ssh连接。启用DNAT转发。
iptables -t nat -A PREROUTING -p tcp -d 192.168.102.37 --dport 422 -j DNAT --to 192.168.102.37:22
除此之外，还需要允许连接到422端口的请求
iptables -A INPUT -i eth0 -p tcp --dport 422 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 422 -m state --state ESTABLISHED -j ACCEPT
 
 
25. 记录丢弃的数据表 第一步：新建名为LOGGING的链
iptables -N LOGGING
第二步：将所有来自INPUT链中的数据包跳转到LOGGING链中
iptables -A INPUT -j LOGGING
第三步：为这些包自定义个前缀，命名为”IPTables Packet Dropped”
iptables -A LOGGING -m limit --limit 2/min -j LOG --log-prefix &quot;IPTables Packet Dropped: &quot; --log-level 7
第四步：丢弃这些数据包
iptables -A LOGGING -j DROP
 
 
26. ip映射(NAT)
假设有一家ISP提供园区Internet接入服务，为了方便管理，该ISP分配给园区用户的IP地址都是伪IP，但是部分用户要求建立自己的WWW服务器对外发布信息。
我们可以再防火墙的外部网卡上绑定多个合法IP地址，然后通过ip映射使发给其中某一 个IP地址的包转发至内部某一用户的WWW服务器上，然后再将该内部WWW服务器响应包伪装成该合法IP发出的包。
 
我们假设以下情景:
该ISP分配给A单位www服务器的ip为:
伪ip:192.168.1.100
真实ip:202.110.123.100
该ISP分配给B单位www服务器的ip为:
伪ip:192.168.1.200
真实ip:202.110.123.200
linux防火墙的ip地址分别为:
内网接口eth1:192.168.1.1
外网接口eth0:202.110.123.1
然后我们将分配给A、B单位的真实ip绑定到防火墙的外网接口，以root权限执行以下命令:
ifconfig eth0 add 202.110.123.100 netmask 255.255.255.0
ifconfig eth0 add 202.110.123.200 netmask 255.255.255.0
 
首先，对防火墙接收到的目的ip为202.110.123.100和202.110.123.200的所有数据包进行目的NAT(DNAT):
iptables -A PREROUTING -i eth0 -d 202.110.123.100 -j DNAT --to 192.168.1.100
iptables -A PREROUTING -i eth0 -d 202.110.123.200 -j DNAT --to 192.168.1.200
 
其次，对防火墙接收到的源ip地址为192.168.1.100和192.168.1.200的数据包进行源NAT(SNAT):
iptables -A POSTROUTING -o eth0 -s 192.168.1.100 -j SNAT --to 202.110.123.100
iptables -A POSTROUTING -o eth0 -s 192.168.1.200 -j SNAT --to 202.110.123.200
 
这样，所有目的ip为202.110.123.100和202.110.123.200的数据包都将分别被转发给192.168.1.100和192.168.1.200;而所有来自192.168.1.100和192.168.1.200的数据包都将分 别被伪装成由202.110.123.100和202.110.123.200，从而也就实现了ip映射。
</code></pre>
<h2 id="参考文档">参考文档</h2>
<pre><code>https://www.cnblogs.com/zejin2008/p/5919550.html
https://blog.csdn.net/skc361/article/details/20481521
https://www.cnblogs.com/zhujingzhi/p/9706664.html
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[使用iptables实现网络互联]]></title>
        <id>https://p.guipulp.top//post/2019082702</id>
        <link href="https://p.guipulp.top//post/2019082702">
        </link>
        <updated>2019-08-27T15:34:48.000Z</updated>
        <content type="html"><![CDATA[<h2 id="iptables实现网络功能的方式">iptables实现网络功能的方式</h2>
<p>SNAT：代理上网实现原地址转换</p>
<p>DNAT：用于端口转发实现目标地址转换</p>
<p>实现以上两种功能均要打开内核路由转发功能</p>
<pre><code>[root@xuegod63 ~]# vim /etc/sysctl.conf
#改：#net.ipv4.ip_forward = 0
#为： net.ipv4.ip_forward = 1
#改完使配置生效：
[root@xuegod63 ~]# sysctl -p
</code></pre>
<h2 id="iptables的源地址转换snat">iptables的源地址转换(SNAT)</h2>
<h3 id="双网卡实现snat">双网卡实现SNAT</h3>
<p><strong>示例一</strong></p>
<p><img src="https://img-blog.csdn.net/20171117145802662?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2hlYXRfZ3JvdW5k/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="è¿éåå¾çæè¿°"></p>
<pre><code># 左边的机器为A，右边的机器为B ，A和B在同一个自网路，B有两个网卡。一个网卡和A同在一个网络，另外一个网卡可以连接外网。 
# 配置方式
[root@xuegod63 ~]# iptables -t nat -A POSTROUTING -s 192.168.240.0/24   -j  SNAT  --to 192.168.1.250
或：
[root@xuegod63 ~]#iptables -t nat -A POSTROUTING -s 192.168.2.0/24  -o eth0  -j MASQUERADE
# 拒绝访问转发机器本身
[root@xuegod63 ~]# iptables -A INPUT -s 192.168.2.2 -j DROP
</code></pre>
<p><strong>示例二</strong></p>
<pre><code>场景：
有一台A服务器不能上网，和B服务器通过内网来连接，B服务器可以上网，要实现A服务器也可以上网。
A IP:192.168.0.35
B IP:192.168.0.146（123.196.112.146为外网ip）

SNAT:改变数据包的源地址。防火墙会使用外部地址，替换数据包的本地网络地址。这样使网络内部主机能够与网络外部通信。

1.在可以上网那台服务器B上，开启内核路由转发功能
echo 1 &gt; /proc/sys/net/ipv4/ip_forward

2.在需要通过代理上网服务器A上，查看路由表。并添加默认网关。route add default gw 192.168.0.146
[root@localhost ~]# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
192.168.0.0     0.0.0.0         255.255.255.0   U     0      0        0 eth0
169.254.0.0     0.0.0.0         255.255.0.0     U     0      0        0 eth0
0.0.0.0         192.168.0.146   0.0.0.0         UG    0      0        0 eth0

3.在可以上网那台服务器B上添加SNAT规则
iptables -t nat -A POSTROUTING -o eth0 -s 192.168.0.0/24 -j SNAT –-to 123.196.112.146

4.保存
service iptables save

5.验证是否可以正常上网。
</code></pre>
<h3 id="单网卡实现snat">单网卡实现SNAT</h3>
<p><strong>示例</strong></p>
<pre><code>场景：
给转发机器网卡绑定两个IP。一个内网IP，无需设置网关，另一个接口，IP要可以连接internet。
直接将下面内容加在 /etc/rc.local 启动脚本内（把192.168.51.250换成外网IP，把192.168.151.1换内网的网络地址）

echo 1 &gt; /proc/sys/net/ipv4/ip_forward
ifconfig eth0:1 192.168.151.1 netmask 255.255.255.0
iptables -F
iptables -F -t nat
iptables -P FORWARD DROP
iptables -A FORWARD -s 192.168.151.0/24 -j ACCEPT
iptables -A FORWARD -i eth0 -m state --state ESTABLISHED,RELATED -j ACCEPT
iptables -t nat -A POSTROUTING -o eth0 -s 192.168.151.0/24 -j SNAT --to 192.168.51.250
</code></pre>
<h2 id="iptables的目标地址转换dnat">iptables的目标地址转换（DNAT）</h2>
<h3 id="双网卡实现dnat">双网卡实现DNAT</h3>
<pre><code># 代理192.168.0.10的80端口到NAT服务器
# NAT服务器IP： 172.16.0.1  192.168.0.1
# 服务端IP： 192.168.0.10 需要将服务端的网关指向NAT服务器
# 做DNAT转发：
iptables -t nat -A PREROUTING -i ens33 -d 172.16.0.1 -p tcp --dport  80 -j DNAT --to-destination 192.168.0.10
</code></pre>
<h3 id="单网卡实现dnat">单网卡实现DNAT</h3>
<p><strong>说明</strong></p>
<pre><code>A)将本机端口转发至目标机器
iptables -t nat -A PREROUTING -p tcp -d [本地服务器主网卡绑定IP] –dport [本地端口] -j DNAT –to-destination [目标IP:目标端口]
iptables -t nat -A PREROUTING -p udp -d [本地服务器主网卡绑定IP] –dport [本地端口] -j DNAT –to-destination [目标IP:目标端口]

B)将目标机器返回的数据转发至本机
iptables -t nat -A POSTROUTING -p tcp -d [目标IP] –dport [目标端口] -j SNAT –to-source [本地服务器主网卡绑定IP]
iptables -t nat -A POSTROUTING -p udp -d [目标IP] –dport [目标端口] -j SNAT –to-source [本地服务器主网卡绑定IP]
</code></pre>
<p><strong>示例</strong></p>
<pre><code># 单网卡iptables配置路由转发，需要配置snat和dnat
# pc1 ip:192.168.23.252 
# pc2 ip:192.168.23.253 

iptables -t nat -A PREROUTING -d 192.168.23.252 -p tcp --dport 80 -j DNAT --to-destination 192.168.23.253:80
# 如果进来的route的访问目的地址是192.168.23.252并且访问的目的端口是80，就进行dnat转换，把目的地址改为192.168.23.253 ，端口还是80
 
iptables -t nat -A POSTROUTING -d 192.168.23.253 -p tcp --dport 80 -j SNAT --to 192.168.23.252
# 当FORWARD 出来后，访问的目的地址是192.168.23.253，端口是80的。进行snat地址转换，把原地址改为192.168.23.252
 
iptables -A FORWARD -o eth0 -d 192.168.23.253 -p tcp --dport 80 -j ACCEPT
# 当从eth0出去的访问目的地址是 192.168.23.253且目的端口是80的route，允许通过
 
iptables -A FORWARD -i eth0 -s 192.168.23.253 -p tcp --dport 80 -j ACCEPT
# 当从eth0进来的原地址是 192.168.23.253且目的端口是80的route，允许通过
 
# 保存规则启动iptables
service iptables save
service iptables start
</code></pre>
<h2 id="参考链接">参考链接</h2>
<p>https://blog.csdn.net/wheat_ground/article/details/78561750</p>
<p>https://blog.51cto.com/jinchuang/1947052</p>
<p>http://www.flynoc.com/help/helpview_390.html</p>
<p>https://blog.51cto.com/2333657/2157070?source=dra</p>
<p>https://blog.csdn.net/weixin_42537861/article/details/81195070</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[使用ipvs模式安装kubernetes]]></title>
        <id>https://p.guipulp.top//post/2019082701</id>
        <link href="https://p.guipulp.top//post/2019082701">
        </link>
        <updated>2019-08-27T07:11:33.000Z</updated>
        <content type="html"><![CDATA[<h3 id="安装ipvs">安装ipvs</h3>
<pre><code>cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
$ chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4

</code></pre>
<h3 id="安装ipset和ipvsadm">安装ipset和ipvsadm</h3>
<pre><code>yum install ipset
yum install ipvsadm
</code></pre>
<h3 id="安装时间服务">安装时间服务</h3>
<pre><code>$ yum install chrony -y
$ systemctl enable chronyd
$ systemctl start chronyd
$ chronyc sources
210 Number of sources = 4
MS Name/IP address         Stratum Poll Reach LastRx Last sample
===============================================================================
^+ sv1.ggsrv.de                  2   6    17    32   -823us[-1128us] +/-   98ms
^- montreal.ca.logiplex.net      2   6    17    32    -17ms[  -17ms] +/-  179ms
^- ntp6.flashdance.cx            2   6    17    32    -32ms[  -32ms] +/-  161ms
^* 119.28.183.184                2   6    33    32   +661us[ +357us] +/-   38ms
$ date
Tue Aug 27 09:28:41 CST 2019
</code></pre>
<h3 id="安装ipvs模式的kubernetes集群模板">安装ipvs模式的kubernetes集群模板</h3>
<pre><code>apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.151.30.11  # apiserver 节点内网IP
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: ydzs-master
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS  # dns类型
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: gcr.azk8s.cn/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.15.3  # k8s版本
networking:
  dnsDomain: cluster.local
  podSubnet: 192.168.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs  # kube-proxy 模式
</code></pre>
<h3 id="参考地址">参考地址</h3>
<p>https://mp.weixin.qq.com/s/vnriX2bTtnkv8i2UpLeNnA</p>
<p>https://www.qikqiak.com/post/use-kubeadm-install-kubernetes-1.15.3/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes的国内软件源配置]]></title>
        <id>https://p.guipulp.top//post/2019080801</id>
        <link href="https://p.guipulp.top//post/2019080801">
        </link>
        <updated>2019-08-08T06:40:52.000Z</updated>
        <content type="html"><![CDATA[<h2 id="ubuntu-软件源配置">ubuntu 软件源配置</h2>
<pre><code>add-apt-repository &quot;deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main&quot;
add-apt-repository &quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot;
wget -q https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg -O- | sudo apt-key add -
curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -
apt-get update
apt-get -y install apt-transport-https ca-certificates curl software-properties-common
apt-get -y install docker-ce kubeadm kubectl kubelet
systemctl enable docker kubelet &amp;&amp; systemctl start docker
</code></pre>
<h2 id="centos-软件源配置">centos 软件源配置</h2>
<pre><code># 切换到配置目录
cd /etc/yum.repos.d/
# 配置docker-ce阿里源
wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
# 配置kubernetes阿里源
cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[centos快速创建kubernetes]]></title>
        <id>https://p.guipulp.top//post/2019090701</id>
        <link href="https://p.guipulp.top//post/2019090701">
        </link>
        <updated>2019-08-07T08:22:17.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前置条件">前置条件</h2>
<ul>
<li>系统要求:64位centos7.6</li>
<li>关闭防火墙和selinux</li>
<li>关闭操作系统swap分区(使用k8s不推荐打开)</li>
</ul>
<h2 id="环境说明">环境说明</h2>
<p>本手册安装方式适用于单节点测试部署</p>
<h2 id="准备工作每个节点都需要执行">准备工作(每个节点都需要执行)</h2>
<h3 id="docker和kubernetes软件源配置">Docker和kubernetes软件源配置</h3>
<pre><code># 切换到配置目录
cd /etc/yum.repos.d/
# 配置docker-ce阿里源
wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
# 配置kubernetes阿里源
cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
</code></pre>
<h3 id="配置内核相关参数">配置内核相关参数</h3>
<pre><code>cat &lt;&lt;EOF &gt;  /etc/sysctl.d/ceph.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system
</code></pre>
<h3 id="安装相应软件包">安装相应软件包</h3>
<pre><code># 安装kubeadm kubelet kubectl
yum install docker-ce kubeadm kubectl kubelet -y

# 开机启动kubelet和docker
systemctl enable docker kubelet

# 启动docker
systemctl start docker
</code></pre>
<h2 id="部署">部署</h2>
<h3 id="安装k8s">安装k8s</h3>
<pre><code># 30.4.241.19 需要更新为当前部署节点ip地址
kubeadm init --apiserver-advertise-address 30.4.241.19 --pod-network-cidr=10.244.0.0/16 --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers --ignore-preflight-errors=Swap

# 初始化完成后按提示执行以下命令
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>
<h3 id="安装flannel">安装flannel</h3>
<pre><code>kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
</code></pre>
<h3 id="检查是否安装完成">检查是否安装完成</h3>
<pre><code>root@k8s4:~# kubectl  get nodes
NAME   STATUS   ROLES    AGE   VERSION
k8s4   Ready    master   20m   v1.14.2
root@k8s4:~# kubectl  get nodes
NAME   STATUS   ROLES    AGE   VERSION
k8s4   Ready    master   20m   v1.14.2
root@k8s4:~# kubectl  get pods --all-namespaces
NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE
kube-system   coredns-8cc96f57d-cfr4j        1/1     Running   0          20m
kube-system   coredns-8cc96f57d-stcz6        1/1     Running   0          20m
kube-system   etcd-k8s4                      1/1     Running   0          19m
kube-system   kube-apiserver-k8s4            1/1     Running   0          19m
kube-system   kube-controller-manager-k8s4   1/1     Running   0          19m
kube-system   kube-flannel-ds-amd64-k4q6q    1/1     Running   0          50s
kube-system   kube-proxy-lhjsf               1/1     Running   0          20m
kube-system   kube-scheduler-k8s4            1/1     Running   0          19m
</code></pre>
<h3 id="测试是否能正常使用">测试是否能正常使用</h3>
<pre><code># 取消节点污点,使master能被正常调度, k8s4请更改为你自有集群的nodename
kubectl  taint node k8s4 node-role.kubernetes.io/master:NoSchedule-

# 创建nginx deploy
root@k8s4:~# kubectl  create deploy nginx --image nginx
deployment.apps/nginx created

root@k8s4:~# kubectl  get pods
NAME                     READY   STATUS    RESTARTS   AGE
nginx-65f88748fd-9sk6z   1/1     Running   0          2m44s

# 暴露nginx到集群外
root@k8s4:~# kubectl  expose deploy nginx --port=80 --type=NodePort
service/nginx exposed
root@k8s4:~# kubectl  get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP        25m
nginx        NodePort    10.104.109.234   &lt;none&gt;        80:32129/TCP   5s
root@k8s4:~# curl 127.0.0.1:32129
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes NVIDIA GPU 实践]]></title>
        <id>https://p.guipulp.top//post/201986</id>
        <link href="https://p.guipulp.top//post/201986">
        </link>
        <updated>2019-08-06T03:36:20.000Z</updated>
        <content type="html"><![CDATA[<p>查看此文档时候，内容可能已经过时，但大体过程不会有什么变化，建议按照官网说明进行安装。</p>
<h3 id="前置条件">前置条件</h3>
<ol>
<li>GNU/Linux x86_64 with kernel version &gt; 3.10</li>
<li>Docker &gt;= 1.12</li>
<li>NVIDIA GPU with Architecture &gt; Fermi (2.1)</li>
<li><a href="http://www.nvidia.com/object/unix.html">NVIDIA drivers</a> ~= 361.93 (untested on older versions)</li>
</ol>
<h3 id="安装nvidia-docker">安装nvidia-docker</h3>
<h4 id="ubuntu-16041804-debian-jessiestretch">Ubuntu 16.04/18.04, Debian Jessie/Stretch</h4>
<pre><code># Add the package repositories
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
</code></pre>
<h4 id="centos-7-docker-ce-rhel-7475-docker-ce-amazon-linux-12">CentOS 7 (docker-ce), RHEL 7.4/7.5 (docker-ce), Amazon Linux 1/2</h4>
<pre><code>distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | sudo tee /etc/yum.repos.d/nvidia-docker.repo

sudo yum install -y nvidia-container-toolkit
sudo systemctl restart docker
</code></pre>
<h4 id="设置nvidia-docker-runtime">设置nvidia-docker runtime</h4>
<pre><code># vim /etc/docker/daemon.json
{
    &quot;default-runtime&quot;: &quot;nvidia&quot;,
    &quot;runtimes&quot;: {
        &quot;nvidia&quot;: {
            &quot;path&quot;: &quot;/usr/bin/nvidia-container-runtime&quot;,
            &quot;runtimeArgs&quot;: []
        }
    }
}

# restart docker
sudo systemctl restart docker
sudo systemctl restart kubelet
</code></pre>
<h4 id="测试nvidia-docker">测试nvidia-docker</h4>
<pre><code>#### Test nvidia-smi with the latest official CUDA image
$ docker run --gpus all nvidia/cuda:9.0-base nvidia-smi

# Start a GPU enabled container on two GPUs
$ docker run --gpus 2 nvidia/cuda:9.0-base nvidia-smi

# Starting a GPU enabled container on specific GPUs
$ docker run --gpus '&quot;device=1,2&quot;' nvidia/cuda:9.0-base nvidia-smi
$ docker run --gpus '&quot;device=UUID-ABCDEF,1'&quot; nvidia/cuda:9.0-base nvidia-smi

# Specifying a capability (graphics, compute, ...) for my container
# Note this is rarely if ever used this way
$ docker run --gpus all,capabilities=utility nvidia/cuda:9.0-base nvidia-smi
</code></pre>
<h3 id="安装kubernetes-插件">安装kubernetes 插件</h3>
<pre><code>kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta/nvidia-device-plugin.yml
</code></pre>
<h3 id="测试运行gpu">测试运行GPU</h3>
<pre><code># 适用于多GPU核心的带企业显卡的集群测试
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  containers:
    - name: cuda-container
      image: nvidia/cuda:9.0-devel
      resources:
        limits:
          nvidia.com/gpu: 2 # requesting 2 GPUs
    - name: digits-container
      image: nvidia/digits:6.0
      resources:
        limits:
          nvidia.com/gpu: 2 # requesting 2 GPUs
</code></pre>
<pre><code># 适用于个人GPU测试
kubectl  create job nvjob --image=nvidia/cuda:9.0-base -- nvidia-smi
</code></pre>
<h3 id="参考资料">参考资料</h3>
<p><a href="https://github.com/NVIDIA/nvidia-docker">https://github.com/NVIDIA/nvidia-docker</a></p>
<p><a href="https://github.com/NVIDIA/k8s-device-plugin">https://github.com/NVIDIA/k8s-device-plugin</a></p>
<p><a href="https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/">https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/</a></p>
<p><a href="https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)#prerequisites">https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)#prerequisites</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[使用SRS自建直播服务]]></title>
        <id>https://p.guipulp.top//post/zi-jian-zhi-bo-fu-wu</id>
        <link href="https://p.guipulp.top//post/zi-jian-zhi-bo-fu-wu">
        </link>
        <updated>2019-08-02T11:16:55.000Z</updated>
        <content type="html"><![CDATA[<h2 id="直播平台搭建分享">直播平台搭建分享</h2>
<pre><code># 分享链接
https://juejin.im/entry/599634d2f265da248a7a66a7
</code></pre>
<h2 id="直播服务">直播服务</h2>
<ul>
<li><a href="https://github.com/arut/nginx-rtmp-module">nginx-rtmp</a></li>
<li><a href="http://www.ossrs.net/srs.release/releases/">srs</a></li>
</ul>
<p>最终选择srs的原因在于其功能丰富，兼容性更好，且支持多种视频格式，支持多级级联，拥有丰富的接口。具体srs和nginx对比如下：</p>
<p><a href="https://github.com/ossrs/srs/tree/2.0release#compare">https://github.com/ossrs/srs/tree/2.0release#compare</a></p>
<p>srs架构：</p>
<p><a href="https://github.com/ossrs/srs/wiki/v2_CN_Architecture">https://github.com/ossrs/srs/wiki/v2_CN_Architecture</a></p>
<h2 id="srs">srs</h2>
<h3 id="srs部署">SRS部署</h3>
<pre><code># https://github.com/ossrs/srs/wiki/v2_CN_Home
# http://blog.itpub.net/31559758/viewspace-2220944/
# https://blog.csdn.net/ai2000ai/article/details/72769961
# srs 可以分角色进行部署具体查看官方文档，推荐使用的客户端为V2版本，V3暂未进入稳定版
# 系统建议采用centos6

[root@lab1 ~]# wget https://codeload.github.com/ossrs/srs/tar.gz/v2.0-r6
[root@lab1 ~]# tar xvf v2.0-r6
[root@lab1 ~]# cd srs-2.0-r6/trunk/
[root@lab1 trunk]# ./configure &amp;&amp; make
[root@lab1 trunk]# ./objs/srs -c conf/http.flv.live.conf （支持输出flv和rtmp）

# 推流地址
rtmp://172.16.10.101:8080/live/demo
# 播放地址
flv: http://172.16.10.101:8080/live/demo.flv
rtmp: rtmp://172.16.10.101:8080/live/demo
</code></pre>
<h3 id="srs热更新">SRS热更新</h3>
<pre><code>Reload的方法为：killall -1 srs
使用启动脚本：/etc/init.d/srs reload
</code></pre>
<h2 id="nginx-rtmp">nginx-rtmp</h2>
<h3 id="nginx-rtmp部署">nginx-rtmp部署</h3>
<pre><code># 部署参考链接
https://www.cnblogs.com/monjeo/p/8492357.html
https://wiki.blanc.site/archives/3f439512.html
https://www.cnblogs.com/Leo_wl/p/5654819.html
https://www.helplib.com/GitHub/article_145412
https://smartshitter.com/musings/2017/12/nginx-rtmp-streaming-with-simple-authentication/
</code></pre>
<h2 id="直播搭建过程中遇见的问题">直播搭建过程中遇见的问题</h2>
<pre><code># GOP（关键帧）设置过长导致直播延迟过大或者直播黑屏
http://blog.itpub.net/31559352/viewspace-2564571/
https://blog.csdn.net/wishfly/article/details/53079303
https://blog.csdn.net/lcalqf/article/details/55258929
https://www.cnblogs.com/suannaibuding/p/6366978.html
https://www.jianshu.com/p/3a84d0c8f466
http://blog.chinaunix.net/uid-26000296-id-4932817.html
https://www.jianshu.com/p/9a0bbe1658eb
http://www.voidcn.com/article/p-naiixvpv-bnw.html  (解释gop和解决办法)

# 直播出现客户端中断问题
buffer被清空，需要检查源流

# 直播无法播放出http流检查
检查流文件是否生成，流路径为 objs/nginx/html/live/
</code></pre>
<h2 id="直播延迟和推流设置">直播延迟和推流设置</h2>
<pre><code>https://developer.qiniu.com/pili/kb/1722/server-live-on-delay
https://cloud.tencent.com/document/product/267/32726
</code></pre>
<h2 id="更优秀的直播方案">更优秀的直播方案</h2>
<pre><code># SD-RTN
https://blog.csdn.net/wolf09311/article/details/52680170
</code></pre>
<h2 id="ffmpeg推流方式">ffmpeg推流方式</h2>
<pre><code>docker run --name  test1 --restart=always -d -v $(pwd):/temp/ jrottenberg/ffmpeg -re  -stream_loop -1 -i /temp/war.mp4 -vcodec libx264 -acodec libfdk_aac \
      -metadata service_name=&quot;Channel 1&quot; -metadata service_provider=&quot;PBS&quot; \
      -b 1.5M\
      -f flv rtmp://172.16.10.101:1935/live/demo
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes使用PodPreset特性预设整个集群]]></title>
        <id>https://p.guipulp.top//post/t-I_dB2CB</id>
        <link href="https://p.guipulp.top//post/t-I_dB2CB">
        </link>
        <updated>2019-06-24T07:25:54.000Z</updated>
        <summary type="html"><![CDATA[<p>有的时候我们需要根据情况,预先配置集群里面的容器的某些属性,则可以通过podpreset配置实现.</p>
]]></summary>
        <content type="html"><![CDATA[<p>有的时候我们需要根据情况,预先配置集群里面的容器的某些属性,则可以通过podpreset配置实现.</p>
<!-- more -->
<h3 id="配置apiserver">配置apiserver</h3>
<blockquote>
<p>通过标签选择器完成对特定pod的部分属性预设</p>
</blockquote>
<pre><code># 开启settings.k8s.io/v1alpha1api
- --runtime-config=settings.k8s.io/v1alpha1=true
# 开启adminssion controller
- --enable-admission-plugins=PodPreset
# 可配置选项 Initializers,NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,Validating
</code></pre>
<h3 id="配置测试yaml">配置测试yaml</h3>
<pre><code>root@ubuntu4:~/podpreset# cat preset-time.yaml
kind: PodPreset
apiVersion: settings.k8s.io/v1alpha1
metadata:
  name: timezone
spec:
  selector:
    matchLabels:
      app: nginx
  volumeMounts:
    - name: host-time
      mountPath: /etc/localtime
  volumes:
    - name: host-time
      hostPath:
        path: /etc/localtime

</code></pre>
<pre><code>root@ubuntu4:~/podpreset# kubectl  create deploy nginx --image nginx -o yaml --dry-run
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: nginx
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes配置外网dns]]></title>
        <id>https://p.guipulp.top//post/9yPSD7pfq</id>
        <link href="https://p.guipulp.top//post/9yPSD7pfq">
        </link>
        <updated>2019-06-21T02:36:34.000Z</updated>
        <summary type="html"><![CDATA[<p>通过更新coredns configmap，设置upstream字段实现指定上游解析服务器</p>
]]></summary>
        <content type="html"><![CDATA[<p>通过更新coredns configmap，设置upstream字段实现指定上游解析服务器</p>
<!-- more -->
<h1 id="编辑coredns-configmap">编辑coredns configmap</h1>
<pre><code>[root@k8s-node1 ~]# cat setdns.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
  labels:
      addonmanager.kubernetes.io/mode: EnsureExists
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
            pods insecure
            upstream 114.114.114.114
            fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        proxy . /etc/resolv.conf
        cache 30
        reload
    }
</code></pre>
<h1 id="应用config-map">应用config map</h1>
<pre><code> kubectl apply -f setdns.yaml
</code></pre>
<h1 id="检查外网dns解析">检查外网dns解析</h1>
<pre><code>
[root@k8s-node1 ~]# kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools
If you don't see a command prompt, try pressing enter.
dnstools# host www.baidu.com
www.baidu.com is an alias for www.a.shifen.com.
www.a.shifen.com has address 220.181.111.188
www.a.shifen.com has address 220.181.112.244

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubeadm搭建v1.10.3kubernetes集群]]></title>
        <id>https://p.guipulp.top//post/jZoeyPf8e</id>
        <link href="https://p.guipulp.top//post/jZoeyPf8e">
        </link>
        <updated>2019-06-21T02:33:01.000Z</updated>
        <summary type="html"><![CDATA[<p>本文使用kubeadm和容器化etcd安装kubernetes集群用于poc</p>
]]></summary>
        <content type="html"><![CDATA[<p>本文使用kubeadm和容器化etcd安装kubernetes集群用于poc</p>
<!-- more -->
<blockquote>
<p>lab 说明：<br>
1.此lab手册不需要越过长城<br>
2.100.64.16.176 为master地址，配置时候需要替换该地址为新环境的master地址<br>
3.lab环境为单节点master环境<br>
4.lab环境etcd访问方式为http方式</p>
</blockquote>
<table>
<thead>
<tr>
<th>clusterinfo</th>
<th>ip</th>
<th>package</th>
</tr>
</thead>
<tbody>
<tr>
<td>k8s-node1(master)</td>
<td>100.64.16.176</td>
<td>kubeadm, etcd, kubelet, kubectl, docker</td>
</tr>
<tr>
<td>k8s-node2(node)</td>
<td>100.64.16.177</td>
<td>kubeadm, kubelet, kubectl, docker</td>
</tr>
<tr>
<td>k8s-node3(node)</td>
<td>100.64.16.178</td>
<td>kubeadm, kubelet, kubectl, docker</td>
</tr>
</tbody>
</table>
<h3 id="环境准备">环境准备</h3>
<h4 id="注意事项">注意事项</h4>
<p>该部分内容需要在所有节点执行</p>
<h4 id="关闭防火墙">关闭防火墙</h4>
<pre><code>systemctl stop firewalld &amp;&amp; systemctl disable firewalld
</code></pre>
<h4 id="关闭交换">关闭交换</h4>
<pre><code>swapoff -a
</code></pre>
<h4 id="设置系统路由参数">设置系统路由参数</h4>
<p>系统路由参数,防止kubeadm报路由警告</p>
<pre><code>cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
vm.swappiness=0
EOF
sysctl --system
</code></pre>
<h4 id="关闭selinux">关闭selinux</h4>
<pre><code>sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinux
setenforce 0
</code></pre>
<h4 id="安装docker">安装docker</h4>
<ol>
<li>安装</li>
</ol>
<pre><code>yum install -y docker
systemctl enable docker
systemctl start docker
</code></pre>
<ol start="2">
<li>配置docker daemon(不需要加速器可省略)</li>
</ol>
<pre><code>vim /etc/docker/daemon.json 
{
  &quot;registry-mirrors&quot;: [&quot;https://v5d7kh0f.mirror.aliyuncs.com&quot;]
}
</code></pre>
<ol start="3">
<li>修改docker启动文件</li>
</ol>
<pre><code>vim /usr/lib/systemd/system/docker.service
# 添加下面行
ExecStartPost=/sbin/iptables -P FORWARD ACCEPT
</code></pre>
<ol start="4">
<li>重新加载docker服务</li>
</ol>
<pre><code>systemctl daemon-reload
service docker restart
</code></pre>
<h3 id="安装k8s">安装k8s</h3>
<h4 id="软件包准备">软件包准备</h4>
<p><strong>注意：该部分内容所有节点都需要执行</strong></p>
<ol>
<li>上传软件包到所有节点</li>
</ol>
<p><a href="https://pan.baidu.com/s/1LPDoy5QHan77QKj8s8nz1w">软件包下载地址</a></p>
<blockquote>
<p>若提示资源不存在，请复制浏览器里的地址，重新访问。</p>
</blockquote>
<pre><code># 所有节点解压并安装软件包
tar xf k8s-*.tgz &amp;&amp; cd k8s-* &amp;&amp; yum localinstall -y *.rpm
</code></pre>
<ol start="2">
<li>配置所有节点的kubelet</li>
</ol>
<pre><code># 修改/etc/systemd/system/kubelet.service.d/10-kubeadm.conf
# 添加如下配置 
Environment=&quot;KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-shanghai.aliyuncs.com/gcr-k8s/pause-amd64:3.0&quot;
</code></pre>
<ol start="3">
<li>所有节点重新载入配置</li>
</ol>
<pre><code>systemctl daemon-reload
systemctl enable kubelet
</code></pre>
<h4 id="master安装">master安装</h4>
<h5 id="安装etcd">安装etcd</h5>
<ol>
<li>配置启动etcd</li>
</ol>
<pre><code>docker stop etcd &amp;&amp; docker rm etcd
rm -rf /data/etcd
mkdir -p /data/etcd
docker run -d \
--restart always \
-v /etc/etcd/ssl/certs:/etc/ssl/certs \
-v /data/etcd:/var/lib/etcd \
-p 2380:2380 \
-p 2379:2379 \
--name etcd \
registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.1.12 \
etcd --name=etcd0 \
--advertise-client-urls=http://100.64.16.176:2379 \
--listen-client-urls=http://0.0.0.0:2379 \
--initial-advertise-peer-urls=http://100.64.16.176:2380 \
--listen-peer-urls=http://0.0.0.0:2380 \
--initial-cluster-token=9477af68bbee1b9ae037d6fd9e7efefd \
--initial-cluster=etcd0=http://100.64.16.176:2380 \
--initial-cluster-state=new \
--auto-tls \
--peer-auto-tls \
--data-dir=/var/lib/etcd
</code></pre>
<ol start="2">
<li>验证etcd</li>
</ol>
<pre><code>docker exec -ti etcd ash
etcdctl member list
etcdctl cluster-health
exit
</code></pre>
<h5 id="生成kubeadmin文件">生成kubeadmin文件</h5>
<pre><code># 生成token
# 保留token后面还要使用
token=$(kubeadm token generate)
echo $token

# 生成配置文件
# advertiseAddress 配置为master地址
cat &gt;kubeadm-master.config&lt;&lt;EOF
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
kubernetesVersion: v1.10.3
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers

api:
  advertiseAddress: 100.64.16.176

apiServerExtraArgs:
  endpoint-reconciler-type: lease

controllerManagerExtraArgs:
  node-monitor-grace-period: 10s
  pod-eviction-timeout: 10s

networking:
  podSubnet: 10.244.0.0/16

etcd:
  endpoints:
  - &quot;http://100.64.16.176:2379&quot;

apiServerCertSANs:
- &quot;node1&quot;
- &quot;100.64.16.176&quot;
- &quot;127.0.0.1&quot;

token: $token
tokenTTL: &quot;0&quot;

featureGates:
  CoreDNS: true
EOF
</code></pre>
<h5 id="初始化master节点">初始化master节点</h5>
<pre><code>kubeadm init --config kubeadm-master.config
systemctl enable kubelet
</code></pre>
<h5 id="配置kubectl">配置kubectl</h5>
<pre><code>mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>
<h5 id="安装网络插件">安装网络插件</h5>
<ol>
<li>下载配置</li>
</ol>
<pre><code>mkdir flannel &amp;&amp; cd flannel
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
</code></pre>
<ol start="2">
<li>修改配置</li>
</ol>
<pre><code># 此处的ip配置要与上面kubeadm的pod-network一致
  net-conf.json: |
    {
      &quot;Network&quot;: &quot;10.244.0.0/16&quot;,
      &quot;Backend&quot;: {
        &quot;Type&quot;: &quot;vxlan&quot;
      }
    }

# 修改镜像
image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64
</code></pre>
<ol start="3">
<li>启动</li>
</ol>
<pre><code>kubectl apply -f kube-flannel.yml
</code></pre>
<ol start="4">
<li>检查master</li>
</ol>
<pre><code>kubectl get pods -n kube-system
kubectl get svc -n kube-system
</code></pre>
<h4 id="node节点安装">node节点安装</h4>
<ol>
<li>配置node kubelet开机启动</li>
</ol>
<pre><code>systemctl enable kubelet
systemctl start kubelet
</code></pre>
<ol start="2">
<li>初始化node</li>
</ol>
<pre><code># 这个命令是之前初始化master完成时，输出的命令
kubeadm join 100.64.16.176:6443 --token i2ha18.euygiv9g922b5fkf --discovery-token-ca-cert-hash sha256:1c8fe595a95e2daed035ba89f9604ef6ad5fb0d589b89adbff80ea8c09db567e
</code></pre>
<ol start="3">
<li>在master上检查集群状态</li>
</ol>
<pre><code>kubectl get nodes
</code></pre>
<h3 id="k8s集群检查">k8s集群检查</h3>
<pre><code>kubectl run nginx --replicas=2 --labels=&quot;run=load-balancer-example&quot; --image=nginx  --port=80
kubectl get pods --all-namespaces -o wide 
</code></pre>
<p><a href="https://asciinema.org/a/bnyI5J6IcupDM5SV2eglMtJKu"><img src="https://asciinema.org/a/bnyI5J6IcupDM5SV2eglMtJKu.png" alt="asciicast"></a></p>
<h3 id="总结">总结</h3>
<p>经过这篇博客, 我们成功部署了一个3个节点的kubernetes 1.10.3的集群, 但是此时集群DashBoard, 监控都还没完成, 在接下来的<a href="https://p.fengjingblog.top/2018/06/11/kubernetes-Addons%E5%AE%89%E8%A3%85/">kubernetes Addons安装</a>中将进行这些组件的部署。</p>
<h3 id="排错">排错</h3>
<p>如果出现如下错误：</p>
<pre><code>Jun 14 12:33:30 k8s-node1 kubelet: E0614 12:33:30.265135   12998 summary.go:102] Failed to get system container stats for &quot;/system.slice/kubelet.service&quot;: failed to get cgroup stats for &quot;/system.slice/kubelet.service&quot;: failed to get container info for &quot;/system.slice/kubelet.service&quot;: unknown container &quot;/system.slice/kubelet.service&quot;
Jun 14 12:33:30 k8s-node1 etcd: rejected connection from &quot;100.64.16.176:54174&quot; (error &quot;EOF&quot;, ServerName &quot;&quot;)
</code></pre>
<ul>
<li>针对etcd的报错可以尝试降低etcd的版本，本人使用<code>https://github.com/coreos/etcd/releases/download/v3.1.15/etcd-v3.1.15-linux-amd64.tar.gz</code>该版本的etcd后解决。</li>
<li>针对kubelet的错误可以在<code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> 文件最后一行末尾追加<code>--runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice</code>,完整文档如下：</li>
</ul>
<pre><code>[Service]
Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf&quot;
Environment=&quot;KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true&quot;
Environment=&quot;KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin&quot;
Environment=&quot;KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local&quot;
Environment=&quot;KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt&quot;
Environment=&quot;KUBELET_CADVISOR_ARGS=--cadvisor-port=0&quot;
Environment=&quot;KUBELET_CGROUP_ARGS=--cgroup-driver=systemd&quot;
Environment=&quot;KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki&quot;
Environment=&quot;KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-shanghai.aliyuncs.com/gcr-k8s/pause-amd64:3.0&quot;
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_CADVISOR_ARGS $KUBELET_CGROUP_ARGS $KUBELET_CERTIFICATE_ARGS $KUBELET_EXTRA_ARGS --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice
</code></pre>
]]></content>
    </entry>
</feed>