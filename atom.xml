<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://p.guipulp.top/</id>
    <title>PeterPan</title>
    <updated>2019-09-02T07:55:46.542Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://p.guipulp.top/"/>
    <link rel="self" href="https://p.guipulp.top//atom.xml"/>
    <subtitle>贵有恒，何必三更起五更睡；最无益，只怕一日暴十寒。</subtitle>
    <logo>https://p.guipulp.top//images/avatar.png</logo>
    <icon>https://p.guipulp.top//favicon.ico</icon>
    <rights>All rights reserved 2019, PeterPan</rights>
    <entry>
        <title type="html"><![CDATA[linux cgroups简介]]></title>
        <id>https://p.guipulp.top//post/2019090202</id>
        <link href="https://p.guipulp.top//post/2019090202">
        </link>
        <updated>2019-09-02T07:54:27.000Z</updated>
        <content type="html"><![CDATA[<p>https://blog.csdn.net/ahilll/article/details/82109008<br>
https://blog.csdn.net/chenleiking/article/details/87988851</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes 清理孤儿POD]]></title>
        <id>https://p.guipulp.top//post/2019090201</id>
        <link href="https://p.guipulp.top//post/2019090201">
        </link>
        <updated>2019-09-02T07:53:25.000Z</updated>
        <content type="html"><![CDATA[<h3 id="孤儿pod的产生">孤儿pod的产生</h3>
<p>节点OOM以后或者节点异常崩溃的情况下，pod未能被正常的清理而导致的孤儿进程。</p>
<p><strong>提示如下</strong></p>
<pre><code>Orphaned pod found - but volume paths are still present on disk
</code></pre>
<p><strong>进入k8s的pod目录</strong></p>
<pre><code>cd /var/lib/kubelet/pods/
</code></pre>
<h3 id="解决问题">解决问题</h3>
<p>过滤日志中的孤儿pod，删除pod。</p>
<pre><code class="language-js">#!/bin/bash
num=$(grep &quot;errors similar to this. Turn up verbosity to see them.&quot;  /var/log/messages |tail -1 | awk '{print $12}' |sed 's/&quot;//g')
echo $num

while [ $num ]
do
   [ -d &quot;/var/lib/kubelet/pods/${num}&quot; ] &amp;&amp; rm -rf /var/lib/kubelet/pods/${num}

  sleep 2s
  num=$(grep &quot;errors similar to this. Turn up verbosity to see them.&quot;  /var/log/messages |tail -1 | awk '{print $12}' |sed 's/&quot;//g')
  [ -d &quot;/var/lib/kubelet/pods/${num}&quot; ] || num=

  echo &quot;$num remaining&quot;

done
</code></pre>
<p>但是这个方法有一定的<code>危险性</code>，还不确认是否有<code>数据丢失</code>的风险，如果可以确认，再执行。</p>
<p>如果已经挂载了PVC等相关存储，先执行umount再执行删除。</p>
<p>再去查看日志，就会发现syslog不会再刷类似的日志了。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes资源预留]]></title>
        <id>https://p.guipulp.top//post/2019083002</id>
        <link href="https://p.guipulp.top//post/2019083002">
        </link>
        <updated>2019-08-30T09:25:37.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>下面内容还处于测试阶段，生产上是否能保证集群稳定暂时还不清楚。😁😁</p>
</blockquote>
<h2 id="事故">事故</h2>
<p>今天我们的开发环境由于java应用内存抢占原因导致k8s集群worker节点全部宕机，主要原因是程序和资源没进行限制规划，且kubelet也没配置资源预留，那host上所有资源都是可以给pod调配使用的，这样就引起集群雪崩效应，比如集群内有一台上跑的pod没做resource limt导致占用资源过大导致将宿主机压死了，此时这个节点在kubernetes内就是一个no ready的状态了，kubernetes会将这台host上所有的pod在其他节点上重建，也就意味着那个有问题的pod重新跑在其他正常的节点上，将另外正常的节点压跨。循怀下去直到集群内所有主机都挂了，这就是集群雪崩效应。</p>
<h2 id="解决办法">解决办法</h2>
<p>在kubernetes中可以通过给kubelet配置参数预留资源给系统进程和kubernetes进程保证它们稳定运行。目前能实现到cpu、memory、ephemeral-storage层面的资源预留。<br>
重点提两点<br>
cpu：cpu是配置cpu shares实际上对应的是cpu的优先级，简单来说，这个在cpu繁忙时，它能有更高优先级获取更多cpu资源。</p>
<p>ephemeral-storage是kubernetes1.8开始引入的一个资源限制的对象，kubernetes 1.10版本中kubelet默认已经打开的了,主要是用于对本地临时存储使用空间大小的限制，如对pod的empty dir、/var/lib/kubelet、日志、容器可读写层的使用大小的限制。</p>
<h2 id="配置">配置</h2>
<h3 id="基本概念">基本概念</h3>
<p>在讲配置之前我们先了解几个概念：</p>
<p>Node capacity：节点总共的资源<br>
kube-reserved：给kubernetes进程预留的资源<br>
system-reserved：给操作系统预留的资源<br>
eviction-threshold：kubelet eviction的阀值<br>
allocatable：留给pod使用的资源</p>
<pre><code>node_allocatable=Node_capacity-(kube-reserved+system-reserved+hard-eviction)
</code></pre>
<pre><code>      Node Capacity
---------------------------
|     kube-reserved       |
|-------------------------|
|     system-reserved     |
|-------------------------|
|    eviction-threshold   |
|-------------------------|
|                         |
|      allocatable        |
|   (available for pods)  |
|                         |
|                         |
---------------------------
</code></pre>
<p>Kubernetes 节点上的 <code>Allocatable</code> 被定义为 pod 可用计算资源量。调度器不会超额申请 <code>Allocatable</code>。目前支持 <code>CPU</code>, <code>memory</code> 和 <code>storage</code> 这几个参数。</p>
<p>Node Allocatable 暴露为 API 中 <code>v1.Node</code> 对象的一部分，也是 CLI 中 <code>kubectl describe node</code> 的一部分。</p>
<p>在 <code>kubelet</code> 中，可以为两类系统守护进程预留资源。</p>
<h3 id="使用kubelet参数进行限制">使用kubelet参数进行限制</h3>
<p>此方法适用于老版本的kubernetes集群，在新版本(1.11之前)中已经不适用了。</p>
<p>https://k8smeetup.github.io/docs/tasks/administer-cluster/reserve-compute-resources/</p>
<p>https://www.bladewan.com/2018/01/26/k8s_resource_resver/</p>
<h3 id="使用kubelet-config进行限制">使用kubelet config进行限制</h3>
<p>kubelet 较新的版本都采用kubelet config对集群的kubelet进行配置，此处采用静态配置方式，当然也可以使用动态配置方式。</p>
<pre><code># 编辑文档kubelet config 文件
vim /var/lib/kubelet/config
</code></pre>
<p><strong>配置资源预留</strong></p>
<pre><code># 找到enforceNodeAllocatable 注释掉
#enforceNodeAllocatable:
#- pods

# 添加以下内容，系统和kubelet均预留CPU500m内存500Mi磁盘5G
systemReserved:
  cpu: &quot;500m&quot;
  memory: &quot;500Mi&quot;
  ephemeral-storage: &quot;5Gi&quot;
kubeReserved:
  cpu: &quot;500m&quot;
  memory: &quot;500Mi&quot;
  ephemeral-storage: &quot;5Gi&quot;
systemReservedCgroup: /system.slice
kubeReservedCgroup: /kubelet.service
EnforceNodeAllocatable:
- pods
- kube-reserved
- system-reserved

</code></pre>
<p><strong>配置软驱逐（默认为硬驱逐）</strong></p>
<p>软驱逐有资源驱逐等待时间，硬驱逐为立刻驱逐。</p>
<pre><code># evictionHard 注释掉，并在后面新增以下内容

#evictionHard:
EvictionSoft:
  imagefs.available: 15%
  memory.available: 10%
  nodefs.available: 10%
  nodefs.inodesFree: 5%
EvictionSoftGracePeriod:
  memory.available: &quot;5m&quot;
  nodefs.available: &quot;2m&quot;
  nodefs.inodesFree: &quot;2m&quot;
  imagefs.available: &quot;2m&quot;
</code></pre>
<pre><code># 如果你使用的cgroup driver是croup还需要进行以下操作
# cpuset和hugetlb subsystem是默认没有初始化system.slice手动创建,
mkdir -p /sys/fs/cgroup/hugetlb/system.slice/kubelet.service/
mkdir -p /sys/fs/cgroup/cpuset/system.slice/kubelet.service/

# 配置在kubelet中避免重启失效
ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/cpuset/system.slice/kubelet.service
ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/hugetlb/system.slice/kubelet.service
</code></pre>
<p><strong>重启kubelet</strong></p>
<pre><code>service kubelet restart
</code></pre>
<h2 id="验证">验证</h2>
<pre><code>[root@m3 pki]# kubectl  describe node m1
Name:               m1
Roles:              master
Labels:             beta.kubernetes.io/arch=amd64
....
CreationTimestamp:  Mon, 26 Aug 2019 20:35:35 -0400
...
Addresses:
  InternalIP:  172.27.100.13
  Hostname:    m1
Capacity:
 cpu:                4
 ephemeral-storage:  36678148Ki
 hugepages-1Gi:      0
 hugepages-2Mi:      0
 memory:             8010576Ki
 pods:               110
Allocatable:
 cpu:                3
 ephemeral-storage:  23065162901
 hugepages-1Gi:      0
 hugepages-2Mi:      0
 memory:             6884176Ki
 pods:               110
</code></pre>
<ul>
<li>可以看到预留后，可用CPU为3，不预留为4，内存是一样的计算方式，此处预留了1G（500Mi+500Mi）</li>
</ul>
<h2 id="参考文档">参考文档</h2>
<p>https://github.com/rootsongjc/qa/issues/3</p>
<p>https://cloud.tencent.com/developer/article/1097002</p>
<p>https://blog.csdn.net/ahilll/article/details/82109008</p>
<p>http://dockone.io/article/4797</p>
<p>https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/</p>
<p>https://kubernetes.io/zh/docs/tasks/administer-cluster/kubelet-config-file/</p>
<p>https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go</p>
<p>https://www.bladewan.com/2018/01/26/k8s_resource_resver/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[修改kubernetes证书过期时间的骚操作]]></title>
        <id>https://p.guipulp.top//post/2019083001</id>
        <link href="https://p.guipulp.top//post/2019083001">
        </link>
        <updated>2019-08-30T08:50:14.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>简单验证（随机修改系统时间）是有效的（10年内），具体用起来怎样，自己去测试吧。</p>
</blockquote>
<p>最近公司早期部署的kubernetes的集群证书过期，研究了下kubernetes的证书时间，大致上是说修改kubeadm的源代码，自己编译kubeadm，实现证书99年。</p>
<p>kubernetes默认CA证书10年，很少有企业的一个服务能跑10年不更换，因此想到以下方法进行了以下实验。测试暂时没发现什么问题。</p>
<p>如果有多master，需要在每个master上进行以下操作。</p>
<pre><code>[root@m2 pki]# date -s 8/30/2028
2028年 08月 30日 星期三 00:00:00 EDT
[root@m2 pki]# kubectl get pods
Unable to connect to the server: x509: certificate has expired or is not yet valid
[root@m2 pki]# kubectl get pods
Unable to connect to the server: x509: certificate has expired or is not yet valid
[root@m2 pki]# kubeadm alpha certs renew all
certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed
certificate for serving the Kubernetes API renewed
certificate the apiserver uses to access etcd renewed
certificate for the API server to connect to kubelet renewed
certificate embedded in the kubeconfig file for the controller manager to use renewed
certificate for liveness probes to healtcheck etcd renewed
certificate for etcd nodes to communicate with each other renewed
certificate for serving etcd renewed
certificate for the front proxy client renewed
certificate embedded in the kubeconfig file for the scheduler manager to use renewed
[root@m2 pki]# ntpdate 0.cn.pool.ntp.org
30 Aug 04:51:35 ntpdate[9983]: step time server 203.107.6.88 offset -284065763.328437 sec
[root@m2 pki]# cp /etc/kubernetes/admin.conf ~/.kube/config
cp：是否覆盖&quot;/root/.kube/config&quot;？ y
[root@m2 pki]# openssl x509 -in apiserver.crt -noout -text
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number: 6495439138002669247 (0x5a246f7b50d18ebf)
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: CN=kubernetes
        Validity
            Not Before: Aug 27 00:34:59 2019 GMT
            Not After : Aug 30 04:00:30 2029 GMT
        Subject: CN=kube-apiserver
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (2048 bit)
                Modulus:
                    00:d5:c5:1a:60:e3:dd:5c:68:c6:a0:38:df:06:54:
                    7b:87:73:3f:6d:1b:bc:65:4e:8e:1a:e0:a7:13:a9:
                    df:a6:67:65:13:cb:c6:e7:c8:5e:60:3e:14:7d:c4:
                    e0:91:d5:de:8b:91:bd:9c:59:2b:35:87:62:87:c9:
                    97:c4:f6:c6:41:b4:6a:80:25:34:1a:d2:b4:ad:e0:
                    bc:d0:af:18:35:68:b9:46:7b:1e:af:da:24:ce:79:
                    94:75:77:3d:ae:67:ac:b9:08:5a:74:87:fd:c0:33:
                    ec:d9:d5:ba:71:ee:23:9e:39:69:fa:2f:76:3f:e1:
                    80:a4:89:a3:39:40:f9:ef:a6:f5:4c:27:3a:7c:60:
                    aa:83:ce:cf:48:2a:e9:6c:15:88:21:8b:fb:53:f8:
                    05:15:bf:35:10:9d:4e:63:f9:09:ae:9c:45:b2:75:
                    4f:8f:4e:a9:9f:bc:f8:c0:9f:29:bc:5d:a3:8f:a9:
                    c7:d1:ea:85:d4:79:32:e1:e3:b7:2f:08:eb:47:99:
                    af:db:b4:5d:a7:d9:c0:7f:29:63:ec:f4:bb:ba:9b:
                    16:f4:c1:11:28:37:7c:c8:f9:13:7b:38:86:83:ba:
                    f7:e8:dd:fd:05:9e:7c:46:4a:1e:d9:9e:8e:0c:ad:
                    86:09:70:da:40:46:e7:e8:5c:82:26:08:3a:4e:ad:
                    8a:a3
                Exponent: 65537 (0x10001)
        X509v3 extensions:
            X509v3 Key Usage: critical
                Digital Signature, Key Encipherment
            X509v3 Extended Key Usage:
                TLS Web Server Authentication
            X509v3 Subject Alternative Name:
                DNS:m2, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, IP Address:10.96.0.1, IP Address:172.27.100.14, IP Address:172.27.100.211, IP Address:172.29.100.211
    Signature Algorithm: sha256WithRSAEncryption
         ab:31:c7:2a:a2:52:40:e2:b5:c6:a4:fc:d7:f5:35:2c:c9:28:
         8d:ba:6a:1d:c3:06:d0:85:4e:bf:43:0c:27:a2:2f:13:62:97:
         89:cc:4e:0c:17:86:11:ef:21:2a:01:9d:1c:25:f2:48:c5:31:
         95:63:41:2c:57:2a:1c:1f:40:00:2f:1a:b1:90:7b:3d:6a:2b:
         7c:6f:33:9a:c8:09:44:4d:a4:ff:fa:06:de:77:66:e1:0f:59:
         29:bd:76:d1:52:b9:63:81:b4:ae:27:0f:ed:54:93:b1:21:8b:
         bf:4f:16:9e:b2:4d:dd:2f:ac:81:e8:86:38:c8:2d:d9:38:1b:
         ac:56:23:5e:8c:dc:96:ae:24:3f:dd:7f:ce:ba:97:b5:e7:07:
         05:df:f2:fd:ba:51:6d:94:13:c5:2d:a8:75:32:21:ad:e9:07:
         13:04:64:e9:c5:f9:3c:46:39:9a:16:59:3b:ff:91:af:7b:fb:
         14:6d:2e:66:2d:52:0d:1b:45:ba:e4:0c:42:23:1c:2c:ea:53:
         6a:20:88:87:24:b1:55:39:d0:93:7b:44:46:bc:46:16:37:d6:
         ff:e6:2f:dd:05:53:03:bc:d6:ba:32:1c:8a:7c:74:4b:cc:4d:
         3b:20:b6:80:a0:a8:28:a5:da:fb:0b:3a:43:c3:b7:79:86:a8:
         2e:cc:91:48

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[iptables的四表五链]]></title>
        <id>https://p.guipulp.top//post/2019082802</id>
        <link href="https://p.guipulp.top//post/2019082802">
        </link>
        <updated>2019-08-28T04:44:56.000Z</updated>
        <content type="html"><![CDATA[<h2 id="netfilter介绍">netfilter介绍</h2>
<ul>
<li>它是iptables的主要的工作模块，位于内核中，在网络层的五个位置（也就是防火墙四表五链中的五链）注册了一些钩子函数，用来抓取数据包；</li>
<li>把数据包的信息拿出来匹配各个链位置在对应表中的规则；</li>
<li>匹配之后，进行相应的处理ACCEPT、DROP等等；</li>
</ul>
<h3 id="netfilter和iptables的关系图">netfilter和iptables的关系图</h3>
<p><img src="https://img.mubu.com/document_image/e0653c59-e432-40d9-a8bd-38558599a4da-1235096.jpg" alt="img"></p>
<h2 id="四表五链">四表五链</h2>
<ul>
<li>链就是位置：共有五个，进路由(PREROUTING)、进系统(INPUT) 、转发(FORWARD)、出系统(OUTPUT)、出路由(POSTROUTING)；</li>
<li>表就是存储的规则；数据包到了该链处，会去对应表中查询设置的规则，然后决定是否放行、丢弃、转发还是修改等等操作。</li>
</ul>
<h3 id="四表">四表</h3>
<ul>
<li>filter表——过滤数据包</li>
<li>Nat表——用于网络地址转换（IP、端口）</li>
<li>Mangle表——修改数据包的服务类型、TTL、并且可以配置路由实现QOS</li>
<li>Raw表——决定数据包是否被状态跟踪机制处理</li>
</ul>
<h3 id="五链">五链</h3>
<ul>
<li>INPUT链——进来的数据包应用此规则链中的策略</li>
<li>OUTPUT链——外出的数据包应用此规则链中的策略</li>
<li>FORWARD链——转发数据包时应用此规则链中的策略</li>
<li>PREROUTING链——对数据包作路由选择前应用此链中的规则（所有的数据包进来的时侯都先由这个链处理）</li>
<li>POSTROUTING链——对数据包作路由选择后应用此链中的规则（所有的数据包出来的时侯都先由这个链处理）</li>
</ul>
<h3 id="链表关系图">链表关系图</h3>
<p><img src="https://img2018.cnblogs.com/blog/1479220/201809/1479220-20180926150504012-1967719887.png" alt="image-20190828125115128"></p>
<h3 id="数据处理流程">数据处理流程</h3>
<p>我们结合下面的图举个例子：<br>
假如有数据包从Network IN要通过iptables，数据包流向如下：<br>
1.Network IN数据包到达服务器的网络接口<br>
2.进入raw表的 PREROUTING 链，这个链的作用是决定数据包是否被状态跟踪。<br>
进入 mangle 表的 PREROUTING 链，在此可以修改数据包，比如 TOS 等。<br>
进入 nat 表的 PREROUTING 链，可以在此做DNAT(目标地址转换)<br>
3.决定路由，查看目标地址是交给本地主机还是转发给其它主机。</p>
<p>4.到这里分两种情况，一种情况是数据包要转发给其它主机（一般情况下它是在担任网关服务器），数据包会依次经过：<br>
5.进入 mangle 表的 FORWARD 链，<br>
进入 filter 表的 FORWARD 链，在这里我们可以对所有转发的数据包进行过滤。<br>
6.进入 mangle 表的 POSTROUTING 链<br>
进入 nat 表的 POSTROUTING 链，在这里一般都是用来做 SNAT （源地址转换）<br>
7.数据包流出网络接口，发往network out。</p>
<p>8.另一种情况，数据包的目标地址就是发给本地主机的，它会依次穿过：<br>
9.进入 mangle 表的 INPUT 链，<br>
进入 filter 表的 INPUT 链，在这里我们可以对流入的所有数据包进行过滤，<br>
数据包交给本地主机的应用程序进行处理。<br>
10.应用程序处理完毕后发送的数据包进行路由发送决定。<br>
11.进入 raw 表的 OUTPUT 链。<br>
进入 mangle 表的 OUTPUT 链，<br>
进入 nat 表的 OUTPUT 链，<br>
进入 filter 表的 OUTPUT 链。<br>
12.进入 mangle 表的 POSTROUTING 链，<br>
进入 nat 表的 POSTROUTING 链。<br>
13.进入出去的网络接口，发送往network out。</p>
<p>所以，如果我们要过滤控制包的进出，只需要把input chain和forward chain这两个关口把控住就好了，这也是我们需要把input chain和forward chain默认设置为drop的原因。</p>
<ul>
<li>
<p>图一</p>
<p><img src="https://img.mubu.com/document_image/c1b9ef44-0c8d-4818-a18b-83b527e8d0e4-1235096.jpg" alt="img"></p>
</li>
<li>
<p>图二</p>
<p><img src="https://img.mubu.com/document_image/bcb89f30-1a57-4967-9a41-3cb6e2c8a5bc-1235096.jpg" alt="img"></p>
</li>
</ul>
<h2 id="iptables操作">iptables操作</h2>
<pre><code>iptables [-t 表名] 选项 [链名] [条件] [-j 控制类型]

-P 设置默认策略:iptables
 
-P INPUT (DROP|ACCEPT)
 
-F 清空规则链
 
-L 查看规则链
 
-A 在规则链的末尾加入新规则
 
-I num 在规则链的头部加入新规则
 
-D num 删除某一条规则
 
-s 匹配来源地址IP/MASK，加叹号&quot;!&quot;表示除这个IP外。
 
-d 匹配目标地址
 
-i 网卡名称 匹配从这块网卡流入的数据
 
-o 网卡名称 匹配从这块网卡流出的数据
 
-p 匹配协议,如tcp,udp,icmp
 
--dport num 匹配目标端口号
 
--sport num 匹配来源端口号
</code></pre>
<h3 id="常用iptables的语句">常用iptables的语句</h3>
<pre><code>1. 删除已有规则
在开始创建iptables规则之前，你也许需要删除已有规则。命令如下：
iptables -F
(or)
iptables –flush
查看已有规则
iptables -nL
 
 
2.设置链的默认策略
链的默认政策设置为”ACCEPT”（接受），若要将INPUT,FORWARD,OUTPUT链设置成”DROP”（拒绝），命令如下：
iptables -P INPUT DROP
iptables -P FORWARD DROP
iptables -P OUTPUT DROP
当INPUT链和OUTPUT链都设置成DROP时，对于每一个防火墙规则，我们都应该定义两个规则。例如：一个传入另一个传出。在下面所有的例子中，由于我们已将DROP设置成INPUT链和OUTPUT链的默认策略，每种情况我们都将制定两条规则。
当然，如果你相信你的内部用户,则可以省略上面的最后一行。例如：默认不丢弃所有出站的数据包。在这种情况下,对于每一个防火墙规则要求,你只需要制定一个规则——只对进站的数据包制定规则。
 
 
3. 阻止指定IP地址
例：丢弃来自IP地址x.x.x.x的包
BLOCK_THIS_IP=&quot;x.x.x.x&quot;
iptables -A INPUT -s &quot;$BLOCK_THIS_IP&quot; -j DROP
注：当你在log里发现来自某ip地址的异常记录，可以通过此命令暂时阻止该地址的访问以做更深入分析
例：阻止来自IP地址x.x.x.x eth0 tcp的包
iptables -A INPUT -i eth0 -s &quot;$BLOCK_THIS_IP&quot; -j DROP
iptables -A INPUT -i eth0 -p tcp -s &quot;$BLOCK_THIS_IP&quot; -j DROP
 
 
4. 允许所有SSH的连接请求
例：允许所有来自外部的SSH连接请求，即只允许进入eth0接口，并且目标端口为22的数据包
iptables -A INPUT -i eth0 -p tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT
 
 
5. 仅允许来自指定网络的SSH连接请求
例：仅允许来自于192.168.100.0/24域的用户的ssh连接请求
iptables -A INPUT -i eth0 -p tcp -s 192.168.100.0/24 --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT
 
 
6.允许http和https的连接请求
例：允许所有来自web - http的连接请求
iptables -A INPUT -i eth0 -p tcp --dport 80 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 80 -m state --state ESTABLISHED -j ACCEPT
例：允许所有来自web - https的连接请求
iptables -A INPUT -i eth0 -p tcp --dport 443 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 443 -m state --state ESTABLISHED -j ACCEPT
 
 
7. 使用multiport 将多个规则结合在一起
允许多个端口从外界连入，除了为每个端口都写一条独立的规则外，我们可以用multiport将其组合成一条规则。如下所示：
例：允许所有ssh,http,https的流量访问
iptables -A INPUT -i eth0 -p tcp -m multiport --dports 22,80,443 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp -m multiport --sports 22,80,443 -m state --state ESTABLISHED -j ACCEPT
 
 
8. 允许从本地发起的SSH请求
iptables -A OUTPUT -o eth0 -p tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A INPUT -i eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT
请注意,这与允许ssh连入的规则略有不同。本例在OUTPUT链上，我们允许NEW和ESTABLISHED状态。在INPUT链上，我们只允许ESTABLISHED状态。ssh连入的规则与之相反。
 
 
9. 仅允许从本地发起到一个指定的网络域的SSH请求
例：仅允许从内部连接到网域192.168.100.0/24
iptables -A OUTPUT -o eth0 -p tcp -d 192.168.100.0/24 --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A INPUT -i eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT
 
 
10. 允许从本地发起的HTTPS连接请求
下面的规则允许输出安全的网络流量。如果你想允许用户访问互联网，这是非常有必要的。在服务器上，这些规则能让你使用wget从外部下载一些文件
iptables -A OUTPUT -o eth0 -p tcp --dport 443 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A INPUT -i eth0 -p tcp --sport 443 -m state --state ESTABLISHED -j ACCEPT
注：对于HTTP web流量的外联请求，只需要将上述命令中的端口从443改成80即可。
 
 
11. 负载平衡传入的网络流量
使用iptables可以实现传入web流量的负载均衡，我们可以传入web流量负载平衡使用iptables防火墙规则。
例：使用iptables nth将HTTPS流量负载平衡至三个不同的ip地址。
iptables -A PREROUTING -i eth0 -p tcp --dport 443 -m state --state NEW -m nth --counter 0 --every 3 --packet 0 -j DNAT --to-destination 192.168.1.101:443
iptables -A PREROUTING -i eth0 -p tcp --dport 443 -m state --state NEW -m nth --counter 0 --every 3 --packet 1 -j DNAT --to-destination 192.168.1.102:443
iptables -A PREROUTING -i eth0 -p tcp --dport 443 -m state --state NEW -m nth --counter 0 --every 3 --packet 2 -j DNAT --to-destination 192.168.1.103:443
 
 
12. 允许外部主机ping内部主机
iptables -A INPUT -p icmp --icmp-type echo-request -j ACCEPT
iptables -A OUTPUT -p icmp --icmp-type echo-reply -j ACCEPT
 
 
13. 允许内部主机ping外部主机
iptables -A OUTPUT -p icmp --icmp-type echo-request -j ACCEPT
iptables -A INPUT -p icmp --icmp-type echo-reply -j ACCEPT
 
 
14. 允许回环访问 例：在服务器上允许127.0.0.1回环访问。
iptables -A INPUT -i lo -j ACCEPT
iptables -A OUTPUT -o lo -j ACCEPT
 
 
15. 允许内部网络域外部网络的通信
防火墙服务器上的其中一个网卡连接到外部，另一个网卡连接到内部服务器，使用以下规则允许内部网络与外部网络的通信。此例中，eth1连接到外部网络(互联网)，eth0连接到内部网络(例如:192.168.1.x)。
iptables -A FORWARD -i eth0 -o eth1 -j ACCEPT
 
 
16. 允许出站的DNS连接
iptables -A OUTPUT -p udp -o eth0 --dport 53 -j ACCEPT
iptables -A INPUT -p udp -i eth0 --sport 53 -j ACCEPT
 
 
17. 允许NIS连接
如果你使用NIS管理用户帐户，你需要允许NIS连接。如果你不允许NIS相关的ypbind连接请求，即使SSH连接请求已被允许，用户仍然无法登录。NIS的端口是动态的，先使用命令rpcinfo –p来知道端口号，此例中为853和850端口。
rpcinfo -p | grep ypbind
例：允许来自111端口以及ypbind使用端口的连接请求
iptables -A INPUT -p tcp --dport 111 -j ACCEPT
iptables -A INPUT -p udp --dport 111 -j ACCEPT
iptables -A INPUT -p tcp --dport 853 -j ACCEPT
iptables -A INPUT -p udp --dport 853 -j ACCEPT
iptables -A INPUT -p tcp --dport 850 -j ACCEPT
iptables -A INPUT -p udp --dport 850 -j ACCEPT
注：当你重启ypbind之后端口将不同，上述命令将无效。有两种解决方案：
1）使用你NIS的静态IP
2）编写shell脚本通过“rpcinfo - p”命令自动获取动态端口号,并在上述iptables规则中使用。
 
 
18. 允许来自指定网络的rsync连接请求
例：允许来自网络192.168.101.0/24的rsync连接请求
iptables -A INPUT -i eth0 -p tcp -s 192.168.101.0/24 --dport 873 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 873 -m state --state ESTABLISHED -j ACCEPT
 
 
19. 允许来自指定网络的MySQL连接请求
很多情况下，MySQL数据库与web服务跑在同一台服务器上。有时候我们仅希望DBA和开发人员从内部网络（192.168.100.0/24）直接登录数据库，可尝试以下命令：
iptables -A INPUT -i eth0 -p tcp -s 192.168.100.0/24 --dport 3306 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 3306 -m state --state ESTABLISHED -j ACCEPT
 
 
20. 允许Sendmail, Postfix邮件服务
Sendmail和postfix都使用了25端口，因此我们只需要允许来自25端口的连接请求即可。
iptables -A INPUT -i eth0 -p tcp --dport 25 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 25 -m state --state ESTABLISHED -j ACCEPT
 
 
21. 允许IMAP和IMAPS 例：允许IMAP/IMAP2流量，端口为143
iptables -A INPUT -i eth0 -p tcp --dport 143 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 143 -m state --state ESTABLISHED -j ACCEPT
例：允许IMAPS流量，端口为993
iptables -A INPUT -i eth0 -p tcp --dport 993 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 993 -m state --state ESTABLISHED -j ACCEPT
 
 
22. 允许POP3和POP3S 例：允许POP3访问
iptables -A INPUT -i eth0 -p tcp --dport 110 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 110 -m state --state ESTABLISHED -j ACCEPT
例：允许POP3S访问
iptables -A INPUT -i eth0 -p tcp --dport 995 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 995 -m state --state ESTABLISHED -j ACCEPT
 
 
23. 防止DoS攻击
iptables -A INPUT -p tcp --dport 80 -m limit --limit 25/minute --limit-burst 100 -j ACCEPT
上述例子中： -m limit: 启用limit扩展 –limit 25/minute: 允许最多每分钟25个连接（根据需求更改）。 –limit-burst 100: 只有当连接达到limit-burst水平(此例为100)时才启用上述limit/minute限制。
 
 
24. 端口转发 例：将来自422端口的流量全部转到22端口。
这意味着我们既能通过422端口又能通过22端口进行ssh连接。启用DNAT转发。
iptables -t nat -A PREROUTING -p tcp -d 192.168.102.37 --dport 422 -j DNAT --to 192.168.102.37:22
除此之外，还需要允许连接到422端口的请求
iptables -A INPUT -i eth0 -p tcp --dport 422 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 422 -m state --state ESTABLISHED -j ACCEPT
 
 
25. 记录丢弃的数据表 第一步：新建名为LOGGING的链
iptables -N LOGGING
第二步：将所有来自INPUT链中的数据包跳转到LOGGING链中
iptables -A INPUT -j LOGGING
第三步：为这些包自定义个前缀，命名为”IPTables Packet Dropped”
iptables -A LOGGING -m limit --limit 2/min -j LOG --log-prefix &quot;IPTables Packet Dropped: &quot; --log-level 7
第四步：丢弃这些数据包
iptables -A LOGGING -j DROP
 
 
26. ip映射(NAT)
假设有一家ISP提供园区Internet接入服务，为了方便管理，该ISP分配给园区用户的IP地址都是伪IP，但是部分用户要求建立自己的WWW服务器对外发布信息。
我们可以再防火墙的外部网卡上绑定多个合法IP地址，然后通过ip映射使发给其中某一 个IP地址的包转发至内部某一用户的WWW服务器上，然后再将该内部WWW服务器响应包伪装成该合法IP发出的包。
 
我们假设以下情景:
该ISP分配给A单位www服务器的ip为:
伪ip:192.168.1.100
真实ip:202.110.123.100
该ISP分配给B单位www服务器的ip为:
伪ip:192.168.1.200
真实ip:202.110.123.200
linux防火墙的ip地址分别为:
内网接口eth1:192.168.1.1
外网接口eth0:202.110.123.1
然后我们将分配给A、B单位的真实ip绑定到防火墙的外网接口，以root权限执行以下命令:
ifconfig eth0 add 202.110.123.100 netmask 255.255.255.0
ifconfig eth0 add 202.110.123.200 netmask 255.255.255.0
 
首先，对防火墙接收到的目的ip为202.110.123.100和202.110.123.200的所有数据包进行目的NAT(DNAT):
iptables -A PREROUTING -i eth0 -d 202.110.123.100 -j DNAT --to 192.168.1.100
iptables -A PREROUTING -i eth0 -d 202.110.123.200 -j DNAT --to 192.168.1.200
 
其次，对防火墙接收到的源ip地址为192.168.1.100和192.168.1.200的数据包进行源NAT(SNAT):
iptables -A POSTROUTING -o eth0 -s 192.168.1.100 -j SNAT --to 202.110.123.100
iptables -A POSTROUTING -o eth0 -s 192.168.1.200 -j SNAT --to 202.110.123.200
 
这样，所有目的ip为202.110.123.100和202.110.123.200的数据包都将分别被转发给192.168.1.100和192.168.1.200;而所有来自192.168.1.100和192.168.1.200的数据包都将分 别被伪装成由202.110.123.100和202.110.123.200，从而也就实现了ip映射。
</code></pre>
<h2 id="参考文档">参考文档</h2>
<p>https://www.cnblogs.com/zejin2008/p/5919550.html<br>
https://blog.csdn.net/skc361/article/details/20481521<br>
https://www.cnblogs.com/zhujingzhi/p/9706664.html</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[使用iptables实现网络互联]]></title>
        <id>https://p.guipulp.top//post/2019082702</id>
        <link href="https://p.guipulp.top//post/2019082702">
        </link>
        <updated>2019-08-27T15:34:48.000Z</updated>
        <content type="html"><![CDATA[<h2 id="iptables实现网络功能的方式">iptables实现网络功能的方式</h2>
<p>SNAT：代理上网实现原地址转换</p>
<p>DNAT：用于端口转发实现目标地址转换</p>
<p>实现以上两种功能均要打开内核路由转发功能</p>
<pre><code>[root@xuegod63 ~]# vim /etc/sysctl.conf
#改：#net.ipv4.ip_forward = 0
#为： net.ipv4.ip_forward = 1
#改完使配置生效：
[root@xuegod63 ~]# sysctl -p
</code></pre>
<h2 id="iptables的源地址转换snat">iptables的源地址转换(SNAT)</h2>
<h3 id="双网卡实现snat">双网卡实现SNAT</h3>
<p><strong>示例一</strong></p>
<p><img src="https://img-blog.csdn.net/20171117145802662?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2hlYXRfZ3JvdW5k/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="è¿éåå¾çæè¿°"></p>
<pre><code># 左边的机器为A，右边的机器为B ，A和B在同一个自网路，B有两个网卡。一个网卡和A同在一个网络，另外一个网卡可以连接外网。 
# 配置方式
[root@xuegod63 ~]# iptables -t nat -A POSTROUTING -s 192.168.240.0/24   -j  SNAT  --to 192.168.1.250
或：
[root@xuegod63 ~]#iptables -t nat -A POSTROUTING -s 192.168.2.0/24  -o eth0  -j MASQUERADE
# 拒绝访问转发机器本身
[root@xuegod63 ~]# iptables -A INPUT -s 192.168.2.2 -j DROP
</code></pre>
<p><strong>示例二</strong></p>
<pre><code>场景：
有一台A服务器不能上网，和B服务器通过内网来连接，B服务器可以上网，要实现A服务器也可以上网。
A IP:192.168.0.35
B IP:192.168.0.146（123.196.112.146为外网ip）

SNAT:改变数据包的源地址。防火墙会使用外部地址，替换数据包的本地网络地址。这样使网络内部主机能够与网络外部通信。

1.在可以上网那台服务器B上，开启内核路由转发功能
echo 1 &gt; /proc/sys/net/ipv4/ip_forward

2.在需要通过代理上网服务器A上，查看路由表。并添加默认网关。route add default gw 192.168.0.146
[root@localhost ~]# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
192.168.0.0     0.0.0.0         255.255.255.0   U     0      0        0 eth0
169.254.0.0     0.0.0.0         255.255.0.0     U     0      0        0 eth0
0.0.0.0         192.168.0.146   0.0.0.0         UG    0      0        0 eth0

3.在可以上网那台服务器B上添加SNAT规则
iptables -t nat -A POSTROUTING -o eth0 -s 192.168.0.0/24 -j SNAT –-to 123.196.112.146

4.保存
service iptables save

5.验证是否可以正常上网。
</code></pre>
<h3 id="单网卡实现snat">单网卡实现SNAT</h3>
<p><strong>示例</strong></p>
<pre><code>场景：
给转发机器网卡绑定两个IP。一个内网IP，无需设置网关，另一个接口，IP要可以连接internet。
直接将下面内容加在 /etc/rc.local 启动脚本内（把192.168.51.250换成外网IP，把192.168.151.1换内网的网络地址）

echo 1 &gt; /proc/sys/net/ipv4/ip_forward
ifconfig eth0:1 192.168.151.1 netmask 255.255.255.0
iptables -F
iptables -F -t nat
iptables -P FORWARD DROP
iptables -A FORWARD -s 192.168.151.0/24 -j ACCEPT
iptables -A FORWARD -i eth0 -m state --state ESTABLISHED,RELATED -j ACCEPT
iptables -t nat -A POSTROUTING -o eth0 -s 192.168.151.0/24 -j SNAT --to 192.168.51.250
</code></pre>
<h2 id="iptables的目标地址转换dnat">iptables的目标地址转换（DNAT）</h2>
<h3 id="双网卡实现dnat">双网卡实现DNAT</h3>
<pre><code># 代理192.168.0.10的80端口到NAT服务器
# NAT服务器IP： 172.16.0.1  192.168.0.1
# 服务端IP： 192.168.0.10 需要将服务端的网关指向NAT服务器
# 做DNAT转发：
iptables -t nat -A PREROUTING -i ens33 -d 172.16.0.1 -p tcp --dport  80 -j DNAT --to-destination 192.168.0.10
</code></pre>
<h3 id="单网卡实现dnat">单网卡实现DNAT</h3>
<p><strong>说明</strong></p>
<pre><code>A)将本机端口转发至目标机器
iptables -t nat -A PREROUTING -p tcp -d [本地服务器主网卡绑定IP] –dport [本地端口] -j DNAT –to-destination [目标IP:目标端口]
iptables -t nat -A PREROUTING -p udp -d [本地服务器主网卡绑定IP] –dport [本地端口] -j DNAT –to-destination [目标IP:目标端口]

B)将目标机器返回的数据转发至本机
iptables -t nat -A POSTROUTING -p tcp -d [目标IP] –dport [目标端口] -j SNAT –to-source [本地服务器主网卡绑定IP]
iptables -t nat -A POSTROUTING -p udp -d [目标IP] –dport [目标端口] -j SNAT –to-source [本地服务器主网卡绑定IP]
</code></pre>
<p><strong>示例</strong></p>
<pre><code># 单网卡iptables配置路由转发，需要配置snat和dnat
# pc1 ip:192.168.23.252 
# pc2 ip:192.168.23.253 

iptables -t nat -A PREROUTING -d 192.168.23.252 -p tcp --dport 80 -j DNAT --to-destination 192.168.23.253:80
# 如果进来的route的访问目的地址是192.168.23.252并且访问的目的端口是80，就进行dnat转换，把目的地址改为192.168.23.253 ，端口还是80
 
iptables -t nat -A POSTROUTING -d 192.168.23.253 -p tcp --dport 80 -j SNAT --to 192.168.23.252
# 当FORWARD 出来后，访问的目的地址是192.168.23.253，端口是80的。进行snat地址转换，把原地址改为192.168.23.252
 
iptables -A FORWARD -o eth0 -d 192.168.23.253 -p tcp --dport 80 -j ACCEPT
# 当从eth0出去的访问目的地址是 192.168.23.253且目的端口是80的route，允许通过
 
iptables -A FORWARD -i eth0 -s 192.168.23.253 -p tcp --dport 80 -j ACCEPT
# 当从eth0进来的原地址是 192.168.23.253且目的端口是80的route，允许通过
 
# 保存规则启动iptables
service iptables save
service iptables start
</code></pre>
<h2 id="参考链接">参考链接</h2>
<p>https://blog.csdn.net/wheat_ground/article/details/78561750</p>
<p>https://blog.51cto.com/jinchuang/1947052</p>
<p>http://www.flynoc.com/help/helpview_390.html</p>
<p>https://blog.51cto.com/2333657/2157070?source=dra</p>
<p>https://blog.csdn.net/weixin_42537861/article/details/81195070</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[使用ipvs模式安装kubernetes]]></title>
        <id>https://p.guipulp.top//post/2019082701</id>
        <link href="https://p.guipulp.top//post/2019082701">
        </link>
        <updated>2019-08-27T07:11:33.000Z</updated>
        <content type="html"><![CDATA[<h3 id="安装ipvs">安装ipvs</h3>
<pre><code>cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
$ chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4

</code></pre>
<h3 id="安装ipset和ipvsadm">安装ipset和ipvsadm</h3>
<pre><code>yum install ipset
yum install ipvsadm
</code></pre>
<h3 id="安装时间服务">安装时间服务</h3>
<pre><code>$ yum install chrony -y
$ systemctl enable chronyd
$ systemctl start chronyd
$ chronyc sources
210 Number of sources = 4
MS Name/IP address         Stratum Poll Reach LastRx Last sample
===============================================================================
^+ sv1.ggsrv.de                  2   6    17    32   -823us[-1128us] +/-   98ms
^- montreal.ca.logiplex.net      2   6    17    32    -17ms[  -17ms] +/-  179ms
^- ntp6.flashdance.cx            2   6    17    32    -32ms[  -32ms] +/-  161ms
^* 119.28.183.184                2   6    33    32   +661us[ +357us] +/-   38ms
$ date
Tue Aug 27 09:28:41 CST 2019
</code></pre>
<h3 id="安装ipvs模式的kubernetes集群模板">安装ipvs模式的kubernetes集群模板</h3>
<pre><code>apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.151.30.11  # apiserver 节点内网IP
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: ydzs-master
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS  # dns类型
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: gcr.azk8s.cn/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.15.3  # k8s版本
networking:
  dnsDomain: cluster.local
  podSubnet: 192.168.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs  # kube-proxy 模式
</code></pre>
<h3 id="参考地址">参考地址</h3>
<p>https://mp.weixin.qq.com/s/vnriX2bTtnkv8i2UpLeNnA</p>
<p>https://www.qikqiak.com/post/use-kubeadm-install-kubernetes-1.15.3/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes的国内软件源配置]]></title>
        <id>https://p.guipulp.top//post/2019080801</id>
        <link href="https://p.guipulp.top//post/2019080801">
        </link>
        <updated>2019-08-08T06:40:52.000Z</updated>
        <content type="html"><![CDATA[<h2 id="ubuntu-软件源配置">ubuntu 软件源配置</h2>
<pre><code>add-apt-repository &quot;deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main&quot;
add-apt-repository &quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot;
wget -q https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg -O- | sudo apt-key add -
curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -
apt-get update
apt-get -y install apt-transport-https ca-certificates curl software-properties-common
apt-get -y install docker-ce kubeadm kubectl kubelet
systemctl enable docker kubelet &amp;&amp; systemctl start docker
</code></pre>
<h2 id="centos-软件源配置">centos 软件源配置</h2>
<pre><code># 切换到配置目录
cd /etc/yum.repos.d/
# 配置docker-ce阿里源
wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
# 配置kubernetes阿里源
cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[centos快速创建kubernetes]]></title>
        <id>https://p.guipulp.top//post/2019090701</id>
        <link href="https://p.guipulp.top//post/2019090701">
        </link>
        <updated>2019-08-07T08:22:17.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前置条件">前置条件</h2>
<ul>
<li>系统要求:64位centos7.6</li>
<li>关闭防火墙和selinux</li>
<li>关闭操作系统swap分区(使用k8s不推荐打开)</li>
</ul>
<h2 id="环境说明">环境说明</h2>
<p>本手册安装方式适用于单节点测试部署</p>
<h2 id="准备工作每个节点都需要执行">准备工作(每个节点都需要执行)</h2>
<h3 id="docker和kubernetes软件源配置">Docker和kubernetes软件源配置</h3>
<pre><code># 切换到配置目录
cd /etc/yum.repos.d/
# 配置docker-ce阿里源
wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
# 配置kubernetes阿里源
cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
</code></pre>
<h3 id="配置内核相关参数">配置内核相关参数</h3>
<pre><code>cat &lt;&lt;EOF &gt;  /etc/sysctl.d/ceph.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system
</code></pre>
<h3 id="安装相应软件包">安装相应软件包</h3>
<pre><code># 安装kubeadm kubelet kubectl
yum install docker-ce kubeadm kubectl kubelet -y

# 开机启动kubelet和docker
systemctl enable docker kubelet

# 启动docker
systemctl start docker
</code></pre>
<h2 id="部署">部署</h2>
<h3 id="安装k8s">安装k8s</h3>
<pre><code># 30.4.241.19 需要更新为当前部署节点ip地址
kubeadm init --apiserver-advertise-address 30.4.241.19 --pod-network-cidr=10.244.0.0/16 --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers --ignore-preflight-errors=Swap

# 初始化完成后按提示执行以下命令
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>
<h3 id="安装flannel">安装flannel</h3>
<pre><code>kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
</code></pre>
<h3 id="检查是否安装完成">检查是否安装完成</h3>
<pre><code>root@k8s4:~# kubectl  get nodes
NAME   STATUS   ROLES    AGE   VERSION
k8s4   Ready    master   20m   v1.14.2
root@k8s4:~# kubectl  get nodes
NAME   STATUS   ROLES    AGE   VERSION
k8s4   Ready    master   20m   v1.14.2
root@k8s4:~# kubectl  get pods --all-namespaces
NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE
kube-system   coredns-8cc96f57d-cfr4j        1/1     Running   0          20m
kube-system   coredns-8cc96f57d-stcz6        1/1     Running   0          20m
kube-system   etcd-k8s4                      1/1     Running   0          19m
kube-system   kube-apiserver-k8s4            1/1     Running   0          19m
kube-system   kube-controller-manager-k8s4   1/1     Running   0          19m
kube-system   kube-flannel-ds-amd64-k4q6q    1/1     Running   0          50s
kube-system   kube-proxy-lhjsf               1/1     Running   0          20m
kube-system   kube-scheduler-k8s4            1/1     Running   0          19m
</code></pre>
<h3 id="测试是否能正常使用">测试是否能正常使用</h3>
<pre><code># 取消节点污点,使master能被正常调度, k8s4请更改为你自有集群的nodename
kubectl  taint node k8s4 node-role.kubernetes.io/master:NoSchedule-

# 创建nginx deploy
root@k8s4:~# kubectl  create deploy nginx --image nginx
deployment.apps/nginx created

root@k8s4:~# kubectl  get pods
NAME                     READY   STATUS    RESTARTS   AGE
nginx-65f88748fd-9sk6z   1/1     Running   0          2m44s

# 暴露nginx到集群外
root@k8s4:~# kubectl  expose deploy nginx --port=80 --type=NodePort
service/nginx exposed
root@k8s4:~# kubectl  get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP        25m
nginx        NodePort    10.104.109.234   &lt;none&gt;        80:32129/TCP   5s
root@k8s4:~# curl 127.0.0.1:32129
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes NVIDIA GPU 实践]]></title>
        <id>https://p.guipulp.top//post/201986</id>
        <link href="https://p.guipulp.top//post/201986">
        </link>
        <updated>2019-08-06T03:36:20.000Z</updated>
        <content type="html"><![CDATA[<p>查看此文档时候，内容可能已经过时，但大体过程不会有什么变化，建议按照官网说明进行安装。</p>
<h3 id="前置条件">前置条件</h3>
<ol>
<li>GNU/Linux x86_64 with kernel version &gt; 3.10</li>
<li>Docker &gt;= 1.12</li>
<li>NVIDIA GPU with Architecture &gt; Fermi (2.1)</li>
<li><a href="http://www.nvidia.com/object/unix.html">NVIDIA drivers</a> ~= 361.93 (untested on older versions)</li>
</ol>
<h3 id="安装nvidia-docker">安装nvidia-docker</h3>
<h4 id="ubuntu-16041804-debian-jessiestretch">Ubuntu 16.04/18.04, Debian Jessie/Stretch</h4>
<pre><code># Add the package repositories
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
</code></pre>
<h4 id="centos-7-docker-ce-rhel-7475-docker-ce-amazon-linux-12">CentOS 7 (docker-ce), RHEL 7.4/7.5 (docker-ce), Amazon Linux 1/2</h4>
<pre><code>distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | sudo tee /etc/yum.repos.d/nvidia-docker.repo

sudo yum install -y nvidia-container-toolkit
sudo systemctl restart docker
</code></pre>
<h4 id="设置nvidia-docker-runtime">设置nvidia-docker runtime</h4>
<pre><code># vim /etc/docker/daemon.json
{
    &quot;default-runtime&quot;: &quot;nvidia&quot;,
    &quot;runtimes&quot;: {
        &quot;nvidia&quot;: {
            &quot;path&quot;: &quot;/usr/bin/nvidia-container-runtime&quot;,
            &quot;runtimeArgs&quot;: []
        }
    }
}

# restart docker
sudo systemctl restart docker
sudo systemctl restart kubelet
</code></pre>
<h4 id="测试nvidia-docker">测试nvidia-docker</h4>
<pre><code>#### Test nvidia-smi with the latest official CUDA image
$ docker run --gpus all nvidia/cuda:9.0-base nvidia-smi

# Start a GPU enabled container on two GPUs
$ docker run --gpus 2 nvidia/cuda:9.0-base nvidia-smi

# Starting a GPU enabled container on specific GPUs
$ docker run --gpus '&quot;device=1,2&quot;' nvidia/cuda:9.0-base nvidia-smi
$ docker run --gpus '&quot;device=UUID-ABCDEF,1'&quot; nvidia/cuda:9.0-base nvidia-smi

# Specifying a capability (graphics, compute, ...) for my container
# Note this is rarely if ever used this way
$ docker run --gpus all,capabilities=utility nvidia/cuda:9.0-base nvidia-smi
</code></pre>
<h3 id="安装kubernetes-插件">安装kubernetes 插件</h3>
<pre><code>kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta/nvidia-device-plugin.yml
</code></pre>
<h3 id="测试运行gpu">测试运行GPU</h3>
<pre><code># 适用于多GPU核心的带企业显卡的集群测试
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  containers:
    - name: cuda-container
      image: nvidia/cuda:9.0-devel
      resources:
        limits:
          nvidia.com/gpu: 2 # requesting 2 GPUs
    - name: digits-container
      image: nvidia/digits:6.0
      resources:
        limits:
          nvidia.com/gpu: 2 # requesting 2 GPUs
</code></pre>
<pre><code># 适用于个人GPU测试
kubectl  create job nvjob --image=nvidia/cuda:9.0-base -- nvidia-smi
</code></pre>
<h3 id="参考资料">参考资料</h3>
<p><a href="https://github.com/NVIDIA/nvidia-docker">https://github.com/NVIDIA/nvidia-docker</a></p>
<p><a href="https://github.com/NVIDIA/k8s-device-plugin">https://github.com/NVIDIA/k8s-device-plugin</a></p>
<p><a href="https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/">https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/</a></p>
<p><a href="https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)#prerequisites">https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)#prerequisites</a></p>
]]></content>
    </entry>
</feed>