<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://tomcat1009.coding.me/hexo</id>
    <title>PeterPan</title>
    <updated>2020-02-14T02:56:46.565Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://tomcat1009.coding.me/hexo"/>
    <link rel="self" href="https://tomcat1009.coding.me/hexo/atom.xml"/>
    <subtitle>贵有恒，何必三更起五更睡；最无益，只怕一日暴十寒。</subtitle>
    <logo>https://tomcat1009.coding.me/hexo/images/avatar.png</logo>
    <icon>https://tomcat1009.coding.me/hexo/favicon.ico</icon>
    <rights>All rights reserved 2020, PeterPan</rights>
    <entry>
        <title type="html"><![CDATA[kubernetes 生产环境优化]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2020010601</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2020010601">
        </link>
        <updated>2020-01-16T03:19:02.000Z</updated>
        <content type="html"><![CDATA[<h3 id="系统稳定性">系统稳定性</h3>
<h4 id="节点异常采集">节点异常采集</h4>
<blockquote>
<p>解决方案<a href="">NPD</a>，NPD可以收集节点相关的异常报错如内核错误、OOM等</p>
</blockquote>
<p><strong>部署方式</strong></p>
<pre><code>kubectl apply -f &quot;https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/node-problem-detector/npd.yaml&quot;
</code></pre>
<p><strong>测试</strong></p>
<pre><code>[root@lab1 ~]# kubectl  get events -w
0s    Warning   TaskHung   node/lab1   kernel: INFO: task docker:20744 blocked for more than 1200 seconds.
</code></pre>
<pre><code>[root@lab1 ~]# sudo sh -c &quot;echo 'kernel: INFO: task docker:20744 blocked for more than 1200 seconds.' &gt;&gt; /dev/kmsg&quot;
</code></pre>
<h4 id="节点资源信息集中监控">节点资源信息集中监控</h4>
<blockquote>
<p>采用weave试图，查看容器和节点的实时开销，支持远程排序过滤等功能</p>
</blockquote>
<p><strong>部署方式</strong></p>
<pre><code>kubectl apply -f &quot;https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d '\n')&amp;k8s-service-type=LoadBalancer&quot;
</code></pre>
<figure data-type="image" tabindex="1"><a href="https://imgchr.com/i/ljBWXq"><img src="https://s2.ax1x.com/2020/01/16/ljBWXq.md.png" alt="ljBWXq.md.png"></a></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes 设置节点异常时候POD调度时间]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2019121701</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2019121701">
        </link>
        <updated>2019-12-17T03:32:19.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>防止节点异常后，pod要很长时间才能在其他node上运行起来，导致业务故障时间过长。</p>
</blockquote>
<h3 id="配置controller-manager">配置controller manager</h3>
<p>编辑配置文件<code>/etc/kubernetes/manifests/kube-controller-manager.yaml</code></p>
<pre><code>--node-monitor-grace-period=10s \
--node-monitor-period=3s \
--node-startup-grace-period=20s \
--pod-eviction-timeout=10s \
</code></pre>
<h3 id="pod-调度过程">pod 调度过程</h3>
<p>kubernetes节点失效后pod的调度过程：</p>
<ol>
<li>
<p>Master每隔一段时间和node联系一次，判定node是否失联，这个时间周期配置项为 node-monitor-period ，默认5s</p>
</li>
<li>
<p>当node失联后一段时间后，kubernetes判定node为notready状态，这段时长的配置项为 node-monitor-grace-period ，默认40s</p>
</li>
<li>
<p>当node失联后一段时间后，kubernetes判定node为unhealthy，这段时长的配置项为 node-startup-grace-period ，默认1m0s</p>
</li>
<li>
<p>当node失联后一段时间后，kubernetes开始删除原node上的pod，这段时长配置项为 pod-eviction-timeout ，默认5m0s</p>
</li>
</ol>
<p>在应用中，想要缩短pod的重启时间，可以修改上述几个参数</p>
<p><strong>参数解释</strong></p>
<pre><code>--node-monitor-grace-period duration     Default: 40s
 	Amount of time which we allow running Node to be unresponsive before marking it unhealthy. Must be N times more than kubelet's nodeStatusUpdateFrequency, where N means number of retries allowed for kubelet to post node status.
--node-monitor-period duration     Default: 5s
 	The period for syncing NodeStatus in NodeController.
--node-startup-grace-period duration     Default: 1m0s
 	Amount of time which we allow starting Node to be unresponsive before marking it unhealthy.
--pod-eviction-timeout duration     Default: 5m0s
 	The grace period for deleting pods on failed nodes.
</code></pre>
<h3 id="参考文档">参考文档</h3>
<p>https://www.cnblogs.com/Qing-840/p/9818404.html</p>
<p>https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes master 更换ip（单节点）]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2019121102</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2019121102">
        </link>
        <updated>2019-12-11T06:44:47.000Z</updated>
        <content type="html"><![CDATA[<h3 id="问题分析">问题分析</h3>
<p>master ip地址变更以后，我们首先应该检查以下内容：</p>
<ol>
<li>
<p><code>/etc/kubernetes/manifests</code>下面的config配置文件，替换里面对应的ip</p>
</li>
<li>
<p>相关的证书文件</p>
</li>
<li>
<p>客户端文件</p>
</li>
</ol>
<h3 id="解决步骤">解决步骤</h3>
<h4 id="准备config文件">准备config文件</h4>
<blockquote>
<p>如果环境能出国网则不用进行该步骤,此文件为kubeadm.config<br>
使用该文件时候注意替换相关的API地址和端口等信息</p>
</blockquote>
<pre><code>apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 100.64.139.62
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master-2
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
kind: ClusterConfiguration
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
kubernetesVersion: v1.16.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
</code></pre>
<h4 id="修改配置文件">修改配置文件</h4>
<pre><code>[root@k8s-master-2 kubernetes]# cd /etc/kubernetes
[root@k8s-master-2 kubernetes]# find . -type f |xargs grep 100.64.139.60 |awk '{print $1}' |sort |uniq
./admin.conf:
./controller-manager.conf:
./kubelet.conf:
./manifests/etcd.yaml:
./manifests/kube-apiserver.yaml:
./scheduler.conf:
</code></pre>
<p>其中几个conf文件为kubeadm自动生成的带证书的客户端配置文件，需要修改的为<code>etcd.yaml</code>,<code>kube-apiserver.yaml</code>两个配置文件。将里面对应的ip地址修改为新的ip地址。</p>
<h4 id="生成新证书">生成新证书</h4>
<p>备份原始证书，根据<code>find</code>命令的输出，以下相关的服务证书需要更换<code>kubelt api proxy</code></p>
<pre><code># 备份原始证书
mv /etc/kubernetes/pki/apiserver.key /etc/kubernetes/pki/apiserver.key.old
mv /etc/kubernetes/pki/apiserver.crt /etc/kubernetes/pki/apiserver.crt.old
mv /etc/kubernetes/pki/apiserver-kubelet-client.crt /etc/kubernetes/pki/apiserver-kubelet-client.crt.old
mv /etc/kubernetes/pki/apiserver-kubelet-client.key /etc/kubernetes/pki/apiserver-kubelet-client.key.old
mv /etc/kubernetes/pki/front-proxy-client.crt /etc/kubernetes/pki/front-proxy-client.crt.old
mv /etc/kubernetes/pki/front-proxy-client.key /etc/kubernetes/pki/front-proxy-client.key.old

# 生成新证书
kubeadm init  phase certs apiserver --config kubeadm.config
kubeadm init  phase certs apiserver-kubelet-client --config kubeadm.config
kubeadm init  phase certs front-proxy-client --config kubeadm.config
</code></pre>
<h4 id="生成新的客户端文件">生成新的客户端文件</h4>
<pre><code>kubeadm  init phase kubeconfig admin --config kubeadm.config
kubeadm  init phase kubeconfig controller-manager --config kubeadm.config
kubeadm  init phase kubeconfig kubelet --config kubeadm.config
kubeadm  init phase kubeconfig scheduler --config kubeadm.config
</code></pre>
<h4 id="查看证书过期时间">查看证书过期时间</h4>
<pre><code>[root@k8s-master-2 pki]# kubeadm  alpha  certs check-expiration
CERTIFICATE                EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
admin.conf                 Dec 10, 2020 05:31 UTC   364d            no
apiserver                  Dec 10, 2020 05:30 UTC   364d            no
apiserver-etcd-client      Dec 10, 2020 05:31 UTC   364d            no
apiserver-kubelet-client   Dec 10, 2020 05:30 UTC   364d            no
controller-manager.conf    Dec 10, 2020 05:31 UTC   364d            no
etcd-healthcheck-client    Dec 10, 2020 05:31 UTC   364d            no
etcd-peer                  Dec 10, 2020 05:31 UTC   364d            no
etcd-server                Dec 10, 2020 05:30 UTC   364d            no
front-proxy-client         Dec 10, 2020 05:30 UTC   364d            no
scheduler.conf             Dec 10, 2020 05:31 UTC   364d            no
</code></pre>
<h4 id="重启服务">重启服务</h4>
<pre><code>service docker restart 
service kubelet restart
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[etcd故障后恢复]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2019121101</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2019121101">
        </link>
        <updated>2019-12-11T04:46:33.000Z</updated>
        <content type="html"><![CDATA[<h3 id="etcd故障描述">etcd故障描述</h3>
<p>etcd集群采用raft算法,所以当集群能够正常运行的时候一定是存活节点数大于总节点数的一半。当少于总结点数一半时，集群会处于不可用状态，这种情况下我们需要恢复集群。此处也可以使用备份进行新节点恢复。</p>
<h3 id="故障模拟">故障模拟</h3>
<ol>
<li>
<p>首先我们需要搭建一个正常的集群，搭建集群参考文档<a href="https://tomcat1009.coding.me/hexo/post/2019102501/">etcd测试集群搭建</a>。</p>
</li>
<li>
<p>根据上述文档完成集群搭建后，选择任意两个节点进行宕机操作</p>
</li>
</ol>
<pre><code>docker stop etcd2
docker stop etcd3
</code></pre>
<h3 id="恢复">恢复</h3>
<h4 id="etcd1">etcd1</h4>
<p>使用剩下的节点<code>etcd1</code>进行如下操作</p>
<pre><code># 备份数据库，注意需要备份到持久化目录中
ETCDCTL_API=3 /root/local/bin/etcdctl snapshot save snapshot.db

# 删除容器
docker rm -f etcd1

# 启动新容器，跟上参数--force-new-cluster，集群恢复后需要取消该选项
docker run --restart=always --net host -it --name etcd1 -d -v /var/etcd:/var/etcd -v /etc/localtime:/etc/localtime registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.3.10 etcd --name etcd-s1 --auto-compaction-retention=1 --max-request-bytes=33554432 --quota-backend-bytes=8589934592 --data-dir=/var/etcd/etcd-data --listen-client-urls http://0.0.0.0:2379 --listen-peer-urls http://0.0.0.0:2380 --initial-advertise-peer-urls http://172.16.10.101:2380 --advertise-client-urls http://172.16.10.101:2379,http://172.16.10.101:2380 -initial-cluster-token etcd-cluster -initial-cluster &quot;etcd-s1=http://172.16.10.101:2380,etcd-s2=http://172.16.10.102:2380,etcd-s3=http://172.16.10.103:2380&quot; -initial-cluster-state new --force-new-cluster

# 查看并添加节点，需要一个一个添加，添加后再启动相应的节点
ETCDCTL_API=3 etcdctl member list
ETCDCTL_API=3 etcdctl member add etcd-s2 --peer-urls=http://172.16.10.102:2380

# 当完成etcd2的启动后再执行etcd3节点的注册添加
ETCDCTL_API=3 etcdctl member add etcd-s3 --peer-urls=http://172.16.10.103:2380
</code></pre>
<h4 id="etcd2">etcd2</h4>
<p>启动<code>etcd2</code>,此处需要注意参数<code>existing</code></p>
<pre><code># 删除etcd2容器，并清理本地etcd数据目录
docker rm -f etcd2
rm -rf /var/etcd

# 启动etcd2新容器
docker run --restart=always --net host -it --name etcd2 -d -v /var/etcd:/var/etcd -v /etc/localtime:/etc/localtime registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.3.10 etcd --name etcd-s2  --auto-compaction-retention=1 --max-request-bytes=33554432 --quota-backend-bytes=8589934592 --data-dir=/var/etcd/etcd-data --listen-client-urls http://0.0.0.0:2379 --listen-peer-urls http://0.0.0.0:2380 --initial-advertise-peer-urls http://172.16.10.102:2380 --advertise-client-urls http://172.16.10.102:2379,http://172.16.10.102:2380 -initial-cluster-token etcd-cluster -initial-cluster &quot;etcd-s1=http://172.16.10.101:2380,etcd-s2=http://172.16.10.102:2380&quot; -initial-cluster-state existing
</code></pre>
<h4 id="etcd3">etcd3</h4>
<p>启动<code>etcd3</code>,此处需要注意参数<code>existing</code></p>
<pre><code># 删除etcd3容器，并清理本地etcd数据目录
docker rm -f etcd3
rm -rf /var/etcd

# 启动etcd3容器
docker run --restart=always --net host -it --name etcd3 -d -v /var/etcd:/var/etcd -v /etc/localtime:/etc/localtime registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.3.10 etcd --name etcd-s3  --auto-compaction-retention=1 --max-request-bytes=33554432 --quota-backend-bytes=8589934592 --data-dir=/var/etcd/etcd-data --listen-client-urls http://0.0.0.0:2379 --listen-peer-urls http://0.0.0.0:2380 --initial-advertise-peer-urls http://172.16.10.103:2380 --advertise-client-urls http://172.16.10.103:2379,http://172.16.10.103:2380 -initial-cluster-token etcd-cluster -initial-cluster &quot;etcd-s1=http://172.16.10.101:2380,etcd-s2=http://172.16.10.102:2380,etcd-s3=http://172.16.10.103:2380&quot; -initial-cluster-state existing
</code></pre>
<h3 id="参考文档">参考文档</h3>
<p>https://blog.csdn.net/yinlongfei_love/article/details/87722341</p>
<p>https://www.cnblogs.com/wangbin/p/9426644.html</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes 新增master节点]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2019112201</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2019112201">
        </link>
        <updated>2019-11-22T02:25:27.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>本方法使用于1.15之后的版本(实验版本为1.16.3)</p>
</blockquote>
<h3 id="初始化集群">初始化集群</h3>
<pre><code>[root@k8s-master-2 ~]# kubeadm init --control-plane-endpoint 100.64.139.60 --pod-network-cidr=10.244.0.0/16 --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers --upload-certs
...
kubeadm join 100.64.139.60:6443 --token 9vbxyg.dc7mupnjuznb8839 \
    --discovery-token-ca-cert-hash sha256:92fb288714ae020deba3740fab23c04e13d40f833b0ce9d0e33b3cb248dd33a5 \
    --control-plane --certificate-key 3181c80884a40d3df623564940c550a01f26e994ac94a3843c87700444ed9bee

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
&quot;kubeadm init phase upload-certs --upload-certs&quot; to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 100.64.139.60:6443 --token 9vbxyg.dc7mupnjuznb8839 \
    --discovery-token-ca-cert-hash sha256:92fb288714ae020deba3740fab23c04e13d40f833b0ce9d0e33b3cb248dd33a5
</code></pre>
<h3 id="后续新加入">后续新加入</h3>
<p><strong>node</strong></p>
<pre><code>kubeadm token create --print-join-command
</code></pre>
<p><strong>master</strong></p>
<pre><code># 子命令
kubeadm token create --print-join-command
kubeadm init phase upload-certs --upload-certs
# 单个命令全部生成
echo &quot;$(kubeadm token create --print-join-command) --control-plane --certificate-key $(kubeadm init phase upload-certs --upload-certs|tail -n 1)&quot;
</code></pre>
<h3 id="参考文档">参考文档</h3>
<p>https://www.cnblogs.com/Dev0ps/p/10980516.html</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[etcd测试集群搭建（http）]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2019102501</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2019102501">
        </link>
        <updated>2019-10-25T07:35:10.000Z</updated>
        <content type="html"><![CDATA[<h3 id="集群搭建">集群搭建</h3>
<h4 id="资源列表">资源列表</h4>
<table>
<thead>
<tr>
<th>角色</th>
<th>ip地址</th>
<th>端口</th>
</tr>
</thead>
<tbody>
<tr>
<td>etcd1</td>
<td>172.16.10.101</td>
<td>2379,2380</td>
</tr>
<tr>
<td>ectd2</td>
<td>172.16.10.102</td>
<td>2379,2380</td>
</tr>
<tr>
<td>Etcd3</td>
<td>172.16.10.103</td>
<td>2379,2380</td>
</tr>
</tbody>
</table>
<h4 id="etcd1">etcd1</h4>
<pre><code>rm -rf /var/etcd
mkdir -p /var/etcd
docker rm etcd1 -f
docker run --restart=always --net host -it --name etcd1 -d \
-v /var/etcd:/var/etcd \
-v /etc/localtime:/etc/localtime \
registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.3.10 \
etcd --name etcd-s1 \
--auto-compaction-retention=1 --max-request-bytes=33554432 --quota-backend-bytes=8589934592 \
--data-dir=/var/etcd/etcd-data \
--listen-client-urls http://0.0.0.0:2379 \
--listen-peer-urls http://0.0.0.0:2380 \
--initial-advertise-peer-urls http://172.16.10.101:2380 \
--advertise-client-urls http://172.16.10.101:2379,http://172.16.10.101:2380 \
-initial-cluster-token etcd-cluster \
-initial-cluster &quot;etcd-s1=http://172.16.10.101:2380,etcd-s2=http://172.16.10.102:2380,etcd-s3=http://172.16.10.103:2380&quot; \
-initial-cluster-state new
</code></pre>
<h4 id="etcd2">etcd2</h4>
<pre><code>rm -rf /var/etcd
mkdir -p /var/etcd
docker rm etcd2 -f
docker run --restart=always --net host -it --name etcd2 -d \
-v /var/etcd:/var/etcd \
-v /etc/localtime:/etc/localtime \
registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.3.10 \
etcd --name etcd-s2  \
--auto-compaction-retention=1 --max-request-bytes=33554432 --quota-backend-bytes=8589934592 \
--data-dir=/var/etcd/etcd-data \
--listen-client-urls http://0.0.0.0:2379 \
--listen-peer-urls http://0.0.0.0:2380 \
--initial-advertise-peer-urls http://172.16.10.102:2380 \
--advertise-client-urls http://172.16.10.102:2379,http://172.16.10.102:2380 \
-initial-cluster-token etcd-cluster \
-initial-cluster &quot;etcd-s1=http://172.16.10.101:2380,etcd-s2=http://172.16.10.102:2380,etcd-s3=http://172.16.10.103:2380&quot; \
-initial-cluster-state new
</code></pre>
<h4 id="etcd3">etcd3</h4>
<pre><code>rm -rf /var/etcd
mkdir -p /var/etcd
docker rm etcd3 -f
docker run --restart=always --net host -it --name etcd3 -d \
-v /var/etcd:/var/etcd \
-v /etc/localtime:/etc/localtime \
registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.3.10 \
etcd --name etcd-s3  \
--auto-compaction-retention=1 --max-request-bytes=33554432 --quota-backend-bytes=8589934592 \
--data-dir=/var/etcd/etcd-data \
--listen-client-urls http://0.0.0.0:2379 \
--listen-peer-urls http://0.0.0.0:2380 \
--initial-advertise-peer-urls http://172.16.10.103:2380 \
--advertise-client-urls http://172.16.10.103:2379,http://172.16.10.103:2380 \
-initial-cluster-token etcd-cluster \
-initial-cluster &quot;etcd-s1=http://172.16.10.101:2380,etcd-s2=http://172.16.10.102:2380,etcd-s3=http://172.16.10.103:2380&quot; \
-initial-cluster-state new
</code></pre>
<h3 id="检查">检查</h3>
<pre><code># 检查
ETCDCTL_API=3 etcdctl endpoint health
ETCDCTL_API=3 etcdctl member list
ETCDCTL_API=3 etcdctl put foo bar
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[使用kubeadm 新加入节点（原始token过期后）]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2019091201</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2019091201">
        </link>
        <updated>2019-09-12T04:47:00.000Z</updated>
        <content type="html"><![CDATA[<h2 id="kubeadm-join">kubeadm join</h2>
<blockquote>
<p>kubeadm init 安装完成后你会得到以下的输出，使用join指令可以新增节点到集群,此token 有效期为24小时</p>
</blockquote>
<pre><code>You should now deploy a pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join 18.16.202.35:6443 --token zr8n5j.yfkanjio0lfsupc0 --discovery-token-ca-cert-hash sha256:380b775b7f9ea362d45e4400be92adc4f71d86793ba6aae091ddb53c489d218c
</code></pre>
<h2 id="kubeadm-token">kubeadm token</h2>
<blockquote>
<p>在新节点没有拿到证书以前，新节点和api server的通信是通过token和ca的签名完成的，具体的步骤如下</p>
</blockquote>
<pre><code># 生成token
[root@node1 flannel]# kubeadm  token create
kiyfhw.xiacqbch8o8fa8qj
[root@node1 flannel]# kubeadm  token list
TOKEN                     TTL         EXPIRES                     USAGES                   DESCRIPTION   EXTRA GROUPS
gvvqwk.hn56nlsgsv11mik6   &lt;invalid&gt;   2018-10-25T14:16:06+08:00   authentication,signing   &lt;none&gt;        system:bootstrappers:kubeadm:default-node-token
kiyfhw.xiacqbch8o8fa8qj   23h         2018-10-27T06:39:24+08:00   authentication,signing   &lt;none&gt;        system:bootstrappers:kubeadm:default-node-token
</code></pre>
<pre><code># 生成ca的sha256 hash值
[root@node1 flannel]# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
5417eb1b68bd4e7a4c82aded83abc55ec91bd601e45734d6aba85de8b1ebb057
</code></pre>
<pre><code># 组装join命令
kubeadm join 18.16.202.35:6443 --token kiyfhw.xiacqbch8o8fa8qj --discovery-token-ca-cert-hash sha256:5417eb1b68bd4e7a4c82aded83abc55ec91bd601e45734d6aba85de8b1ebb057
</code></pre>
<pre><code># 一步完成以上步骤
kubeadm token create --print-join-command
</code></pre>
<pre><code># 手动生成token，完成命令打印
token=$(kubeadm token generate)
kubeadm token create $token --print-join-command --ttl=0
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes1.15.3离线安装]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2019090901</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2019090901">
        </link>
        <updated>2019-09-09T10:09:51.000Z</updated>
        <content type="html"><![CDATA[<h1 id="kubeadm-k8s集群离线安装">kubeadm k8s集群离线安装</h1>
<h2 id="下载软件包">下载软件包</h2>
<pre><code>yum install --downloadonly --downloaddir=./  kubeadm-1.15.3-0 kubelet-1.15.3-0 kubectl-1.15.3-0 docker-ce-18.06.3.ce-3.el7 keepalived ipset ipvsadm   
</code></pre>
<h2 id="下载镜像并打包">下载镜像并打包</h2>
<pre><code>kubeadm  config images pull --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers

for x in $(docker images |awk  '{print $1&quot;:&quot;$2}'|grep -v REP);do docker save -o $(echo $x|awk -F '/' '{print $3}').tar $x;done

</code></pre>
<h2 id="恢复镜像">恢复镜像</h2>
<pre><code>cd images;for x in `ls`;do docker load -i $x;done
</code></pre>
<h2 id="安装脚本">安装脚本</h2>
<p>软件包下载地址：</p>
<pre><code>链接:https://pan.baidu.com/s/1Vw0geL4Pn9_cq0rLxw7dRQ  密码:3v4g
</code></pre>
<pre><code># hostname
#hostnamectl set-hostname $HOSTNAME

# disable swap
swapoff -a

# disable selinux
sed -i &quot;s/SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config
setenforce 0

# stop and disable firewalld
systemctl stop firewalld ; systemctl disable firewalld

# add ipvs modules
cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4

# change node kernel
cat &lt;&lt;EOF &gt;  /etc/sysctl.d/ceph.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system

# install rpm
cd /root/local/rpm ; yum localinstall ./*

# load images
systemctl start docker 
systemctl enable docker kubelet
cd /root/local/images ; for x in `ls`;do docker load -i $x;done

# install kubernetes
# kubeadm init --apiserver-advertise-address 172.16.10.101 --pod-network-cidr=10.244.0.0/16 --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers
echo &quot;172.16.10.101 api.k8s.com&quot; &gt;&gt; /etc/hosts
kubeadm init --config /root/local/kubeadm.conf

# install client env
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# install addons
cd /root/local/yaml; kubectl apply -f .

# remove taint
#kubectl  taint node lab1 node-role.kubernetes.io/master:NoSchedule-
kubectl  get nodes |grep master |awk '{print $1}' |xargs -i kubectl  taint node {} node-role.kubernetes.io/master:NoSchedule-
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[etcd数据备份和恢复]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2019090503</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2019090503">
        </link>
        <updated>2019-09-05T03:46:23.000Z</updated>
        <content type="html"><![CDATA[<h2 id="对于etcd-api-v3数据备份与恢复方法">对于etcd api v3数据备份与恢复方法</h2>
<pre><code> # export ETCDCTL_API=3
</code></pre>
<pre><code> # etcdctl --endpoints localhost:2379 snapshot save snapshot.db （备份）
</code></pre>
<pre><code> # etcdctl snapshot restore snapshot.db --name m3 --data-dir=/home/etcd_data （还原）
</code></pre>
<blockquote>
<p>恢复后的文件需要修改权限为 etcd:etcd<br>
–name:重新指定一个数据目录，可以不指定，默认为 default.etcd<br>
–data-dir：指定数据目录<br>
建议使用时不指定 name 但指定 data-dir，并将 data-dir 对应于 etcd 服务中配置的 data-dir</p>
</blockquote>
<h2 id="实践方法">实践方法</h2>
<p><strong>单机备份</strong></p>
<pre><code>[root@k8s-master1 ~]# etcdctl --endpoints 127.0.0.1:2379 snapshot save snashot.db
Snapshot saved at snashot.db
[root@k8s-master1 ~]# ll
-rw-r--r--   1 root root 3756064 Apr 18 10:38 snashot.db
[root@k8s-master1 ~]#
</code></pre>
<p><strong>集群备份</strong></p>
<pre><code>[root@k8s-master1 ~]# etcdctl --endpoints=&quot;https://192.168.32.129:2379,https://192.168.32.130:2379,192.168.32.128:2379&quot; --cacert=/etc/kubernetes/cert/ca.pem --key=/etc/etcd/cert/etcd-key.pem --cert=/etc/etcd/cert/etcd.pem  snapshot save snashot1.db
Snapshot saved at snashot1.db
[root@k8s-master1 ~]#
[root@k8s-master1 ~]# ll
-rw-r--r--   1 root root 3756064 Apr 18 10:53 snashot1.db
-rw-r--r--   1 root root 3756064 Apr 18 10:38 snashot.db
</code></pre>
<p><strong>数据恢复</strong></p>
<p>做下面的操作,请慎重,有可能造成集群崩溃数据丢失.请在实验环境测试.</p>
<p>执行命令:systemctl stop etcd<br>
所有节点的etcd服务全部停止.</p>
<p>执行命令:rm -rf /var/lib/etcd/<br>
所有节点删除etcd的数据</p>
<p><strong>恢复v3的数据</strong></p>
<pre><code>[root@k8s-master1 ~]#  etcdctl --name=k8s-master1 --endpoints=&quot;https://192.168.32.128:2379&quot; --cacert=/etc/kubernetes/cert/ca.pem --key=/etc/etcd/cert/etcd-key.pem --cert=/etc/etcd/cert/etcd.pem --initial-cluster-token=etcd-cluster-1 --initial-advertise-peer-urls=https://192.168.32.128:2380 --initial-cluster=k8s-master1=https://192.168.32.128:2380,k8s-master2=https://192.168.32.129:2380,k8s-master3=https://192.168.32.130:2380 --data-dir=/var/lib/etcd snapshot restore snashot1.db
2019-04-18 13:43:42.570882 I | mvcc: restore compact to 148651
2019-04-18 13:43:42.584194 I | etcdserver/membership: added member 4c99f52323a3e391 [https://192.168.32.129:2380] to cluster 2a0978507970d828
2019-04-18 13:43:42.584224 I | etcdserver/membership: added member 5a74b01f28ece933 [https://192.168.32.128:2380] to cluster 2a0978507970d828
2019-04-18 13:43:42.584234 I | etcdserver/membership: added member b29b94ace458096d [https://192.168.32.130:2380] to cluster 2a0978507970d828
</code></pre>
<pre><code>[root@k8s-master2 ~]# etcdctl --name=k8s-master2 --endpoints=&quot;https://192.168.32.129:2379&quot; --cacert=/etc/kubernetes/cert/ca.pem --key=/etc/etcd/cert/etcd-key.pem --cert=/etc/etcd/cert/etcd.pem --initial-cluster-token=etcd-cluster-1 --initial-advertise-peer-urls=https://192.168.32.129:2380 --initial-cluster=k8s-master1=https://192.168.32.128:2380,k8s-master2=https://192.168.32.129:2380,k8s-master3=https://192.168.32.130:2380 --data-dir=/var/lib/etcd snapshot restore snashot1.db
2019-04-18 13:43:56.313096 I | mvcc: restore compact to 148651
2019-04-18 13:43:56.324779 I | etcdserver/membership: added member 4c99f52323a3e391 [https://192.168.32.129:2380] to cluster 2a0978507970d828
2019-04-18 13:43:56.324806 I | etcdserver/membership: added member 5a74b01f28ece933 [https://192.168.32.128:2380] to cluster 2a0978507970d828
2019-04-18 13:43:56.324819 I | etcdserver/membership: added member b29b94ace458096d [https://192.168.32.130:2380] to cluster 2a0978507970d828
</code></pre>
<pre><code>[root@k8s-master3 ~]# etcdctl --name=k8s-master3 --endpoints=&quot;https://192.168.32.130:2379&quot; --cacert=/etc/kubernetes/cert/ca.pem --key=/etc/etcd/cert/etcd-key.pem --cert=/etc/etcd/cert/etcd.pem --initial-cluster-token=etcd-cluster-1 --initial-advertise-peer-urls=https://192.168.32.130:2380 --initial-cluster=k8s-master1=https://192.168.32.128:2380,k8s-master2=https://192.168.32.129:2380,k8s-master3=https://192.168.32.130:2380 --data-dir=/var/lib/etcd snapshot restore snashot1.db
2019-04-18 13:44:10.643115 I | mvcc: restore compact to 148651
2019-04-18 13:44:10.649920 I | etcdserver/membership: added member 4c99f52323a3e391 [https://192.168.32.129:2380] to cluster 2a0978507970d828
2019-04-18 13:44:10.649957 I | etcdserver/membership: added member 5a74b01f28ece933 [https://192.168.32.128:2380] to cluster 2a0978507970d828
2019-04-18 13:44:10.649973 I | etcdserver/membership: added member b29b94ace458096d [https://192.168.32.130:2380] to cluster 2a0978507970d828
</code></pre>
<p>服务起不来</p>
<pre><code>[root@k8s-master1 ~]# tail -n 30 /var/log/messages
Apr 18 13:46:41 k8s-master1 systemd: Starting Etcd Server...
Apr 18 13:46:41 k8s-master1 etcd: etcd Version: 3.3.7
Apr 18 13:46:41 k8s-master1 etcd: Git SHA: 56536de55
Apr 18 13:46:41 k8s-master1 etcd: Go Version: go1.9.6
Apr 18 13:46:41 k8s-master1 etcd: Go OS/Arch: linux/amd64
Apr 18 13:46:41 k8s-master1 etcd: setting maximum number of CPUs to 1, total number of available CPUs is 1
Apr 18 13:46:41 k8s-master1 etcd: error listing data dir: /var/lib/etcd
Apr 18 13:46:41 k8s-master1 systemd: etcd.service: main process exited, code=exited, status=1/FAILURE
Apr 18 13:46:41 k8s-master1 systemd: Failed to start Etcd Server.
Apr 18 13:46:41 k8s-master1 systemd: Unit etcd.service entered failed state.
Apr 18 13:46:41 k8s-master1 systemd: etcd.service failed.
Apr 18 13:46:41 k8s-master1 flanneld: timed out
Apr 18 13:46:41 k8s-master1 flanneld: E0418 13:46:41.858283   63943 main.go:349] Couldn't fetch network config: client: etcd cluster is unavailable or misconfigured; error #0: EOF
Apr 18 13:46:41 k8s-master1 flanneld: ; error #1: EOF
Apr 18 13:46:41 k8s-master1 flanneld: ; error #2: EOF
[root@k8s-master1 ~]#
</code></pre>
<p>修改数据目录权限,默认是root:root<br>
chown -R etcd:etcd /var/lib/etcd<br>
恢复正常.</p>
<pre><code>[root@k8s-master1 ~]# etcdctl member list
4c99f52323a3e391, started, k8s-master2, https://192.168.32.129:2380, https://192.168.32.129:2379
5a74b01f28ece933, started, k8s-master1, https://192.168.32.128:2380, https://192.168.32.128:2379
b29b94ace458096d, started, k8s-master3, https://192.168.32.130:2380, https://192.168.32.130:2379
[root@k8s-master1 ~]#
</code></pre>
<p>可以看到v3的数据恢复成功.</p>
<h2 id="参考地址">参考地址</h2>
<p>https://blog.csdn.net/liukuan73/article/details/78986652</p>
<p>https://blog.51cto.com/goome/2380854</p>
<p>https://yq.aliyun.com/articles/336781</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Centos7下的日志切割]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2019090502</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2019090502">
        </link>
        <updated>2019-09-05T03:27:49.000Z</updated>
        <content type="html"><![CDATA[<h2 id="logrotate">logrotate</h2>
<p>/etc/logrotate.conf 是 Logrotate 工具的一个配置文件，这个工具用来自动切割系统日志，Logrotate 是基于 cron 来运行的，如下：</p>
<pre><code>[root@localhost ~]$ cat /etc/cron.daily/logrotate    # 每天运行
#!/bin/sh

/usr/sbin/logrotate /etc/logrotate.conf &gt;/dev/null 2&gt;&amp;1
EXITVALUE=$?
if [ $EXITVALUE != 0 ]; then
    /usr/bin/logger -t logrotate &quot;ALERT exited abnormally with [$EXITVALUE]&quot;
fi
exit 0
</code></pre>
<p>实际运行时，Logrotate 会调用配置文件 /etc/logrotate.conf ，默认的配置如下：</p>
<pre><code>[root@localhost ~]$ cat /etc/logrotate.conf 

weekly                      # 每周切割一次
rotate 4                    # 只保留四份文件
create                      # 切割后会创建一个新的文件
dateext                     # 指定切割文件的后缀名，这里以日期为后缀名
include /etc/logrotate.d    # 包含其他配置文件的目录

/var/log/wtmp {             # 对哪个文件进行切割
    monthly                 # 每个月切割一次
    create 0664 root utmp   # 指定创建的新文件的权限，属主，属组
        minsize 1M          # 文件容量超过这个值时才进行切割
    rotate 1                # 只保留一份文件
}

/var/log/btmp {
    missingok
    monthly
    create 0600 root utmp
    rotate 1
}
</code></pre>
<h2 id="参考链接">参考链接</h2>
<p>https://blog.51cto.com/linuxblind/1269458</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes里面的GC]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2019090501</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2019090501">
        </link>
        <updated>2019-09-05T03:02:08.000Z</updated>
        <content type="html"><![CDATA[<h2 id="什么是gc">什么是GC</h2>
<p>GC 是 Garbage Collector 的简称。从功能层面上来说，它和编程语言当中的「GC」 基本上是一样的。它清理 Kubernetes 中「符合特定条件」的 Resource Object。</p>
<p>Kubelet的GC功能将清理未使用的image和container。Kubelet每分钟对container执行一次GC，每5分钟对image执行一次GC。不建议使用外部垃圾收集工具，因为这些工具可能破坏Kubelet。</p>
<h2 id="kubernetes里面的基本常识">kubernetes里面的基本常识</h2>
<ul>
<li>在 k8s 中，你可以认为万物皆资源，很多逻辑的操作对象都是 Resource Object。</li>
<li>Kubernetes 在不同的 Resource Objects 中维护一定的「从属关系」。内置的 Resource Objects 一般会默认在一个 Resource Object 和它的创建者之间建立一个「从属关系」。</li>
<li>你也可以利用<code>ObjectMeta.OwnerReferences</code>自由的去给两个 Resource Object 建立关系，前提是被建立关系的两个对象必须在一个 Namespace 下。</li>
<li>K8s 实现了一种「Cascading deletion」（级联删除）的机制，它利用已经建立的「从属关系」进行资源对象的清理工作。例如，当一个 dependent 资源的 owner 已经被删除或者不存在的时候，从某种角度就可以判定，这个 dependent 的对象已经是异常（无人管辖）的了，需要进行清理。而 「cascading deletion」则是被 k8s 中的一个 controller 组件实现的：<code>Garbage Collector</code></li>
<li>k8s 是通过 <code>Garbage Collector</code> 和 <code>ownerReference</code> 一起配合实现了「垃圾回收」的功能。</li>
</ul>
<h2 id="kubernetes的gc组成">kubernetes的gc组成</h2>
<figure data-type="image" tabindex="1"><img src="http://img.blog.csdn.net/20161225115013410?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvV2FsdG9uV2FuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="kubernetes GC architecture in v1.3"></figure>
<p>一个 Garbage Collector 通常由三部分实现：</p>
<ul>
<li>Scanner： 它负责收集目前系统中已存在的 Resource，并且周期性的将这些资源对象放入一个队列中，等待处理（检测是否要对某一个Resource Object 进行 GC 操作）</li>
<li>Garbage Processor: Garbage Processor 由两部分组成
<ul>
<li>
<p>Dirty Queue： Scanner 会将周期性扫描到的 Resource Object 放入这个队列中等待处理</p>
</li>
<li>
<p>Worker：worker 负责从这个队列中取出元素进行处理</p>
<ul>
<li>
<p>检查 Object 的 metaData 部分，查看<code>ownerReference</code>字段是否为空</p>
<ul>
<li>
<p>如果为空，则本次处理结束</p>
</li>
<li>
<p>如果不为空，检测<code>ownerReference</code>字段内标识的 Owner Resource Object是否存在</p>
<ul>
<li>存在：则本次处理结束</li>
<li>不存在：删除这个 Object</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Propagator： Propagator 由三个部分构成
<ul>
<li>
<p>EventQueue：负责存储 k8s 中资源对象的事件（Eg：ADD，UPDATE，DELETE）</p>
</li>
<li>
<p>DAG(有向无环图)：负责存储 k8s 中所有资源对象的「owner-dependent」 关系</p>
</li>
<li>
<p>Worker：从 EventQueue 中，取出资源对象的事件，根据事件的类型会采取以下两种操作</p>
<ul>
<li>ADD/UPDATE: 将该事件对应的资源对象加入 DAG，且如果该对象有 owner 且 owner 不在 DAG 中，将它同时加入 Garbage Processor 的 Dirty Queue 中</li>
<li>DELETE：将该事件对应的资源对象从 DAG 中删除，并且将其「管辖」的对象（只向下寻找一级，如删除 Deployment，那么只操作 ReplicaSet ）加入 Garbage Processor 的 Dirty Queue 中</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>其实，在有了 Scanner 和 Garbage Processor 之后，Garbage Collector 就已经能够实现「垃圾回收」的功能了。但是有一个明显的问题：Scanner 的扫描频率设置多少好呢？太长了，k8s 内部就会积累过多的「废弃资源」；太短了，尤其是在集群内部资源对象较多的时候，频繁的拉取信息对 API-Server 也是一个不小的压力。</p>
<p>k8s 作为一个分布式的服务编排系统，其内部执行任何一项逻辑或者行为，都依赖一种机制：「事件驱动」。说的简单点，k8s 中一些看起来「自动」的行为，其实都是由一些神秘的「力量」在驱动着。而这个「力量」就是我们所说的「Event」。任意一个 Resource Object 发生变动的时候（新建，更新，删除），都会触发一个 k8s 的事件（Event），这个事件在 k8s 的内部是公开的，也就是说，我们可以在任意一个地方监听这些事件。</p>
<p>总的来说，无论是「事件的监听机制」还是「周期性访问 API-Server 批量获取 Resource Object 信息」，其目的都是为了能够掌握 Resource Object 的最新信息。两者是各有优势的：</p>
<ol>
<li>批量拉取：一次性拉取所有的 Resource Object，全面</li>
<li>监听 Resource 的 Event：实时性强， 且对 API—SERVER 不会造成太大的压力</li>
</ol>
<p>综上所述，在实现 Garbage Collector 的过程中，k8s 向其添加了一个「增强型」的组件：Propagator</p>
<p>在有了 Propagator 的加入之后，我们完全可以仅在 GC 开始运行的时候，让 Scanner 扫描一下系统中所有的 Object，然后将这些信息传递给 Propagator 和 Dirty Queue。只要 DAG 一建立起来之后，那么 Scanner 其实就没有再工作的必要了。「事件驱动」的机制提供了一种增量的方式让 GC 来监控 k8s 集群内部的资源对象变化情况。</p>
<h2 id="参考地址">参考地址</h2>
<p>https://mp.weixin.qq.com/s/6b5jdDkvmtywvcRa4MMjQA</p>
<p>https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go</p>
<p>https://yq.aliyun.com/articles/679728</p>
<p>https://zhuanlan.zhihu.com/p/50101300</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[linux cgroups简介]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2019090202</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2019090202">
        </link>
        <updated>2019-09-02T07:54:27.000Z</updated>
        <content type="html"><![CDATA[<p>https://blog.csdn.net/ahilll/article/details/82109008<br>
https://blog.csdn.net/chenleiking/article/details/87988851</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes 清理孤儿POD]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2019090201</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2019090201">
        </link>
        <updated>2019-09-02T07:53:25.000Z</updated>
        <content type="html"><![CDATA[<h3 id="孤儿pod的产生">孤儿pod的产生</h3>
<p>节点OOM以后或者节点异常崩溃的情况下，pod未能被正常的清理而导致的孤儿进程。</p>
<p><strong>提示如下</strong></p>
<pre><code>Orphaned pod found - but volume paths are still present on disk
</code></pre>
<p><strong>进入k8s的pod目录</strong></p>
<pre><code>cd /var/lib/kubelet/pods/
</code></pre>
<h3 id="解决问题">解决问题</h3>
<p>过滤日志中的孤儿pod，删除pod。</p>
<pre><code class="language-js">#!/bin/bash
num=$(grep &quot;errors similar to this. Turn up verbosity to see them.&quot;  /var/log/messages |tail -1 | awk '{print $12}' |sed 's/&quot;//g')
echo $num

while [ $num ]
do
   [ -d &quot;/var/lib/kubelet/pods/${num}&quot; ] &amp;&amp; rm -rf /var/lib/kubelet/pods/${num}

  sleep 2s
  num=$(grep &quot;errors similar to this. Turn up verbosity to see them.&quot;  /var/log/messages |tail -1 | awk '{print $12}' |sed 's/&quot;//g')
  [ -d &quot;/var/lib/kubelet/pods/${num}&quot; ] || num=

  echo &quot;$num remaining&quot;

done
</code></pre>
<p>但是这个方法有一定的<code>危险性</code>，还不确认是否有<code>数据丢失</code>的风险，如果可以确认，再执行。</p>
<p>如果已经挂载了PVC等相关存储，先执行umount再执行删除。</p>
<p>再去查看日志，就会发现syslog不会再刷类似的日志了。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes资源预留]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2019083002</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2019083002">
        </link>
        <updated>2019-08-30T09:25:37.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>下面内容还处于测试阶段，生产上是否能保证集群稳定暂时还不清楚。😁😁</p>
</blockquote>
<h2 id="事故">事故</h2>
<p>今天我们的开发环境由于java应用内存抢占原因导致k8s集群worker节点全部宕机，主要原因是程序和资源没进行限制规划，且kubelet也没配置资源预留，那host上所有资源都是可以给pod调配使用的，这样就引起集群雪崩效应，比如集群内有一台上跑的pod没做resource limt导致占用资源过大导致将宿主机压死了，此时这个节点在kubernetes内就是一个no ready的状态了，kubernetes会将这台host上所有的pod在其他节点上重建，也就意味着那个有问题的pod重新跑在其他正常的节点上，将另外正常的节点压跨。循怀下去直到集群内所有主机都挂了，这就是集群雪崩效应。</p>
<h2 id="解决办法">解决办法</h2>
<p>在kubernetes中可以通过给kubelet配置参数预留资源给系统进程和kubernetes进程保证它们稳定运行。目前能实现到cpu、memory、ephemeral-storage层面的资源预留。<br>
重点提两点<br>
cpu：cpu是配置cpu shares实际上对应的是cpu的优先级，简单来说，这个在cpu繁忙时，它能有更高优先级获取更多cpu资源。</p>
<p>ephemeral-storage是kubernetes1.8开始引入的一个资源限制的对象，kubernetes 1.10版本中kubelet默认已经打开的了,主要是用于对本地临时存储使用空间大小的限制，如对pod的empty dir、/var/lib/kubelet、日志、容器可读写层的使用大小的限制。</p>
<h2 id="配置">配置</h2>
<h3 id="基本概念">基本概念</h3>
<p>在讲配置之前我们先了解几个概念：</p>
<p>Node capacity：节点总共的资源<br>
kube-reserved：给kubernetes进程预留的资源<br>
system-reserved：给操作系统预留的资源<br>
eviction-threshold：kubelet eviction的阀值<br>
allocatable：留给pod使用的资源</p>
<pre><code>node_allocatable=Node_capacity-(kube-reserved+system-reserved+hard-eviction)
</code></pre>
<pre><code>      Node Capacity
---------------------------
|     kube-reserved       |
|-------------------------|
|     system-reserved     |
|-------------------------|
|    eviction-threshold   |
|-------------------------|
|                         |
|      allocatable        |
|   (available for pods)  |
|                         |
|                         |
---------------------------
</code></pre>
<p>Kubernetes 节点上的 <code>Allocatable</code> 被定义为 pod 可用计算资源量。调度器不会超额申请 <code>Allocatable</code>。目前支持 <code>CPU</code>, <code>memory</code> 和 <code>storage</code> 这几个参数。</p>
<p>Node Allocatable 暴露为 API 中 <code>v1.Node</code> 对象的一部分，也是 CLI 中 <code>kubectl describe node</code> 的一部分。</p>
<p>在 <code>kubelet</code> 中，可以为两类系统守护进程预留资源。</p>
<h3 id="使用kubelet参数进行限制">使用kubelet参数进行限制</h3>
<p>此方法适用于老版本的kubernetes集群，在新版本(1.11之前)中已经不适用了。</p>
<p>https://k8smeetup.github.io/docs/tasks/administer-cluster/reserve-compute-resources/</p>
<p>https://www.bladewan.com/2018/01/26/k8s_resource_resver/</p>
<h3 id="使用kubelet-config进行限制">使用kubelet config进行限制</h3>
<p>kubelet 较新的版本都采用kubelet config对集群的kubelet进行配置，此处采用静态配置方式，当然也可以使用动态配置方式。</p>
<pre><code># 编辑文档kubelet config 文件
vim /var/lib/kubelet/config
</code></pre>
<p><strong>配置资源预留</strong></p>
<pre><code># 找到enforceNodeAllocatable 注释掉
#enforceNodeAllocatable:
#- pods

# 添加以下内容，系统和kubelet均预留CPU500m内存500Mi磁盘5G
systemReserved:
  cpu: &quot;500m&quot;
  memory: &quot;500Mi&quot;
  ephemeral-storage: &quot;5Gi&quot;
kubeReserved:
  cpu: &quot;500m&quot;
  memory: &quot;500Mi&quot;
  ephemeral-storage: &quot;5Gi&quot;
systemReservedCgroup: /system.slice
kubeReservedCgroup: /kubelet.service
EnforceNodeAllocatable:
- pods
- kube-reserved
- system-reserved

</code></pre>
<p><strong>配置软驱逐（默认为硬驱逐）</strong></p>
<p>软驱逐有资源驱逐等待时间，硬驱逐为立刻驱逐。</p>
<pre><code># evictionHard 注释掉，并在后面新增以下内容

#evictionHard:
EvictionSoft:
  imagefs.available: 15%
  memory.available: 10%
  nodefs.available: 10%
  nodefs.inodesFree: 5%
EvictionSoftGracePeriod:
  memory.available: &quot;5m&quot;
  nodefs.available: &quot;2m&quot;
  nodefs.inodesFree: &quot;2m&quot;
  imagefs.available: &quot;2m&quot;
</code></pre>
<pre><code># 如果你使用的cgroup driver是croup还需要进行以下操作
# cpuset和hugetlb subsystem是默认没有初始化system.slice手动创建,
mkdir -p /sys/fs/cgroup/hugetlb/system.slice/kubelet.service/
mkdir -p /sys/fs/cgroup/cpuset/system.slice/kubelet.service/

# 配置在kubelet中避免重启失效
ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/cpuset/system.slice/kubelet.service
ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/hugetlb/system.slice/kubelet.service
</code></pre>
<p><strong>重启kubelet</strong></p>
<pre><code>service kubelet restart
</code></pre>
<h2 id="验证">验证</h2>
<pre><code>[root@m3 pki]# kubectl  describe node m1
Name:               m1
Roles:              master
Labels:             beta.kubernetes.io/arch=amd64
....
CreationTimestamp:  Mon, 26 Aug 2019 20:35:35 -0400
...
Addresses:
  InternalIP:  172.27.100.13
  Hostname:    m1
Capacity:
 cpu:                4
 ephemeral-storage:  36678148Ki
 hugepages-1Gi:      0
 hugepages-2Mi:      0
 memory:             8010576Ki
 pods:               110
Allocatable:
 cpu:                3
 ephemeral-storage:  23065162901
 hugepages-1Gi:      0
 hugepages-2Mi:      0
 memory:             6884176Ki
 pods:               110
</code></pre>
<ul>
<li>可以看到预留后，可用CPU为3，不预留为4，内存是一样的计算方式，此处预留了1G（500Mi+500Mi）</li>
</ul>
<h2 id="参考文档">参考文档</h2>
<p>https://github.com/rootsongjc/qa/issues/3</p>
<p>https://cloud.tencent.com/developer/article/1097002</p>
<p>https://blog.csdn.net/ahilll/article/details/82109008</p>
<p>http://dockone.io/article/4797</p>
<p>https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/</p>
<p>https://kubernetes.io/zh/docs/tasks/administer-cluster/kubelet-config-file/</p>
<p>https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go</p>
<p>https://www.bladewan.com/2018/01/26/k8s_resource_resver/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes集群证书过期之后]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2019083001</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2019083001">
        </link>
        <updated>2019-08-30T08:50:14.000Z</updated>
        <content type="html"><![CDATA[<h2 id="步骤">步骤</h2>
<p>如果有多master，需要在每个master上进行以下操作。</p>
<p><strong>需要进行以下步骤</strong></p>
<ol>
<li>重新生成证书</li>
<li>重新生成对应的配置文件</li>
<li>重启docker 和 kubelet</li>
<li>拷贝kubectl 客户端文件</li>
</ol>
<pre><code>[root@lab1 local]# kubeadm alpha certs renew all --config kubeadm.conf
...
[root@lab1 local]# mv /etc/kubernetes/*.conf ~/.
[root@lab1 local]# kubeadm init phase kubeconfig all --config kubeadm.conf
...
[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;
[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file
[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file
[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file
[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file
[root@lab1 local]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[root@lab1 local]# sudo systemctl restart docker; sudo systemctl restart kubelet
</code></pre>
<blockquote>
<p>如果遇见static pod 启动不了的情况可以采用以下方式处理</p>
</blockquote>
<pre><code>docker ps -a |grep -i exit |awk '{print $1}'|xargs docker rm
</code></pre>
<h2 id="参考地址">参考地址</h2>
<p>https://stackoverflow.com/questions/56320930/renew-kubernetes-pki-after-expired</p>
<p>https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init-phase/</p>
<p>https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[iptables的四表五链]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2019082802</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2019082802">
        </link>
        <updated>2019-08-28T04:44:56.000Z</updated>
        <content type="html"><![CDATA[<h2 id="netfilter介绍">netfilter介绍</h2>
<ul>
<li>它是iptables的主要的工作模块，位于内核中，在网络层的五个位置（也就是防火墙四表五链中的五链）注册了一些钩子函数，用来抓取数据包；</li>
<li>把数据包的信息拿出来匹配各个链位置在对应表中的规则；</li>
<li>匹配之后，进行相应的处理ACCEPT、DROP等等；</li>
</ul>
<h3 id="netfilter和iptables的关系图">netfilter和iptables的关系图</h3>
<figure data-type="image" tabindex="1"><img src="https://img.mubu.com/document_image/e0653c59-e432-40d9-a8bd-38558599a4da-1235096.jpg" alt="img"></figure>
<h2 id="四表五链">四表五链</h2>
<ul>
<li>链就是位置：共有五个，进路由(PREROUTING)、进系统(INPUT) 、转发(FORWARD)、出系统(OUTPUT)、出路由(POSTROUTING)；</li>
<li>表就是存储的规则；数据包到了该链处，会去对应表中查询设置的规则，然后决定是否放行、丢弃、转发还是修改等等操作。</li>
</ul>
<h3 id="四表">四表</h3>
<ul>
<li>filter表——过滤数据包</li>
<li>Nat表——用于网络地址转换（IP、端口）</li>
<li>Mangle表——修改数据包的服务类型、TTL、并且可以配置路由实现QOS</li>
<li>Raw表——决定数据包是否被状态跟踪机制处理</li>
</ul>
<h3 id="五链">五链</h3>
<ul>
<li>INPUT链——进来的数据包应用此规则链中的策略</li>
<li>OUTPUT链——外出的数据包应用此规则链中的策略</li>
<li>FORWARD链——转发数据包时应用此规则链中的策略</li>
<li>PREROUTING链——对数据包作路由选择前应用此链中的规则（所有的数据包进来的时侯都先由这个链处理）</li>
<li>POSTROUTING链——对数据包作路由选择后应用此链中的规则（所有的数据包出来的时侯都先由这个链处理）</li>
</ul>
<h3 id="链表关系图">链表关系图</h3>
<figure data-type="image" tabindex="2"><img src="http://peterpan.yunsean.com/iptables01.png" alt="image-20190828125115128"></figure>
<h3 id="数据处理流程">数据处理流程</h3>
<p>我们结合下面的图举个例子：<br>
假如有数据包从Network IN要通过iptables，数据包流向如下：<br>
1.Network IN数据包到达服务器的网络接口<br>
2.进入raw表的 PREROUTING 链，这个链的作用是决定数据包是否被状态跟踪。<br>
进入 mangle 表的 PREROUTING 链，在此可以修改数据包，比如 TOS 等。<br>
进入 nat 表的 PREROUTING 链，可以在此做DNAT(目标地址转换)<br>
3.决定路由，查看目标地址是交给本地主机还是转发给其它主机。</p>
<p>4.到这里分两种情况，一种情况是数据包要转发给其它主机（一般情况下它是在担任网关服务器），数据包会依次经过：<br>
5.进入 mangle 表的 FORWARD 链，<br>
进入 filter 表的 FORWARD 链，在这里我们可以对所有转发的数据包进行过滤。<br>
6.进入 mangle 表的 POSTROUTING 链<br>
进入 nat 表的 POSTROUTING 链，在这里一般都是用来做 SNAT （源地址转换）<br>
7.数据包流出网络接口，发往network out。</p>
<p>8.另一种情况，数据包的目标地址就是发给本地主机的，它会依次穿过：<br>
9.进入 mangle 表的 INPUT 链，<br>
进入 filter 表的 INPUT 链，在这里我们可以对流入的所有数据包进行过滤，<br>
数据包交给本地主机的应用程序进行处理。<br>
10.应用程序处理完毕后发送的数据包进行路由发送决定。<br>
11.进入 raw 表的 OUTPUT 链。<br>
进入 mangle 表的 OUTPUT 链，<br>
进入 nat 表的 OUTPUT 链，<br>
进入 filter 表的 OUTPUT 链。<br>
12.进入 mangle 表的 POSTROUTING 链，<br>
进入 nat 表的 POSTROUTING 链。<br>
13.进入出去的网络接口，发送往network out。</p>
<p>所以，如果我们要过滤控制包的进出，只需要把input chain和forward chain这两个关口把控住就好了，这也是我们需要把input chain和forward chain默认设置为drop的原因。</p>
<ul>
<li>
<p>图一</p>
<figure data-type="image" tabindex="3"><img src="https://img.mubu.com/document_image/c1b9ef44-0c8d-4818-a18b-83b527e8d0e4-1235096.jpg" alt="img"></figure>
</li>
<li>
<p>图二</p>
<figure data-type="image" tabindex="4"><img src="https://img.mubu.com/document_image/bcb89f30-1a57-4967-9a41-3cb6e2c8a5bc-1235096.jpg" alt="img"></figure>
</li>
</ul>
<h2 id="iptables操作">iptables操作</h2>
<pre><code>iptables [-t 表名] 选项 [链名] [条件] [-j 控制类型]

-P 设置默认策略:iptables
 
-P INPUT (DROP|ACCEPT)
 
-F 清空规则链
 
-L 查看规则链
 
-A 在规则链的末尾加入新规则
 
-I num 在规则链的头部加入新规则
 
-D num 删除某一条规则
 
-s 匹配来源地址IP/MASK，加叹号&quot;!&quot;表示除这个IP外。
 
-d 匹配目标地址
 
-i 网卡名称 匹配从这块网卡流入的数据
 
-o 网卡名称 匹配从这块网卡流出的数据
 
-p 匹配协议,如tcp,udp,icmp
 
--dport num 匹配目标端口号
 
--sport num 匹配来源端口号
</code></pre>
<h3 id="常用iptables的语句">常用iptables的语句</h3>
<pre><code>1. 删除已有规则
在开始创建iptables规则之前，你也许需要删除已有规则。命令如下：
iptables -F
(or)
iptables –flush
查看已有规则
iptables -nL
 
 
2.设置链的默认策略
链的默认政策设置为”ACCEPT”（接受），若要将INPUT,FORWARD,OUTPUT链设置成”DROP”（拒绝），命令如下：
iptables -P INPUT DROP
iptables -P FORWARD DROP
iptables -P OUTPUT DROP
当INPUT链和OUTPUT链都设置成DROP时，对于每一个防火墙规则，我们都应该定义两个规则。例如：一个传入另一个传出。在下面所有的例子中，由于我们已将DROP设置成INPUT链和OUTPUT链的默认策略，每种情况我们都将制定两条规则。
当然，如果你相信你的内部用户,则可以省略上面的最后一行。例如：默认不丢弃所有出站的数据包。在这种情况下,对于每一个防火墙规则要求,你只需要制定一个规则——只对进站的数据包制定规则。
 
 
3. 阻止指定IP地址
例：丢弃来自IP地址x.x.x.x的包
BLOCK_THIS_IP=&quot;x.x.x.x&quot;
iptables -A INPUT -s &quot;$BLOCK_THIS_IP&quot; -j DROP
注：当你在log里发现来自某ip地址的异常记录，可以通过此命令暂时阻止该地址的访问以做更深入分析
例：阻止来自IP地址x.x.x.x eth0 tcp的包
iptables -A INPUT -i eth0 -s &quot;$BLOCK_THIS_IP&quot; -j DROP
iptables -A INPUT -i eth0 -p tcp -s &quot;$BLOCK_THIS_IP&quot; -j DROP
 
 
4. 允许所有SSH的连接请求
例：允许所有来自外部的SSH连接请求，即只允许进入eth0接口，并且目标端口为22的数据包
iptables -A INPUT -i eth0 -p tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT
 
 
5. 仅允许来自指定网络的SSH连接请求
例：仅允许来自于192.168.100.0/24域的用户的ssh连接请求
iptables -A INPUT -i eth0 -p tcp -s 192.168.100.0/24 --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT
 
 
6.允许http和https的连接请求
例：允许所有来自web - http的连接请求
iptables -A INPUT -i eth0 -p tcp --dport 80 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 80 -m state --state ESTABLISHED -j ACCEPT
例：允许所有来自web - https的连接请求
iptables -A INPUT -i eth0 -p tcp --dport 443 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 443 -m state --state ESTABLISHED -j ACCEPT
 
 
7. 使用multiport 将多个规则结合在一起
允许多个端口从外界连入，除了为每个端口都写一条独立的规则外，我们可以用multiport将其组合成一条规则。如下所示：
例：允许所有ssh,http,https的流量访问
iptables -A INPUT -i eth0 -p tcp -m multiport --dports 22,80,443 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp -m multiport --sports 22,80,443 -m state --state ESTABLISHED -j ACCEPT
 
 
8. 允许从本地发起的SSH请求
iptables -A OUTPUT -o eth0 -p tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A INPUT -i eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT
请注意,这与允许ssh连入的规则略有不同。本例在OUTPUT链上，我们允许NEW和ESTABLISHED状态。在INPUT链上，我们只允许ESTABLISHED状态。ssh连入的规则与之相反。
 
 
9. 仅允许从本地发起到一个指定的网络域的SSH请求
例：仅允许从内部连接到网域192.168.100.0/24
iptables -A OUTPUT -o eth0 -p tcp -d 192.168.100.0/24 --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A INPUT -i eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT
 
 
10. 允许从本地发起的HTTPS连接请求
下面的规则允许输出安全的网络流量。如果你想允许用户访问互联网，这是非常有必要的。在服务器上，这些规则能让你使用wget从外部下载一些文件
iptables -A OUTPUT -o eth0 -p tcp --dport 443 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A INPUT -i eth0 -p tcp --sport 443 -m state --state ESTABLISHED -j ACCEPT
注：对于HTTP web流量的外联请求，只需要将上述命令中的端口从443改成80即可。
 
 
11. 负载平衡传入的网络流量
使用iptables可以实现传入web流量的负载均衡，我们可以传入web流量负载平衡使用iptables防火墙规则。
例：使用iptables nth将HTTPS流量负载平衡至三个不同的ip地址。
iptables -A PREROUTING -i eth0 -p tcp --dport 443 -m state --state NEW -m nth --counter 0 --every 3 --packet 0 -j DNAT --to-destination 192.168.1.101:443
iptables -A PREROUTING -i eth0 -p tcp --dport 443 -m state --state NEW -m nth --counter 0 --every 3 --packet 1 -j DNAT --to-destination 192.168.1.102:443
iptables -A PREROUTING -i eth0 -p tcp --dport 443 -m state --state NEW -m nth --counter 0 --every 3 --packet 2 -j DNAT --to-destination 192.168.1.103:443
 
 
12. 允许外部主机ping内部主机
iptables -A INPUT -p icmp --icmp-type echo-request -j ACCEPT
iptables -A OUTPUT -p icmp --icmp-type echo-reply -j ACCEPT
 
 
13. 允许内部主机ping外部主机
iptables -A OUTPUT -p icmp --icmp-type echo-request -j ACCEPT
iptables -A INPUT -p icmp --icmp-type echo-reply -j ACCEPT
 
 
14. 允许回环访问 例：在服务器上允许127.0.0.1回环访问。
iptables -A INPUT -i lo -j ACCEPT
iptables -A OUTPUT -o lo -j ACCEPT
 
 
15. 允许内部网络域外部网络的通信
防火墙服务器上的其中一个网卡连接到外部，另一个网卡连接到内部服务器，使用以下规则允许内部网络与外部网络的通信。此例中，eth1连接到外部网络(互联网)，eth0连接到内部网络(例如:192.168.1.x)。
iptables -A FORWARD -i eth0 -o eth1 -j ACCEPT
 
 
16. 允许出站的DNS连接
iptables -A OUTPUT -p udp -o eth0 --dport 53 -j ACCEPT
iptables -A INPUT -p udp -i eth0 --sport 53 -j ACCEPT
 
 
17. 允许NIS连接
如果你使用NIS管理用户帐户，你需要允许NIS连接。如果你不允许NIS相关的ypbind连接请求，即使SSH连接请求已被允许，用户仍然无法登录。NIS的端口是动态的，先使用命令rpcinfo –p来知道端口号，此例中为853和850端口。
rpcinfo -p | grep ypbind
例：允许来自111端口以及ypbind使用端口的连接请求
iptables -A INPUT -p tcp --dport 111 -j ACCEPT
iptables -A INPUT -p udp --dport 111 -j ACCEPT
iptables -A INPUT -p tcp --dport 853 -j ACCEPT
iptables -A INPUT -p udp --dport 853 -j ACCEPT
iptables -A INPUT -p tcp --dport 850 -j ACCEPT
iptables -A INPUT -p udp --dport 850 -j ACCEPT
注：当你重启ypbind之后端口将不同，上述命令将无效。有两种解决方案：
1）使用你NIS的静态IP
2）编写shell脚本通过“rpcinfo - p”命令自动获取动态端口号,并在上述iptables规则中使用。
 
 
18. 允许来自指定网络的rsync连接请求
例：允许来自网络192.168.101.0/24的rsync连接请求
iptables -A INPUT -i eth0 -p tcp -s 192.168.101.0/24 --dport 873 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 873 -m state --state ESTABLISHED -j ACCEPT
 
 
19. 允许来自指定网络的MySQL连接请求
很多情况下，MySQL数据库与web服务跑在同一台服务器上。有时候我们仅希望DBA和开发人员从内部网络（192.168.100.0/24）直接登录数据库，可尝试以下命令：
iptables -A INPUT -i eth0 -p tcp -s 192.168.100.0/24 --dport 3306 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 3306 -m state --state ESTABLISHED -j ACCEPT
 
 
20. 允许Sendmail, Postfix邮件服务
Sendmail和postfix都使用了25端口，因此我们只需要允许来自25端口的连接请求即可。
iptables -A INPUT -i eth0 -p tcp --dport 25 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 25 -m state --state ESTABLISHED -j ACCEPT
 
 
21. 允许IMAP和IMAPS 例：允许IMAP/IMAP2流量，端口为143
iptables -A INPUT -i eth0 -p tcp --dport 143 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 143 -m state --state ESTABLISHED -j ACCEPT
例：允许IMAPS流量，端口为993
iptables -A INPUT -i eth0 -p tcp --dport 993 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 993 -m state --state ESTABLISHED -j ACCEPT
 
 
22. 允许POP3和POP3S 例：允许POP3访问
iptables -A INPUT -i eth0 -p tcp --dport 110 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 110 -m state --state ESTABLISHED -j ACCEPT
例：允许POP3S访问
iptables -A INPUT -i eth0 -p tcp --dport 995 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 995 -m state --state ESTABLISHED -j ACCEPT
 
 
23. 防止DoS攻击
iptables -A INPUT -p tcp --dport 80 -m limit --limit 25/minute --limit-burst 100 -j ACCEPT
上述例子中： -m limit: 启用limit扩展 –limit 25/minute: 允许最多每分钟25个连接（根据需求更改）。 –limit-burst 100: 只有当连接达到limit-burst水平(此例为100)时才启用上述limit/minute限制。
 
 
24. 端口转发 例：将来自422端口的流量全部转到22端口。
这意味着我们既能通过422端口又能通过22端口进行ssh连接。启用DNAT转发。
iptables -t nat -A PREROUTING -p tcp -d 192.168.102.37 --dport 422 -j DNAT --to 192.168.102.37:22
除此之外，还需要允许连接到422端口的请求
iptables -A INPUT -i eth0 -p tcp --dport 422 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -o eth0 -p tcp --sport 422 -m state --state ESTABLISHED -j ACCEPT
 
 
25. 记录丢弃的数据表 第一步：新建名为LOGGING的链
iptables -N LOGGING
第二步：将所有来自INPUT链中的数据包跳转到LOGGING链中
iptables -A INPUT -j LOGGING
第三步：为这些包自定义个前缀，命名为”IPTables Packet Dropped”
iptables -A LOGGING -m limit --limit 2/min -j LOG --log-prefix &quot;IPTables Packet Dropped: &quot; --log-level 7
第四步：丢弃这些数据包
iptables -A LOGGING -j DROP
 
 
26. ip映射(NAT)
假设有一家ISP提供园区Internet接入服务，为了方便管理，该ISP分配给园区用户的IP地址都是伪IP，但是部分用户要求建立自己的WWW服务器对外发布信息。
我们可以再防火墙的外部网卡上绑定多个合法IP地址，然后通过ip映射使发给其中某一 个IP地址的包转发至内部某一用户的WWW服务器上，然后再将该内部WWW服务器响应包伪装成该合法IP发出的包。
 
我们假设以下情景:
该ISP分配给A单位www服务器的ip为:
伪ip:192.168.1.100
真实ip:202.110.123.100
该ISP分配给B单位www服务器的ip为:
伪ip:192.168.1.200
真实ip:202.110.123.200
linux防火墙的ip地址分别为:
内网接口eth1:192.168.1.1
外网接口eth0:202.110.123.1
然后我们将分配给A、B单位的真实ip绑定到防火墙的外网接口，以root权限执行以下命令:
ifconfig eth0 add 202.110.123.100 netmask 255.255.255.0
ifconfig eth0 add 202.110.123.200 netmask 255.255.255.0
 
首先，对防火墙接收到的目的ip为202.110.123.100和202.110.123.200的所有数据包进行目的NAT(DNAT):
iptables -A PREROUTING -i eth0 -d 202.110.123.100 -j DNAT --to 192.168.1.100
iptables -A PREROUTING -i eth0 -d 202.110.123.200 -j DNAT --to 192.168.1.200
 
其次，对防火墙接收到的源ip地址为192.168.1.100和192.168.1.200的数据包进行源NAT(SNAT):
iptables -A POSTROUTING -o eth0 -s 192.168.1.100 -j SNAT --to 202.110.123.100
iptables -A POSTROUTING -o eth0 -s 192.168.1.200 -j SNAT --to 202.110.123.200
 
这样，所有目的ip为202.110.123.100和202.110.123.200的数据包都将分别被转发给192.168.1.100和192.168.1.200;而所有来自192.168.1.100和192.168.1.200的数据包都将分 别被伪装成由202.110.123.100和202.110.123.200，从而也就实现了ip映射。
</code></pre>
<h2 id="参考文档">参考文档</h2>
<p>https://www.cnblogs.com/zejin2008/p/5919550.html<br>
https://blog.csdn.net/skc361/article/details/20481521<br>
https://www.cnblogs.com/zhujingzhi/p/9706664.html</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[使用iptables实现网络互联]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2019082702</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2019082702">
        </link>
        <updated>2019-08-27T15:34:48.000Z</updated>
        <content type="html"><![CDATA[<h2 id="iptables实现网络功能的方式">iptables实现网络功能的方式</h2>
<p>SNAT：代理上网实现原地址转换</p>
<p>DNAT：用于端口转发实现目标地址转换</p>
<p>实现以上两种功能均要打开内核路由转发功能</p>
<pre><code>[root@xuegod63 ~]# vim /etc/sysctl.conf
#改：#net.ipv4.ip_forward = 0
#为： net.ipv4.ip_forward = 1
#改完使配置生效：
[root@xuegod63 ~]# sysctl -p
</code></pre>
<h2 id="iptables的源地址转换snat">iptables的源地址转换(SNAT)</h2>
<h3 id="双网卡实现snat">双网卡实现SNAT</h3>
<p><strong>示例一</strong></p>
<figure data-type="image" tabindex="1"><img src="https://img-blog.csdn.net/20171117145802662?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2hlYXRfZ3JvdW5k/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="è¿éåå¾çæè¿°"></figure>
<pre><code># 左边的机器为A，右边的机器为B ，A和B在同一个自网路，B有两个网卡。一个网卡和A同在一个网络，另外一个网卡可以连接外网。 
# 配置方式
[root@xuegod63 ~]# iptables -t nat -A POSTROUTING -s 192.168.240.0/24   -j  SNAT  --to 192.168.1.250
或：
[root@xuegod63 ~]#iptables -t nat -A POSTROUTING -s 192.168.2.0/24  -o eth0  -j MASQUERADE
# 拒绝访问转发机器本身
[root@xuegod63 ~]# iptables -A INPUT -s 192.168.2.2 -j DROP
</code></pre>
<p><strong>示例二</strong></p>
<pre><code>场景：
有一台A服务器不能上网，和B服务器通过内网来连接，B服务器可以上网，要实现A服务器也可以上网。
A IP:192.168.0.35
B IP:192.168.0.146（123.196.112.146为外网ip）

SNAT:改变数据包的源地址。防火墙会使用外部地址，替换数据包的本地网络地址。这样使网络内部主机能够与网络外部通信。

1.在可以上网那台服务器B上，开启内核路由转发功能
echo 1 &gt; /proc/sys/net/ipv4/ip_forward

2.在需要通过代理上网服务器A上，查看路由表。并添加默认网关。route add default gw 192.168.0.146
[root@localhost ~]# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
192.168.0.0     0.0.0.0         255.255.255.0   U     0      0        0 eth0
169.254.0.0     0.0.0.0         255.255.0.0     U     0      0        0 eth0
0.0.0.0         192.168.0.146   0.0.0.0         UG    0      0        0 eth0

3.在可以上网那台服务器B上添加SNAT规则
iptables -t nat -A POSTROUTING -o eth0 -s 192.168.0.0/24 -j SNAT –-to 123.196.112.146

4.保存
service iptables save

5.验证是否可以正常上网。
</code></pre>
<h3 id="单网卡实现snat">单网卡实现SNAT</h3>
<p><strong>示例</strong></p>
<pre><code>场景：
给转发机器网卡绑定两个IP。一个内网IP，无需设置网关，另一个接口，IP要可以连接internet。
直接将下面内容加在 /etc/rc.local 启动脚本内（把192.168.51.250换成外网IP，把192.168.151.1换内网的网络地址）

echo 1 &gt; /proc/sys/net/ipv4/ip_forward
ifconfig eth0:1 192.168.151.1 netmask 255.255.255.0
iptables -F
iptables -F -t nat
iptables -P FORWARD DROP
iptables -A FORWARD -s 192.168.151.0/24 -j ACCEPT
iptables -A FORWARD -i eth0 -m state --state ESTABLISHED,RELATED -j ACCEPT
iptables -t nat -A POSTROUTING -o eth0 -s 192.168.151.0/24 -j SNAT --to 192.168.51.250
</code></pre>
<h2 id="iptables的目标地址转换dnat">iptables的目标地址转换（DNAT）</h2>
<h3 id="双网卡实现dnat">双网卡实现DNAT</h3>
<pre><code># 代理192.168.0.10的80端口到NAT服务器
# NAT服务器IP： 172.16.0.1  192.168.0.1
# 服务端IP： 192.168.0.10 需要将服务端的网关指向NAT服务器
# 做DNAT转发：
iptables -t nat -A PREROUTING -i ens33 -d 172.16.0.1 -p tcp --dport  80 -j DNAT --to-destination 192.168.0.10
</code></pre>
<h3 id="单网卡实现dnat">单网卡实现DNAT</h3>
<p><strong>说明</strong></p>
<pre><code>A)将本机端口转发至目标机器
iptables -t nat -A PREROUTING -p tcp -d [本地服务器主网卡绑定IP] –dport [本地端口] -j DNAT –to-destination [目标IP:目标端口]
iptables -t nat -A PREROUTING -p udp -d [本地服务器主网卡绑定IP] –dport [本地端口] -j DNAT –to-destination [目标IP:目标端口]

B)将目标机器返回的数据转发至本机
iptables -t nat -A POSTROUTING -p tcp -d [目标IP] –dport [目标端口] -j SNAT –to-source [本地服务器主网卡绑定IP]
iptables -t nat -A POSTROUTING -p udp -d [目标IP] –dport [目标端口] -j SNAT –to-source [本地服务器主网卡绑定IP]
</code></pre>
<p><strong>示例</strong></p>
<pre><code># 单网卡iptables配置路由转发，需要配置snat和dnat
# pc1 ip:192.168.23.252 
# pc2 ip:192.168.23.253 

iptables -t nat -A PREROUTING -d 192.168.23.252 -p tcp --dport 80 -j DNAT --to-destination 192.168.23.253:80
# 如果进来的route的访问目的地址是192.168.23.252并且访问的目的端口是80，就进行dnat转换，把目的地址改为192.168.23.253 ，端口还是80
 
iptables -t nat -A POSTROUTING -d 192.168.23.253 -p tcp --dport 80 -j SNAT --to 192.168.23.252
# 当FORWARD 出来后，访问的目的地址是192.168.23.253，端口是80的。进行snat地址转换，把原地址改为192.168.23.252
 
iptables -A FORWARD -o eth0 -d 192.168.23.253 -p tcp --dport 80 -j ACCEPT
# 当从eth0出去的访问目的地址是 192.168.23.253且目的端口是80的route，允许通过
 
iptables -A FORWARD -i eth0 -s 192.168.23.253 -p tcp --dport 80 -j ACCEPT
# 当从eth0进来的原地址是 192.168.23.253且目的端口是80的route，允许通过
 
# 保存规则启动iptables
service iptables save
service iptables start
</code></pre>
<h2 id="参考链接">参考链接</h2>
<p>https://blog.csdn.net/wheat_ground/article/details/78561750</p>
<p>https://blog.51cto.com/jinchuang/1947052</p>
<p>http://www.flynoc.com/help/helpview_390.html</p>
<p>https://blog.51cto.com/2333657/2157070?source=dra</p>
<p>https://blog.csdn.net/weixin_42537861/article/details/81195070</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[使用ipvs模式安装kubernetes]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2019082701</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2019082701">
        </link>
        <updated>2019-08-27T07:11:33.000Z</updated>
        <content type="html"><![CDATA[<h3 id="安装ipvs">安装ipvs</h3>
<pre><code>cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
$ chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4

</code></pre>
<h3 id="安装ipset和ipvsadm">安装ipset和ipvsadm</h3>
<pre><code>yum install ipset
yum install ipvsadm
</code></pre>
<h3 id="安装时间服务">安装时间服务</h3>
<pre><code>$ yum install chrony -y
$ systemctl enable chronyd
$ systemctl start chronyd
$ chronyc sources
210 Number of sources = 4
MS Name/IP address         Stratum Poll Reach LastRx Last sample
===============================================================================
^+ sv1.ggsrv.de                  2   6    17    32   -823us[-1128us] +/-   98ms
^- montreal.ca.logiplex.net      2   6    17    32    -17ms[  -17ms] +/-  179ms
^- ntp6.flashdance.cx            2   6    17    32    -32ms[  -32ms] +/-  161ms
^* 119.28.183.184                2   6    33    32   +661us[ +357us] +/-   38ms
$ date
Tue Aug 27 09:28:41 CST 2019
</code></pre>
<h3 id="安装ipvs模式的kubernetes集群模板">安装ipvs模式的kubernetes集群模板</h3>
<pre><code>apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.151.30.11  # apiserver 节点内网IP
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: ydzs-master
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS  # dns类型
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: gcr.azk8s.cn/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.15.3  # k8s版本
networking:
  dnsDomain: cluster.local
  podSubnet: 192.168.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs  # kube-proxy 模式
</code></pre>
<h3 id="参考地址">参考地址</h3>
<p>https://mp.weixin.qq.com/s/vnriX2bTtnkv8i2UpLeNnA</p>
<p>https://www.qikqiak.com/post/use-kubeadm-install-kubernetes-1.15.3/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes的国内软件源配置]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2019080801</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2019080801">
        </link>
        <updated>2019-08-08T06:40:52.000Z</updated>
        <content type="html"><![CDATA[<h2 id="ubuntu-软件源配置">ubuntu 软件源配置</h2>
<pre><code>add-apt-repository &quot;deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main&quot;
add-apt-repository &quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot;
wget -q https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg -O- | sudo apt-key add -
curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -
apt-get update
apt-get -y install apt-transport-https ca-certificates curl software-properties-common
apt-get -y install docker-ce kubeadm kubectl kubelet
systemctl enable docker kubelet &amp;&amp; systemctl start docker
</code></pre>
<h2 id="centos-软件源配置">centos 软件源配置</h2>
<pre><code># 切换到配置目录
cd /etc/yum.repos.d/
# 配置docker-ce阿里源
wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
# 配置kubernetes阿里源
cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[centos快速创建kubernetes]]></title>
        <id>https://tomcat1009.coding.me/hexo/post/2019090701</id>
        <link href="https://tomcat1009.coding.me/hexo/post/2019090701">
        </link>
        <updated>2019-08-07T08:22:17.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前置条件">前置条件</h2>
<ul>
<li>系统要求:64位centos7.6</li>
<li>关闭防火墙和selinux</li>
<li>关闭操作系统swap分区(使用k8s不推荐打开)</li>
</ul>
<h2 id="环境说明">环境说明</h2>
<p>本手册安装方式适用于单节点测试部署</p>
<h2 id="准备工作每个节点都需要执行">准备工作(每个节点都需要执行)</h2>
<h3 id="docker和kubernetes软件源配置">Docker和kubernetes软件源配置</h3>
<pre><code># 切换到配置目录
cd /etc/yum.repos.d/
# 配置docker-ce阿里源
wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
# 配置kubernetes阿里源
cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
</code></pre>
<h3 id="配置内核相关参数">配置内核相关参数</h3>
<pre><code>cat &lt;&lt;EOF &gt;  /etc/sysctl.d/ceph.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system
</code></pre>
<h3 id="安装相应软件包">安装相应软件包</h3>
<pre><code># 安装kubeadm kubelet kubectl
yum install docker-ce kubeadm kubectl kubelet -y

# 开机启动kubelet和docker
systemctl enable docker kubelet

# 启动docker
systemctl start docker
</code></pre>
<h2 id="部署">部署</h2>
<h3 id="安装k8s">安装k8s</h3>
<pre><code># 30.4.241.19 需要更新为当前部署节点ip地址
kubeadm init --apiserver-advertise-address 30.4.241.19 --pod-network-cidr=10.244.0.0/16 --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers --ignore-preflight-errors=Swap

# 初始化完成后按提示执行以下命令
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>
<h3 id="安装flannel">安装flannel</h3>
<pre><code>kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
</code></pre>
<h3 id="检查是否安装完成">检查是否安装完成</h3>
<pre><code>root@k8s4:~# kubectl  get nodes
NAME   STATUS   ROLES    AGE   VERSION
k8s4   Ready    master   20m   v1.14.2
root@k8s4:~# kubectl  get nodes
NAME   STATUS   ROLES    AGE   VERSION
k8s4   Ready    master   20m   v1.14.2
root@k8s4:~# kubectl  get pods --all-namespaces
NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE
kube-system   coredns-8cc96f57d-cfr4j        1/1     Running   0          20m
kube-system   coredns-8cc96f57d-stcz6        1/1     Running   0          20m
kube-system   etcd-k8s4                      1/1     Running   0          19m
kube-system   kube-apiserver-k8s4            1/1     Running   0          19m
kube-system   kube-controller-manager-k8s4   1/1     Running   0          19m
kube-system   kube-flannel-ds-amd64-k4q6q    1/1     Running   0          50s
kube-system   kube-proxy-lhjsf               1/1     Running   0          20m
kube-system   kube-scheduler-k8s4            1/1     Running   0          19m
</code></pre>
<h3 id="测试是否能正常使用">测试是否能正常使用</h3>
<pre><code># 取消节点污点,使master能被正常调度, k8s4请更改为你自有集群的nodename
kubectl  taint node k8s4 node-role.kubernetes.io/master:NoSchedule-

# 创建nginx deploy
root@k8s4:~# kubectl  create deploy nginx --image nginx
deployment.apps/nginx created

root@k8s4:~# kubectl  get pods
NAME                     READY   STATUS    RESTARTS   AGE
nginx-65f88748fd-9sk6z   1/1     Running   0          2m44s

# 暴露nginx到集群外
root@k8s4:~# kubectl  expose deploy nginx --port=80 --type=NodePort
service/nginx exposed
root@k8s4:~# kubectl  get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP        25m
nginx        NodePort    10.104.109.234   &lt;none&gt;        80:32129/TCP   5s
root@k8s4:~# curl 127.0.0.1:32129
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;

</code></pre>
]]></content>
    </entry>
</feed>