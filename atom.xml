<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://js2hlu.coding-pages.com</id>
    <title>PeterPan</title>
    <updated>2020-03-30T06:35:53.352Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://js2hlu.coding-pages.com"/>
    <link rel="self" href="https://js2hlu.coding-pages.com/atom.xml"/>
    <subtitle>贵有恒，何必三更起五更睡；最无益，只怕一日暴十寒。</subtitle>
    <logo>https://js2hlu.coding-pages.com/images/avatar.png</logo>
    <icon>https://js2hlu.coding-pages.com/favicon.ico</icon>
    <rights>All rights reserved 2020, PeterPan</rights>
    <entry>
        <title type="html"><![CDATA[kubernetes 证书过期时间修改]]></title>
        <id>https://js2hlu.coding-pages.com/post/2020033001/</id>
        <link href="https://js2hlu.coding-pages.com/post/2020033001/">
        </link>
        <updated>2020-03-30T06:32:29.000Z</updated>
        <content type="html"><![CDATA[<h3 id="前言">前言</h3>
<p>kubernetes 默认对服务的证书签发时间为1年。</p>
<p>其对应的CA服务：kubernetes CA、 etcd CA、 front-proxy-CA 为10年。</p>
<p>因此修改证书过期时间长度需要更正ca时间和证书签发时间两部分。</p>
<h3 id="代码下载和更新">代码下载和更新</h3>
<pre><code># git clone（需要去掉dirty后缀）
git clone https://github.com/kubernetes/kubernetes -b v1.17.3
cd kubernetes
git checkout -b v1.17.3

# wget (推荐)
wget https://codeload.github.com/kubernetes/kubernetes/tar.gz/v1.17.3
tar xvf kubernetes-1.17.3.tar.gz
cd kubernetes-1.17.3
</code></pre>
<h4 id="修改ca时间">修改CA时间</h4>
<pre><code>[root@k8s-m1 kubernetes]# vim ./staging/src/k8s.io/client-go/util/cert/cert.go
</code></pre>
<pre><code>// NewSelfSignedCACert creates a CA certificate
func NewSelfSignedCACert(cfg Config, key crypto.Signer) (*x509.Certificate, error) {
        now := time.Now()
        tmpl := x509.Certificate{
                SerialNumber: new(big.Int).SetInt64(0),
                Subject: pkix.Name{
                        CommonName:   cfg.CommonName,
                        Organization: cfg.Organization,
                },
                NotBefore:             now.UTC(),
                NotAfter:              now.Add(duration365d * 10).UTC(),         # 修改NotAfter字段，默认为10年改为100年
                KeyUsage:              x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign,
                BasicConstraintsValid: true,
                IsCA:                  true,
        }

        certDERBytes, err := x509.CreateCertificate(cryptorand.Reader, &amp;tmpl, &amp;tmpl, key.Public(), key)
        if err != nil {
                return nil, err
        }
        return x509.ParseCertificate(certDERBytes)
}
</code></pre>
<h4 id="修改证书时间">修改证书时间</h4>
<pre><code>[root@k8s-m1 kubernetes]# vim ./cmd/kubeadm/app/constants/constants.go
</code></pre>
<pre><code>const (
        // KubernetesDir is the directory Kubernetes owns for storing various configuration files
        KubernetesDir = &quot;/etc/kubernetes&quot;
        // ManifestsSubDirName defines directory name to store manifests
        ManifestsSubDirName = &quot;manifests&quot;
        // TempDirForKubeadm defines temporary directory for kubeadm
        // should be joined with KubernetesDir.
        TempDirForKubeadm = &quot;tmp&quot;

        // CertificateValidity defines the validity for all the signed certificates generated by kubeadm
        CertificateValidity = time.Hour * 24 * 365 * 100       # 修改CertificateValidity字段

        // CACertAndKeyBaseName defines certificate authority base name
        CACertAndKeyBaseName = &quot;ca&quot;
        // CACertName defines certificate name
        CACertName = &quot;ca.crt&quot;
        // CAKeyName defines certificate name
        CAKeyName = &quot;ca.key&quot;
</code></pre>
<h3 id="编译">编译</h3>
<h4 id="方法1本地直接编译">方法1：本地直接编译</h4>
<pre><code># 执行编译
cd /Users/peter/workspace/kubernetes-1.17.3
KUBE_BUILD_PLATFORMS=linux/amd64 make all WHAT=cmd/kubeadm GOFLAGS=-v GOGCFLAGS=&quot;-N -l&quot;
# 编译输出文件为_output/local/bin/linux/amd64/kubeadm
</code></pre>
<h4 id="方法2docker编译">方法2：docker编译</h4>
<pre><code># 编译容器dockerfile

FROM centos:centos7.6.1810
MAINTAINER fengjing

ENV GOROOT /usr/local/go
ENV GOPATH /usr/local/gopath
ENV PATH /usr/local/go/bin:$PATH

RUN yum install rpm-build which where rsync gcc gcc-c++ automake autoconf libtool make -y \
    &amp;&amp; curl -L https://studygolang.com/dl/golang/go1.13.9.linux-amd64.tar.gz | tar zxvf - -C /usr/local
</code></pre>
<pre><code># 生成镜像
[root@master build-k8s]# docker build -t centos-7.6-go-1.13.9-k8s-1.17.3 .
</code></pre>
<pre><code># 镜像挂载目录
[root@master kubernetes-1.17.3]# docker run -it --rm \
-v /root/build-k8s/kubernetes-1.17.3:/usr/local/gopath/src/k8s.io/kubernetes \
centos-7.6-go-1.13.9-k8s-1.17.3 \
/bin/bash
</code></pre>
<pre><code># 进入k8s跟目录编译k8s
[root@0f351f2de25d /]# cd /usr/local/gopath/src/k8s.io/kubernetes/
[root@0f351f2de25d kubernetes]# KUBE_BUILD_PLATFORMS=linux/amd64 make all WHAT=cmd/kubeadm GOFLAGS=-v GOGCFLAGS=&quot;-N -l&quot;
</code></pre>
<h3 id="使用">使用</h3>
<h4 id="旧集群更新">旧集群更新</h4>
<pre><code>cp -r /etc/kubernetes/pki /etc/kubernetes/pki.backup
kubeadm alpha certs renew all
</code></pre>
<pre><code># ca 时间依然不会更新
[root@fengjingk8s ~]# kubeadm alpha certs check-expiration
CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED
admin.conf                 Mar 30, 2021 03:51 UTC   364d                                    no
apiserver                  Mar 30, 2021 03:51 UTC   364d            ca                      no
apiserver-etcd-client      Mar 30, 2021 03:51 UTC   364d            etcd-ca                 no
apiserver-kubelet-client   Mar 30, 2021 03:51 UTC   364d            ca                      no
controller-manager.conf    Mar 30, 2021 03:51 UTC   364d                                    no
etcd-healthcheck-client    Mar 30, 2021 03:51 UTC   364d            etcd-ca                 no
etcd-peer                  Mar 30, 2021 03:51 UTC   364d            etcd-ca                 no
etcd-server                Mar 30, 2021 03:51 UTC   364d            etcd-ca                 no
front-proxy-client         Mar 30, 2021 03:51 UTC   364d            front-proxy-ca          no
scheduler.conf             Mar 30, 2021 03:51 UTC   364d                                    no

CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
ca                      Mar 05, 2030 15:36 UTC   9y              no
etcd-ca                 Mar 05, 2030 15:36 UTC   9y              no
front-proxy-ca          Mar 05, 2030 15:36 UTC   9y              no
</code></pre>
<blockquote>
<p>警告： kubeadm 不能管理由外部 CA 签名的证书</p>
</blockquote>
<blockquote>
<p>注意： 上面的列表中没有包含 kubelet.conf 因为 kubeadm 将 kubelet 配置为自动更新证书。</p>
</blockquote>
<blockquote>
<p>注意：上面命令不能更新CA</p>
</blockquote>
<blockquote>
<p>完成后重启kube-apiserver,kube-controller,kube-scheduler这三个容器</p>
</blockquote>
<blockquote>
<p>警告： 如果您运行了一个 HA 集群，这个命令需要在所有控制面板节点上执行。</p>
</blockquote>
<blockquote>
<p>注意：alpha certs renew 使用现有的证书作为属性 (Common Name、Organization、SAN 等) 的权威来源，而不是 kubeadm-config ConfigMap 。强烈建议使它们保持同步。</p>
</blockquote>
<h4 id="新集群安装检查">新集群安装检查</h4>
<pre><code># 安装
./kubeadm init --config localinstall/kubeadm.conf --ignore-preflight-errors=all
</code></pre>
<pre><code># 使用编译的kubeadm安装集群证书信息如下：
[root@lop1 ~]# kubeadm alpha certs check-expiration
CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED
admin.conf                 Mar 06, 2120 05:09 UTC   99y                                     no
apiserver                  Mar 06, 2120 05:09 UTC   99y             ca                      no
apiserver-etcd-client      Mar 06, 2120 05:09 UTC   99y             etcd-ca                 no
apiserver-kubelet-client   Mar 06, 2120 05:09 UTC   99y             ca                      no
controller-manager.conf    Mar 06, 2120 05:09 UTC   99y                                     no
etcd-healthcheck-client    Mar 06, 2120 05:09 UTC   99y             etcd-ca                 no
etcd-peer                  Mar 06, 2120 05:09 UTC   99y             etcd-ca                 no
etcd-server                Mar 06, 2120 05:09 UTC   99y             etcd-ca                 no
front-proxy-client         Mar 06, 2120 05:09 UTC   99y             front-proxy-ca          no
scheduler.conf             Mar 06, 2120 05:09 UTC   99y                                     no

CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
ca                      Mar 06, 2120 05:09 UTC   99y             no
etcd-ca                 Mar 06, 2120 05:09 UTC   99y             no
front-proxy-ca          Mar 06, 2120 05:09 UTC   99y             no
</code></pre>
<h3 id="启用-kubelet-server-证书">启用 Kubelet Server 证书</h3>
<p>kubelet证书分为<code>server</code>和<code>client</code>两种， k8s 1.9默认启用了client证书的自动轮换，但server证书出于安全原因需要用户手动签发</p>
<h4 id="增加-kubelet-参数">增加 kubelet 参数</h4>
<pre><code># 在/etc/systemd/system/kubelet.service.d/10-kubeadm.conf 增加如下参数
Environment=&quot;KUBELET_EXTRA_ARGS=--feature-gates=RotateKubeletServerCertificate=true&quot;

# 或在文件/var/lib/kubelet/config.yaml中添加如下内容，也可使用kubelet configmap管理
featureGates:
  RotateKubeletServerCertificate: true
</code></pre>
<h4 id="增加-controller-manager-参数">增加 controller-manager 参数</h4>
<pre><code># 在/etc/kubernetes/manifests/kube-controller-manager.yaml 添加如下参数
  - command:
    - kube-controller-manager
    - --experimental-cluster-signing-duration=87600h0m0s
    - --feature-gates=RotateKubeletServerCertificate=true
    - ....
</code></pre>
<h3 id="参考地址">参考地址</h3>
<p>https://www.cnblogs.com/2019peng/p/11988424.html</p>
<p>https://www.cnblogs.com/aguncn/p/11465584.html</p>
<p>https://system51.github.io/2019/12/05/Kubeadm-certificate-modified/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes部署metric-server]]></title>
        <id>https://js2hlu.coding-pages.com/post/2020031902/</id>
        <link href="https://js2hlu.coding-pages.com/post/2020031902/">
        </link>
        <updated>2020-03-19T06:36:54.000Z</updated>
        <content type="html"><![CDATA[<h3 id="获取metric-server服务">获取metric-server服务</h3>
<pre><code>git clone https://gitee.com/peterpan1009/metrics-server.git
</code></pre>
<h3 id="编辑deployment文件">编辑deployment文件</h3>
<pre><code># edit metric-server deployment to add the flags
args:
- --kubelet-insecure-tls
- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
</code></pre>
<h3 id="部署metrics-server">部署metrics-server</h3>
<pre><code>cd metrics-server/deploy/kubernetes
kubectl apply -f .
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[linux 配置ssh客户端（便于集群管理）]]></title>
        <id>https://js2hlu.coding-pages.com/post/2020031901/</id>
        <link href="https://js2hlu.coding-pages.com/post/2020031901/">
        </link>
        <updated>2020-03-19T05:45:05.000Z</updated>
        <content type="html"><![CDATA[<h3 id="配置连接管理">配置连接管理</h3>
<p>可以配置<code>远程别名、主机地址、用户名、端口、证书文件</code></p>
<pre><code>➜  nginx cat ~/.ssh/config
#####aws 客户（学生）####
host c2node1
  hostname ec2-34-233-126-45.compute-1.amazonaws.com
  user ubuntu
  port 22
  IdentityFile ~/pem/project.pem
</code></pre>
<h3 id="配置客户端第一次连接不输入yes并且不验证指纹">配置客户端第一次连接不输入yes,并且不验证指纹</h3>
<p>方法一：连接时加入StrictHostKeyChecking=no</p>
<pre><code>ssh -o StrictHostKeyChecking=no root@192.168.1.100
</code></pre>
<p>方法二：修改/etc/ssh/ssh_config配置文件，添加：</p>
<pre><code>StrictHostKeyChecking no
</code></pre>
<p>此外，为防止出现这类警告：POSSIBLE BREAK-IN ATTEMPT!</p>
<pre><code>Address 192.168.0.101 maps to localhost, but this does not map back to the address - POSSIBLE BREAK-IN ATTEMPT!
</code></pre>
<p>可以将/etc/ssh/ssh_config配置文件中的</p>
<pre><code>GSSAPIAuthentication yes
</code></pre>
<p>改为：</p>
<pre><code>GSSAPIAuthentication no
</code></pre>
<p>可以使用sed流编辑器来完成修改：</p>
<pre><code>sed -i 's/#   StrictHostKeyChecking ask/StrictHostKeyChecking no/' /etc/ssh/ssh_config
sed -i 's/GSSAPIAuthentication yes/GSSAPIAuthentication no/' /etc/ssh/ssh_config
service sshd restart
</code></pre>
<h3 id="docker-里面启用ssh">docker 里面启用ssh</h3>
<pre><code># 安装ssh
yum install openssh-server
yum install openssh-clients

# 生成必要的ssh相关文件
/usr/sbin/sshd-keygen -A

# 配置本机免密登陆
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys

# 启动ssh
/usr/sbin/sshd
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[keepalived 单播和组播配置]]></title>
        <id>https://js2hlu.coding-pages.com/post/2020031403/</id>
        <link href="https://js2hlu.coding-pages.com/post/2020031403/">
        </link>
        <updated>2020-03-14T15:45:09.000Z</updated>
        <content type="html"><![CDATA[<h3 id="单播">单播</h3>
<pre><code>cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.conf
! Configuration File for keepalived

global_defs {
   router_id k8s-master1                      #主调度器的主机名
}

vrrp_instance VI_1 {
    state BACKUP
    interface eth0                            # 修改网卡名
    virtual_router_id 66
    nopreempt
    priority 90
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 123456
    }
    unicast_src_ip 10.20.1.2         #配置单薄的源地址
    unicast_peer {
    10.20.1.3                        #配置单薄的目标地址
    }
    virtual_ipaddress {
        10.20.1.8                            #VIP地址声明
    }
}
EOF

</code></pre>
<h3 id="组播">组播</h3>
<pre><code>cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.conf
! Configuration File for keepalived

global_defs {
   router_id k8s-master1                      #主调度器的主机名
   vrrp_mcast_group4 224.26.1.1         

}

vrrp_instance VI_1 {
    state BACKUP                          
    interface eth0                           # 修改网卡名
    virtual_router_id 66              
    nopreempt                             
    priority 90                         
    advert_int 1
    authentication {
        auth_type PASS                     
        auth_pass 123456                 
    }
    virtual_ipaddress {
        10.20.1.8                            #VIP地址声明
    }
}
EOF
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[nginx部署https]]></title>
        <id>https://js2hlu.coding-pages.com/post/2020031402/</id>
        <link href="https://js2hlu.coding-pages.com/post/2020031402/">
        </link>
        <updated>2020-03-14T15:43:58.000Z</updated>
        <content type="html"><![CDATA[<h3 id="生成证书相关文件">生成证书相关文件</h3>
<pre><code># 生成私钥和证书签名请求，请替换subj的相关信息
openssl req -new -newkey rsa:2048 -sha256 -nodes -out test_com.csr -keyout test_com.key -subj &quot;/C=CN/ST=ShenZhen/L=ShenZhen/O=Example Inc./OU=Web Security/CN=test.com&quot;

# 自签证书
openssl x509 -req -days 3650 -in test_com.csr  -signkey test_com.key -out test_com.crt
</code></pre>
<h3 id="配置nginx">配置nginx</h3>
<pre><code># nginx 版本为nginx/1.16.1
    server {
        listen     443 ssl  ;
        server_name  test.com;  #指定服务域名
        ssl_certificate     test_com.crt;  #证书文件，需要放在conf目录下
        ssl_certificate_key test_com.key;  #证书文件，需要放在conf目录下
        ssl_protocols       TLSv1 TLSv1.1 TLSv1.2;
        ssl_session_cache    shared:SSL:1m;
        ssl_session_timeout  5m;
        ssl_ciphers  HIGH:!aNULL:!MD5;
        ssl_prefer_server_ciphers  on;
        location / {
            root   html;
            index  index.html index.htm;
        }
        location = /50x.html {
            root   html;
        }
    }
</code></pre>
<h3 id="参考地址">参考地址</h3>
<p>https://aotu.io/notes/2016/08/16/nginx-https/index.html</p>
<p>https://www.cnblogs.com/crazymagic/p/11042333.html</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[http中的301和302状态码]]></title>
        <id>https://js2hlu.coding-pages.com/post/2020031401/</id>
        <link href="https://js2hlu.coding-pages.com/post/2020031401/">
        </link>
        <updated>2020-03-14T15:21:05.000Z</updated>
        <content type="html"><![CDATA[<h3 id="http状态码的301和302">HTTP状态码的301和302</h3>
<p>301 redirect: 301 代表永久性转移(Permanently Moved)</p>
<p>302 redirect: 302 代表暂时性转移(Temporarily Moved )</p>
<p>详细来说，301和302状态码都表示重定向，就是说浏览器在拿到服务器返回的这个状态码后会自动跳转到一个新的URL地址，这个地址可以从响应的Location首部中获取（用户看到的效果就是他输入的地址A瞬间变成了另一个地址B）——这是它们的共同点。他们的不同在于。301表示旧地址A的资源已经被永久地移除了（这个资源不可访问了），<strong>搜索引擎在抓取新内容的同时也将旧的网址交换为重定向之后的网址</strong>；302表示旧地址A的资源还在（仍然可以访问），这个重定向只是临时地从旧地址A跳转到地址B，<strong>搜索引擎会抓取新的内容而保存旧的网址。</strong></p>
<p>301重定向常用于站点永久废弃迁移到新域名的情况，或者多个域名共用一个站点的情况。（301的跳转需要慎用）</p>
<p>302则用于站点临时跳转的情况，后续还会重新使用之前的域名。（常用）</p>
<p>302跳转的风险会存在URL劫持的问题。</p>
<h3 id="301跳转配置">301跳转配置</h3>
<p>Nginx下常见的301跳转有以下三种，虽然都能达到同样的目的。但是三种写法上还是有区别的，主要的区别是在正则匹配的性能上。</p>
<p>第一种：使用rewrite指令，通过正则匹配所有的URI后再去掉开头第一个/(反斜线)</p>
<pre><code>rewrite ^/(.*)$ https://www.hi-linux.com/$1;
</code></pre>
<p>第二种：同样使用rewrite指令，不同的是通过<code>$request_uri</code>变量匹配所有的URI。</p>
<pre><code>rewrite ^ https://www.hi-linux.com$request_uri? permanent;
</code></pre>
<p>这样写的好处是省去了去掉开头第一个反斜线的过程，正则匹配上性能更优。</p>
<p>第三种：使用return指令，通过301状态码和<code>$request_uri</code>参数，直接告诉Nginx这是个301重定向和抓取指定URI。</p>
<pre><code>return 301 https://www.hi-linux.com$request_uri;
</code></pre>
<p>这种方法是性能上最优的，因为<code>rewrite</code>指令有很多写法和规则，执行完所有正则匹配后，Nginx 才会知道这是一个301永久重定向。</p>
<h3 id="302跳转配置">302跳转配置</h3>
<p>Nginx下常见的301跳转有以下三种，虽然都能达到同样的目的。但是三种写法上还是有区别的，主要的区别是在正则匹配的性能上。</p>
<p>第一种：使用rewrite指令，通过正则匹配所有的URI后再去掉开头第一个/(反斜线)</p>
<pre><code>rewrite ^/(.*)$ https://www.hi-linux.com/$1 redirect;
</code></pre>
<p>第二种：同样使用rewrite指令，不同的是通过<code>$request_uri</code>变量匹配所有的URI。</p>
<pre><code>rewrite ^(.*)$ https://www.hi-linux.com$request_uri? redirect;
</code></pre>
<p>这样写的好处是省去了去掉开头第一个反斜线的过程，正则匹配上性能更优。</p>
<p>第三种：使用return指令，通过301状态码和<code>$request_uri</code>参数，直接告诉Nginx这是个301重定向和抓取指定URI。</p>
<pre><code>return 302 https://www.hi-linux.com$request_uri;
</code></pre>
<p>这种方法是性能上最优的，因为<code>rewrite</code>指令有很多写法和规则，执行完所有正则匹配后，Nginx 才会知道这是一个302临时。</p>
<h3 id="配置实例">配置实例</h3>
<pre><code>    server {
        listen       80;
        server_name  test.com;
        #rewrite ^/(.*) http://www.qq.com/$1 permanent;
        #return 302 https://www.qq.com$request_uri;
        return 301 https://www.baidu.com$request_uri;
        location / {
            root   html;
            index  index.html index.htm;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
</code></pre>
<h3 id="参考地址">参考地址</h3>
<p>https://mp.weixin.qq.com/s/Bvf6SkDafzeW9Kh-_-RxDw</p>
<p>https://www.jianshu.com/p/3fe4f45729e6</p>
<p>https://www.jianshu.com/p/cb4b47186335</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes升级iptables为ipvs]]></title>
        <id>https://js2hlu.coding-pages.com/post/20200227001/</id>
        <link href="https://js2hlu.coding-pages.com/post/20200227001/">
        </link>
        <updated>2020-02-27T06:06:14.000Z</updated>
        <content type="html"><![CDATA[<p>Kubernetes 原生的 Service 负载均衡基于 Iptables 实现，其规则链会随 Service 的数量呈线性增长，在大规模场景下对 Service 性能会有严重的影响。具体测试可参考这篇文章：<a href="https://zhuanlan.zhihu.com/p/37230013">《华为云在 K8S 大规模场景下的 Service 性能优化实践》</a></p>
<p>本文主要记录操作层面的。</p>
<h3 id="所有节点运行">所有节点运行</h3>
<p>在每个节点上安装ipvs，加载相关系统模块</p>
<pre><code>#!/bin/bash

# 每台机器都要运行modprobe

cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4

yum install -y ipset ipvsadm
</code></pre>
<h3 id="k8s-master运行">k8s master运行</h3>
<p>修改 kube-proxy 的配置，明确使用 ipvs 方式。</p>
<pre><code>#!/bin/bash

rm *.yaml;
kubectl get configmap kube-proxy -n kube-system -oyaml &gt;kube-proxy-configmap.yaml
sed -e '/mode:/s/&quot;&quot;/&quot;ipvs&quot;/g' kube-proxy-configmap.yaml  &gt;kube-proxy-configmap-ipvs.yaml
kubectl replace -f kube-proxy-configmap-ipvs.yaml
kubectl get pod  -n kube-system|grep kube-proxy|awk '{print &quot;kubectl delete po &quot;$1&quot; -n kube-system&quot;}'|sh

# https://segmentfault.com/a/1190000015104653
# https://blog.frognew.com/2018/10/kubernetes-kube-proxy-enable-ipvs.html

# kubectl logs kube-proxy -n kube-system
# ipvsadm -ln
</code></pre>
<h3 id="原文链接">原文链接</h3>
<p>https://blog.kelu.org/tech/2019/04/04/kubernetes-change-iptable-to-ipvs.html</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[centos7针对大并发场景的系统优化]]></title>
        <id>https://js2hlu.coding-pages.com/post/2020022301/</id>
        <link href="https://js2hlu.coding-pages.com/post/2020022301/">
        </link>
        <updated>2020-02-23T12:51:52.000Z</updated>
        <content type="html"><![CDATA[<h3 id="为什么端口号最大为65535">为什么端口号最大为65535</h3>
<ul>
<li>在TCP、UDP协议的开头，会分别有16位来存储源端口号和目标端口号，所以端口个数是2^16-1=65535个。</li>
<li>tcp客户端连接服务端的时候，需要获取本地的临时端口，传输层协议限制了最多只有65535个端口</li>
</ul>
<hr>
<h3 id="linux打开的文件数限制">linux打开的文件数限制</h3>
<blockquote>
<p>参考文档：https://blog.csdn.net/u010320108/article/details/82854014</p>
</blockquote>
<h4 id="单进程文件句柄限制">单进程文件句柄限制</h4>
<p>句柄数限制又分为系统总限制和单进程限制.</p>
<p>使用命令 ulimit -n 可以看到系统对于单个进程的限制，即open files。执行命令 ulimit -a 如下:</p>
<pre><code>[root@k8s-node-2 ~]# ulimit  -a
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 31189
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 204800
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 204800
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
</code></pre>
<p>open files 204800 表示登录的用户（root），每个进程可以打开204800个句柄，当然总和不能超过file-max限制。</p>
<p><strong>临时修改该值</strong></p>
<pre><code># 临时修改
ulimit -HSn 10000 将open-files 修改为10000，退出当前shell后即失效。H和S选项表示硬限制和软限制，下面有解释，省略的话表示同时修改。
</code></pre>
<p><strong>永久修改该值</strong></p>
<pre><code># 永久修改
vim /etc/security/limits.conf
root soft nofile 204800
root hard nofile 204800
* soft nofile 204800
* hard nofile 204800
</code></pre>
<p><strong>该值的最大限制</strong></p>
<p>这个值是有上限的，这个上限的值设置在 <code>/proc/sys/fs/nr_open</code> ,默认值为 1048576，nr_open 表示一个线程最多能分配的文件句柄数。</p>
<pre><code>[root@k8s-node-2 goim]# cat  /proc/sys/fs/nr_open
1048576
</code></pre>
<h4 id="系统总打开文件句柄限制">系统总打开文件句柄限制</h4>
<p>系统级的限制在这个文件中 /proc/sys/fs/file-max</p>
<pre><code>[root@k8s-node-2 goim]# cat /proc/sys/fs/file-max
790774
</code></pre>
<p>file-max指定了系统范围内所有进程可以打开的文件句柄限制。修改上面那个文件也是临时生效的，重启后会失效。如果要永久生效，则要修改这个文件， <code>/etc/sysctl.conf</code>。</p>
<pre><code>fs.file-max  = 6815744
</code></pre>
<p><code>lsof -p 进程pid</code>  查看单个进程打开的文件句柄</p>
<p><code>/proc/sys/fs/file-nr</code>记录当前系统打开的句柄数</p>
<pre><code class="language-shell">[root@k8s-node-2 goim]# cat /proc/sys/fs/file-nr
2080	0	790774
# 第一列表示已打开的句柄数
# 第二列表示已分配但是未使用的句柄数
# 第三列表示系统总的句柄数，即 file-max
</code></pre>
<h4 id="文件打开说明小节">文件打开说明小节</h4>
<pre><code>1、所有进程能够打开的文件句柄总数不能超过 file-max
2、单个进程打开的句柄数不能超过nofile soft limit
3、nofile soft limit 的设置不能超过nofile hard limit
4、nofile hard limit 的设置不能超过 nr_open
5、nr_open 的设置不能超过 file-max
</code></pre>
<h4 id="系统优化">系统优化</h4>
<pre><code># /etc/sysctl.conf 增加下面内容
fs.file-max  = 6815744

# /etc/security/limits.conf 增加下面内容
* soft nofile 204800
* hard nofile 204800
</code></pre>
<h3 id="linux端口的相关设置">linux端口的相关设置</h3>
<blockquote>
<p>参考地址：https://blog.csdn.net/xf_87/article/details/91810087</p>
</blockquote>
<p><strong>查看：</strong></p>
<pre><code>[root@k8s-node-2 goim]# sysctl -a |grep ipv4.ip_local_port_range
net.ipv4.ip_local_port_range = 10000	60999
</code></pre>
<p><strong>修改：</strong></p>
<pre><code># 编辑/etc/sysctl.conf 在文件中新增以下内容
net.ipv4.ip_local_port_range = 1024 65535
</code></pre>
<h3 id="linux-tcp相关设置">linux TCP相关设置</h3>
<p><strong>linux服务器常见的几种状态</strong></p>
<ul>
<li>TIME_WAIT (主动关闭方的状态)</li>
<li>CLOSE_WAIT（被动关闭方的状态）</li>
<li>FIN_WAIT1（被动关闭方的状态）</li>
<li>FIN_WAIT2（主动关闭方的状态）</li>
</ul>
<p><strong>大并发报错</strong></p>
<pre><code>Cannot assign requested address
</code></pre>
<blockquote>
<p>参考地址：</p>
<p>https://www.jianshu.com/p/51a953b789a4</p>
<p>https://www.cnblogs.com/dadonggg/p/8778318.html</p>
<p>https://blog.csdn.net/libaineu2004/article/details/78886182</p>
</blockquote>
<p>原因：高并发的场景下，就会出现端口不足(主动关闭连接的一方，连接会处在TIME-WAIT的状态下，需要等2MSL时间后，系统才会回收这条连接，端口才可以继续被使用）。</p>
<p>解决：</p>
<pre><code># 编辑/etc/sysctl.conf 在文件中新增以下内容
net.ipv4.tcp_tw_reuse = 1 
#开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；
net.ipv4.tcp_tw_recycle = 1 
#开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。
net.ipv4.tcp_max_tw_buckets = 5000
# 表示系统同时保持TIME_WAIT套接字的最大数量，如果超过这个数字，TIME_WAIT套接字将立刻被清除并打印警告信息。默认为180000，改为5000。
</code></pre>
<pre><code>close wait 过多
</code></pre>
<p>在被动关闭连接情况下，在已经接收到FIN，但是还没有发送自己的FIN的时刻，连接处于CLOSE_WAIT状态。<br>
通常来讲，CLOSE_WAIT状态的持续时间应该很短，正如SYN_RCVD状态。但是在一些特殊情况下，就会出现连接长时间处于CLOSE_WAIT状态的情况。</p>
<p>出现大量close_wait的现象，主要原因是某种情况下对方关闭了socket链接，但是我方忙与读或者写，没有关闭连接。代码需要判断socket，一旦读到0，断开连接，read返回负，检查一下errno，如果不是AGAIN，就断开连接。</p>
<p>解决：</p>
<blockquote>
<p>参考地址：https://blog.csdn.net/wwd0501/article/details/78674170</p>
</blockquote>
<pre><code># 编辑/etc/sysctl.conf 在文件中新增以下内容
net.ipv4.tcp_keepalive_time = 1800 #当keepalive打开的情况下，TCP发送keepalive消息的频率
net.ipv4.tcp_keepalive_probes = 3 #TCP发送keepalive探测以确定该连接已经断开的次数。
net.ipv4.tcp_keepalive_intvl = 15 #当探测没有确认时，重新发送探测的频度。探测消息发送的频率
</code></pre>
<h3 id="大并发优化总结">大并发优化总结</h3>
<blockquote>
<p>参考地址：https://my.oschina.net/shyloveliyi/blog/2979058</p>
</blockquote>
<p>最大文件描述符</p>
<pre><code>ulimit -SHn 1024000 
echo &quot;ulimit -SHn 1024000&quot; &gt;&gt; /etc/rc.d/rc.local 
source /etc/rc.d/rc.local
</code></pre>
<p>内核参数优化<code>/etc/sysctl.conf</code></p>
<pre><code>#关闭ipv6
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1

#决定检查过期多久邻居条目
net.ipv4.neigh.default.gc_stale_time=120

#使用arp_announce / arp_ignore解决ARP映射问题
net.ipv4.conf.default.arp_announce = 2
net.ipv4.conf.all.arp_announce=2
net.ipv4.conf.lo.arp_announce=2 # 避免放大攻击
net.ipv4.icmp_echo_ignore_broadcasts = 1 # 开启恶意icmp错误消息保护
net.ipv4.icmp_ignore_bogus_error_responses = 1

#处理无源路由的包
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.conf.default.accept_source_route = 0

#core文件名中添加pid作为扩展名
kernel.core_uses_pid = 1 # 开启SYN洪水攻击保护
net.ipv4.tcp_syncookies = 1

#修改消息队列长度
kernel.msgmnb = 65536
kernel.msgmax = 65536

#timewait的数量，默认180000
net.ipv4.tcp_max_tw_buckets = 6000
net.ipv4.tcp_sack = 1
net.ipv4.tcp_window_scaling = 1
net.ipv4.tcp_rmem = 4096 87380 4194304
net.ipv4.tcp_wmem = 4096 16384 4194304
net.core.wmem_default = 8388608
net.core.rmem_default = 8388608
net.core.rmem_max = 16777216
net.core.wmem_max = 16777216

#限制仅仅是为了防止简单的DoS 攻击
net.ipv4.tcp_max_orphans = 3276800

#未收到客户端确认信息的连接请求的最大值
net.ipv4.tcp_max_syn_backlog = 262144
net.ipv4.tcp_timestamps = 0

#内核放弃建立连接之前发送SYNACK 包的数量
net.ipv4.tcp_synack_retries = 1

#内核放弃建立连接之前发送SYN 包的数量
net.ipv4.tcp_syn_retries = 1

#启用timewait 快速回收
net.ipv4.tcp_tw_recycle = 1

#开启重用。允许将TIME-WAIT sockets 重新用于新的TCP 连接
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_mem = 94500000 915000000 927000000
net.ipv4.tcp_fin_timeout = 1
</code></pre>
<p>然后执行以下命令重载配置</p>
<pre><code> sysctl -p
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes双栈集群安装]]></title>
        <id>https://js2hlu.coding-pages.com/post/2020021501/</id>
        <link href="https://js2hlu.coding-pages.com/post/2020021501/">
        </link>
        <updated>2020-02-15T10:14:41.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>目前因为IPV4地址耗尽，国家实行推广IPV6的网络生产场景，目前我们公司也将进行应用的IPV6的改造，IPV4和IPV6双栈是过渡期最好的选择。</p>
</blockquote>
<h3 id="环境说明">环境说明</h3>
<table>
<thead>
<tr>
<th>角色</th>
<th>主机名</th>
<th style="text-align:left">IP地址</th>
<th style="text-align:left">IPV6地址</th>
</tr>
</thead>
<tbody>
<tr>
<td>master</td>
<td>master</td>
<td style="text-align:left">100.64.13.141</td>
<td style="text-align:left">240e:f:a004:800:1:13:6440:d8d</td>
</tr>
</tbody>
</table>
<h3 id="安装准备">安装准备</h3>
<p>根据官网文档提示，使用双栈IPv6需要进行以下操作，可以在kubeam配置文件中进行声明。</p>
<ul>
<li>kube-controller-manager:
<ul>
<li><code>--feature-gates=&quot;IPv6DualStack=true&quot;</code></li>
<li><code>--cluster-cidr=,</code> eg. <code>--cluster-cidr=10.244.0.0/16,fc00::/24</code></li>
<li><code>--service-cluster-ip-range=,</code></li>
<li><code>--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6</code> defaults to /24 for IPv4 and /64 for IPv6</li>
</ul>
</li>
<li>kubelet:
<ul>
<li><code>--feature-gates=&quot;IPv6DualStack=true&quot;</code></li>
</ul>
</li>
<li>kube-proxy:
<ul>
<li><code>--proxy-mode=ipvs</code></li>
<li><code>--cluster-cidrs=,</code></li>
<li><code>--feature-gates=&quot;IPv6DualStack=true&quot;</code></li>
</ul>
</li>
</ul>
<pre><code># 准备满足以上需求的kubeam.yaml文件，该文件适用于kubeadm V1.17.3版本，其他版本请自行更新
# 安装时候替换advertiseAddress为实际环境IP
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: 08732j.nr8gmywciiydc057
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 100.64.13.141
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: master
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: cmop-cluster
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
kind: ClusterConfiguration
kubernetesVersion: v1.17.2
networking:
  podSubnet: 10.244.0.0/16,fc00::/64
  serviceSubnet: 10.96.0.0/12,fd00::/112
scheduler: {}
featureGates:
  IPv6DualStack: true
---
# enable ipvs
#https://godoc.org/k8s.io/kube-proxy/config/v1alpha1#KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs
ipvs:
  minSyncPeriod: 1s
  scheduler: rr
  syncPeriod: 10s
featureGates:
  IPv6DualStack: true
---
#https://godoc.org/k8s.io/kubelet/config/v1beta1#KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
failSwapOn: false
featureGates:
  IPv6DualStack: true
</code></pre>
<h3 id="安装单主集群">安装单主集群</h3>
<h4 id="初始化集群">初始化集群</h4>
<pre><code>[root@master tmp]# kubeadm  init --config kubeadm.yaml
W0215 17:53:16.361609   10779 validation.go:28] Cannot validate kube-proxy config - no validator is available
W0215 17:53:16.361729   10779 validation.go:28] Cannot validate kubelet config - no validator is available
...
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 100.64.13.141:6443 --token 08732j.nr8gmywciiydc057 \
    --discovery-token-ca-cert-hash sha256:51ce864e8beae0f1288f1ae9dc88a665ed13809d2ba461b0058e4d29b8f388ee

[root@master tmp]# mkdir -p $HOME/.kube
[root@master tmp]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
cp：是否覆盖&quot;/root/.kube/config&quot;？ y
[root@master tmp]# sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>
<h4 id="安装网络插件">安装网络插件</h4>
<p><strong>准备calico.yaml</strong></p>
<pre><code># 准备calico.yaml,内容如下：
---
# Source: calico/templates/calico-config.yaml
# This ConfigMap is used to configure a self-hosted Calico installation.
kind: ConfigMap
apiVersion: v1
metadata:
  name: calico-config
  namespace: kube-system
data:
  # Typha is disabled.
  typha_service_name: &quot;none&quot;
  # Configure the backend to use.
  calico_backend: &quot;bird&quot;

  # Configure the MTU to use
  veth_mtu: &quot;1440&quot;

  # The CNI network configuration to install on each node.  The special
  # values in this config will be automatically populated.
  cni_network_config: |-
    {
      &quot;name&quot;: &quot;k8s-pod-network&quot;,
      &quot;cniVersion&quot;: &quot;0.3.1&quot;,
      &quot;plugins&quot;: [
        {
          &quot;type&quot;: &quot;calico&quot;,
          &quot;log_level&quot;: &quot;info&quot;,
          &quot;datastore_type&quot;: &quot;kubernetes&quot;,
          &quot;nodename&quot;: &quot;__KUBERNETES_NODE_NAME__&quot;,
          &quot;mtu&quot;: __CNI_MTU__,
          &quot;ipam&quot;: {
              &quot;type&quot;: &quot;calico-ipam&quot;,
              &quot;assign_ipv4&quot;: &quot;true&quot;,
              &quot;assign_ipv6&quot;: &quot;true&quot;
          },
          &quot;policy&quot;: {
              &quot;type&quot;: &quot;k8s&quot;
          },
          &quot;kubernetes&quot;: {
              &quot;kubeconfig&quot;: &quot;__KUBECONFIG_FILEPATH__&quot;
          }
        },
        {
          &quot;type&quot;: &quot;portmap&quot;,
          &quot;snat&quot;: true,
          &quot;capabilities&quot;: {&quot;portMappings&quot;: true}
        },
        {
          &quot;type&quot;: &quot;bandwidth&quot;,
          &quot;capabilities&quot;: {&quot;bandwidth&quot;: true}
        }
      ]
    }

---
# Source: calico/templates/kdd-crds.yaml
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: felixconfigurations.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: FelixConfiguration
    plural: felixconfigurations
    singular: felixconfiguration
---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: ipamblocks.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: IPAMBlock
    plural: ipamblocks
    singular: ipamblock

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: blockaffinities.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: BlockAffinity
    plural: blockaffinities
    singular: blockaffinity

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: ipamhandles.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: IPAMHandle
    plural: ipamhandles
    singular: ipamhandle

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: ipamconfigs.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: IPAMConfig
    plural: ipamconfigs
    singular: ipamconfig

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: bgppeers.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: BGPPeer
    plural: bgppeers
    singular: bgppeer

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: bgpconfigurations.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: BGPConfiguration
    plural: bgpconfigurations
    singular: bgpconfiguration

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: ippools.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: IPPool
    plural: ippools
    singular: ippool

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: hostendpoints.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: HostEndpoint
    plural: hostendpoints
    singular: hostendpoint

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: clusterinformations.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: ClusterInformation
    plural: clusterinformations
    singular: clusterinformation

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: globalnetworkpolicies.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: GlobalNetworkPolicy
    plural: globalnetworkpolicies
    singular: globalnetworkpolicy

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: globalnetworksets.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: GlobalNetworkSet
    plural: globalnetworksets
    singular: globalnetworkset

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: networkpolicies.crd.projectcalico.org
spec:
  scope: Namespaced
  group: crd.projectcalico.org
  version: v1
  names:
    kind: NetworkPolicy
    plural: networkpolicies
    singular: networkpolicy

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: networksets.crd.projectcalico.org
spec:
  scope: Namespaced
  group: crd.projectcalico.org
  version: v1
  names:
    kind: NetworkSet
    plural: networksets
    singular: networkset
---
# Source: calico/templates/rbac.yaml

# Include a clusterrole for the kube-controllers component,
# and bind it to the calico-kube-controllers serviceaccount.
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: calico-kube-controllers
rules:
  # Nodes are watched to monitor for deletions.
  - apiGroups: [&quot;&quot;]
    resources:
      - nodes
    verbs:
      - watch
      - list
      - get
  # Pods are queried to check for existence.
  - apiGroups: [&quot;&quot;]
    resources:
      - pods
    verbs:
      - get
  # IPAM resources are manipulated when nodes are deleted.
  - apiGroups: [&quot;crd.projectcalico.org&quot;]
    resources:
      - ippools
    verbs:
      - list
  - apiGroups: [&quot;crd.projectcalico.org&quot;]
    resources:
      - blockaffinities
      - ipamblocks
      - ipamhandles
    verbs:
      - get
      - list
      - create
      - update
      - delete
  # Needs access to update clusterinformations.
  - apiGroups: [&quot;crd.projectcalico.org&quot;]
    resources:
      - clusterinformations
    verbs:
      - get
      - create
      - update
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: calico-kube-controllers
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: calico-kube-controllers
subjects:
- kind: ServiceAccount
  name: calico-kube-controllers
  namespace: kube-system
---
# Include a clusterrole for the calico-node DaemonSet,
# and bind it to the calico-node serviceaccount.
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: calico-node
rules:
  # The CNI plugin needs to get pods, nodes, and namespaces.
  - apiGroups: [&quot;&quot;]
    resources:
      - pods
      - nodes
      - namespaces
    verbs:
      - get
  - apiGroups: [&quot;&quot;]
    resources:
      - endpoints
      - services
    verbs:
      # Used to discover service IPs for advertisement.
      - watch
      - list
      # Used to discover Typhas.
      - get
  - apiGroups: [&quot;&quot;]
    resources:
      - nodes/status
    verbs:
      # Needed for clearing NodeNetworkUnavailable flag.
      - patch
      # Calico stores some configuration information in node annotations.
      - update
  # Watch for changes to Kubernetes NetworkPolicies.
  - apiGroups: [&quot;networking.k8s.io&quot;]
    resources:
      - networkpolicies
    verbs:
      - watch
      - list
  # Used by Calico for policy information.
  - apiGroups: [&quot;&quot;]
    resources:
      - pods
      - namespaces
      - serviceaccounts
    verbs:
      - list
      - watch
  # The CNI plugin patches pods/status.
  - apiGroups: [&quot;&quot;]
    resources:
      - pods/status
    verbs:
      - patch
  # Calico monitors various CRDs for config.
  - apiGroups: [&quot;crd.projectcalico.org&quot;]
    resources:
      - globalfelixconfigs
      - felixconfigurations
      - bgppeers
      - globalbgpconfigs
      - bgpconfigurations
      - ippools
      - ipamblocks
      - globalnetworkpolicies
      - globalnetworksets
      - networkpolicies
      - networksets
      - clusterinformations
      - hostendpoints
      - blockaffinities
    verbs:
      - get
      - list
      - watch
  # Calico must create and update some CRDs on startup.
  - apiGroups: [&quot;crd.projectcalico.org&quot;]
    resources:
      - ippools
      - felixconfigurations
      - clusterinformations
    verbs:
      - create
      - update
  # Calico stores some configuration information on the node.
  - apiGroups: [&quot;&quot;]
    resources:
      - nodes
    verbs:
      - get
      - list
      - watch
  # These permissions are only requried for upgrade from v2.6, and can
  # be removed after upgrade or on fresh installations.
  - apiGroups: [&quot;crd.projectcalico.org&quot;]
    resources:
      - bgpconfigurations
      - bgppeers
    verbs:
      - create
      - update
  # These permissions are required for Calico CNI to perform IPAM allocations.
  - apiGroups: [&quot;crd.projectcalico.org&quot;]
    resources:
      - blockaffinities
      - ipamblocks
      - ipamhandles
    verbs:
      - get
      - list
      - create
      - update
      - delete
  - apiGroups: [&quot;crd.projectcalico.org&quot;]
    resources:
      - ipamconfigs
    verbs:
      - get
  # Block affinities must also be watchable by confd for route aggregation.
  - apiGroups: [&quot;crd.projectcalico.org&quot;]
    resources:
      - blockaffinities
    verbs:
      - watch
  # The Calico IPAM migration needs to get daemonsets. These permissions can be
  # removed if not upgrading from an installation using host-local IPAM.
  - apiGroups: [&quot;apps&quot;]
    resources:
      - daemonsets
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: calico-node
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: calico-node
subjects:
- kind: ServiceAccount
  name: calico-node
  namespace: kube-system

---
# Source: calico/templates/calico-node.yaml
# This manifest installs the calico-node container, as well
# as the CNI plugins and network config on
# each master and worker node in a Kubernetes cluster.
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: calico-node
  namespace: kube-system
  labels:
    k8s-app: calico-node
spec:
  selector:
    matchLabels:
      k8s-app: calico-node
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        k8s-app: calico-node
      annotations:
        # This, along with the CriticalAddonsOnly toleration below,
        # marks the pod as a critical add-on, ensuring it gets
        # priority scheduling and that its resources are reserved
        # if it ever gets evicted.
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      nodeSelector:
        kubernetes.io/os: linux
      hostNetwork: true
      tolerations:
        # Make sure calico-node gets scheduled on all nodes.
        - effect: NoSchedule
          operator: Exists
        # Mark the pod as a critical add-on for rescheduling.
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoExecute
          operator: Exists
      serviceAccountName: calico-node
      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a &quot;force
      # deletion&quot;: https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.
      terminationGracePeriodSeconds: 0
      priorityClassName: system-node-critical
      initContainers:
        # This container performs upgrade from host-local IPAM to calico-ipam.
        # It can be deleted if this is a fresh installation, or if you have already
        # upgraded to use calico-ipam.
        - name: upgrade-ipam
          image: calico/cni:v3.12.0
          command: [&quot;/opt/cni/bin/calico-ipam&quot;, &quot;-upgrade&quot;]
          env:
            - name: KUBERNETES_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: CALICO_NETWORKING_BACKEND
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: calico_backend
          volumeMounts:
            - mountPath: /var/lib/cni/networks
              name: host-local-net-dir
            - mountPath: /host/opt/cni/bin
              name: cni-bin-dir
          securityContext:
            privileged: true
        # This container installs the CNI binaries
        # and CNI network config file on each node.
        - name: install-cni
          image: calico/cni:v3.12.0
          command: [&quot;/install-cni.sh&quot;]
          env:
            # Name of the CNI config file to create.
            - name: CNI_CONF_NAME
              value: &quot;10-calico.conflist&quot;
            # The CNI network config to install on each node.
            - name: CNI_NETWORK_CONFIG
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: cni_network_config
            # Set the hostname based on the k8s node name.
            - name: KUBERNETES_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            # CNI MTU Config variable
            - name: CNI_MTU
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: veth_mtu
            # Prevents the container from sleeping forever.
            - name: SLEEP
              value: &quot;false&quot;
          volumeMounts:
            - mountPath: /host/opt/cni/bin
              name: cni-bin-dir
            - mountPath: /host/etc/cni/net.d
              name: cni-net-dir
          securityContext:
            privileged: true
        # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes
        # to communicate with Felix over the Policy Sync API.
        - name: flexvol-driver
          image: calico/pod2daemon-flexvol:v3.12.0
          volumeMounts:
          - name: flexvol-driver-host
            mountPath: /host/driver
          securityContext:
            privileged: true
      containers:
        # Runs calico-node container on each Kubernetes node.  This
        # container programs network policy and routes on each
        # host.
        - name: calico-node
          image: calico/node:v3.12.0
          env:
            # Use Kubernetes API as the backing datastore.
            - name: DATASTORE_TYPE
              value: &quot;kubernetes&quot;
            # Wait for the datastore.
            - name: WAIT_FOR_DATASTORE
              value: &quot;true&quot;
            # Set based on the k8s node name.
            - name: NODENAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            # Choose the backend to use.
            - name: CALICO_NETWORKING_BACKEND
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: calico_backend
            # Cluster type to identify the deployment type
            - name: CLUSTER_TYPE
              value: &quot;k8s,bgp&quot;
            # Auto-detect the BGP IP address.
            - name: IP
              value: &quot;autodetect&quot;
            - name: IP6
              value: &quot;autodetect&quot;
            # Enable IPIP
            - name: CALICO_IPV4POOL_IPIP
              value: &quot;Always&quot;
            - name: CALICO_IPV6POOL_IPIP
              value: &quot;Always&quot;
            # Set MTU for tunnel device used if ipip is enabled
            - name: FELIX_IPINIPMTU
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: veth_mtu
            # The default IPv4 pool to create on startup if none exists. Pod IPs will be
            # chosen from this range. Changing this value after installation will have
            # no effect. This should fall within `--cluster-cidr`.
            - name: CALICO_IPV4POOL_CIDR
              value: &quot;10.244.0.0/16&quot;
            - name: CALICO_IPV6POOL_CIDR
              value: &quot;fc00::/64&quot;
            # Disable file logging so `kubectl logs` works.
            - name: CALICO_DISABLE_FILE_LOGGING
              value: &quot;true&quot;
            # Set Felix endpoint to host default action to ACCEPT.
            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
              value: &quot;ACCEPT&quot;
            # Disable IPv6 on Kubernetes.
            - name: FELIX_IPV6SUPPORT
              value: &quot;true&quot;
            # Set Felix logging to &quot;info&quot;
            - name: FELIX_LOGSEVERITYSCREEN
              value: &quot;info&quot;
            - name: FELIX_HEALTHENABLED
              value: &quot;true&quot;
          securityContext:
            privileged: true
          resources:
            requests:
              cpu: 250m
          livenessProbe:
            exec:
              command:
              - /bin/calico-node
              - -felix-live
              - -bird-live
            periodSeconds: 10
            initialDelaySeconds: 10
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
              - /bin/calico-node
              - -felix-ready
              - -bird-ready
            periodSeconds: 10
          volumeMounts:
            - mountPath: /lib/modules
              name: lib-modules
              readOnly: true
            - mountPath: /run/xtables.lock
              name: xtables-lock
              readOnly: false
            - mountPath: /var/run/calico
              name: var-run-calico
              readOnly: false
            - mountPath: /var/lib/calico
              name: var-lib-calico
              readOnly: false
            - name: policysync
              mountPath: /var/run/nodeagent
      volumes:
        # Used by calico-node.
        - name: lib-modules
          hostPath:
            path: /lib/modules
        - name: var-run-calico
          hostPath:
            path: /var/run/calico
        - name: var-lib-calico
          hostPath:
            path: /var/lib/calico
        - name: xtables-lock
          hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
        # Used to install CNI.
        - name: cni-bin-dir
          hostPath:
            path: /opt/cni/bin
        - name: cni-net-dir
          hostPath:
            path: /etc/cni/net.d
        # Mount in the directory for host-local IPAM allocations. This is
        # used when upgrading from host-local to calico-ipam, and can be removed
        # if not using the upgrade-ipam init container.
        - name: host-local-net-dir
          hostPath:
            path: /var/lib/cni/networks
        # Used to create per-pod Unix Domain Sockets
        - name: policysync
          hostPath:
            type: DirectoryOrCreate
            path: /var/run/nodeagent
        # Used to install Flex Volume Driver
        - name: flexvol-driver-host
          hostPath:
            type: DirectoryOrCreate
            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: calico-node
  namespace: kube-system

---
# Source: calico/templates/calico-kube-controllers.yaml

# See https://github.com/projectcalico/kube-controllers
apiVersion: apps/v1
kind: Deployment
metadata:
  name: calico-kube-controllers
  namespace: kube-system
  labels:
    k8s-app: calico-kube-controllers
spec:
  # The controllers can only have a single active instance.
  replicas: 1
  selector:
    matchLabels:
      k8s-app: calico-kube-controllers
  strategy:
    type: Recreate
  template:
    metadata:
      name: calico-kube-controllers
      namespace: kube-system
      labels:
        k8s-app: calico-kube-controllers
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        # Mark the pod as a critical add-on for rescheduling.
        - key: CriticalAddonsOnly
          operator: Exists
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
      serviceAccountName: calico-kube-controllers
      priorityClassName: system-cluster-critical
      containers:
        - name: calico-kube-controllers
          image: calico/kube-controllers:v3.12.0
          env:
            # Choose which controllers to run.
            - name: ENABLED_CONTROLLERS
              value: node
            - name: DATASTORE_TYPE
              value: kubernetes
          readinessProbe:
            exec:
              command:
              - /usr/bin/check-status
              - -r

---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: calico-kube-controllers
  namespace: kube-system
---
# Source: calico/templates/calico-etcd-secrets.yaml

---
# Source: calico/templates/calico-typha.yaml

---
# Source: calico/templates/configure-canal.yaml
</code></pre>
<p><strong>安装calico</strong></p>
<pre><code>[root@master tmp]# kubectl  apply -f calico.yaml
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
...
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
</code></pre>
<h3 id="测试集群">测试集群</h3>
<p><strong>创建IPV6 SVC</strong></p>
<pre><code>[root@master tmp]# cat svc.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
  name: my-service
spec:
  ipFamily: IPv6
  ports:
  - port: 80
    protocol: TCP
    targetPort: 9376
  selector:
    app: MyApp
  type: ClusterIP
[root@master tmp]# cat svc.yaml |kubectl  apply -f -
service/my-service unchanged
[root@master tmp]# kubectl  get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   12m
my-service   ClusterIP   fd00::d6c5   &lt;none&gt;        80/TCP    4m15s
[root@master tmp]# ping6 fd00::d6c5
PING fd00::d6c5(fd00::d6c5) 56 data bytes
64 bytes from fd00::d6c5: icmp_seq=1 ttl=64 time=0.261 ms
64 bytes from fd00::d6c5: icmp_seq=2 ttl=64 time=0.172 ms
^C
--- fd00::d6c5 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1000ms
rtt min/avg/max/mdev = 0.172/0.216/0.261/0.046 ms
</code></pre>
<p><strong>创建IPV6pod</strong></p>
<pre><code>[root@master tmp]# kubectl  create deploy tomcat --image=cmop-registry:30000/tomcat:v1
deployment.apps/tomcat created
[root@master tmp]# kubectl  taint node master node-role.kubernetes.io/master:NoSchedule-
node/master untainted
[root@master tmp]# kubectl  get pods
NAME                     READY   STATUS    RESTARTS   AGE
tomcat-9bf649785-4zxvl   1/1     Running   0          59s
[root@master tmp]# kubectl get pods tomcat-9bf649785-4zxvl -o go-template --template='{{range .status.podIPs}}{{printf &quot;%s \n&quot; .ip}}{{end}}'
10.244.219.68
fc00::34b8:247c:36da:db43
[root@master tmp]# ping6 fc00::34b8:247c:36da:db43
PING fc00::34b8:247c:36da:db43(fc00::34b8:247c:36da:db43) 56 data bytes
64 bytes from fc00::34b8:247c:36da:db43: icmp_seq=1 ttl=64 time=0.351 ms
64 bytes from fc00::34b8:247c:36da:db43: icmp_seq=2 ttl=64 time=0.171 ms
^C
--- fc00::34b8:247c:36da:db43 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 0.171/0.261/0.351/0.090 ms
</code></pre>
<h3 id="可能遇见的问题">可能遇见的问题</h3>
<p><strong>得不到IPV6地址：</strong></p>
<p>由于节点IPV6地址不可路由导致，需要修改IPV6接口的检测规则，比如：</p>
<pre><code>kubectl set env daemonset/calico-node -n kube-system IP6_AUTODETECTION_METHOD=&lt;autodetection-method&gt;
kubectl set env daemonset/calico-node -n kube-system IP6_AUTODETECTION_METHOD=can-reach=240E:120F:A004:1::6440:0D01
kubectl set env daemonset/calico-node -n kube-system IP6_AUTODETECTION_METHOD=interface=ens192
</code></pre>
<h3 id="参考地址">参考地址</h3>
<p>https://docs.projectcalico.org/networking/dual-stack</p>
<p>https://docs.projectcalico.org/networking/ip-autodetection#change-the-autodetection-method</p>
<p>https://kubernetes.io/docs/concepts/services-networking/dual-stack/#prerequisites</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes 生产环境优化]]></title>
        <id>https://js2hlu.coding-pages.com/post/2020010601/</id>
        <link href="https://js2hlu.coding-pages.com/post/2020010601/">
        </link>
        <updated>2020-01-16T03:19:02.000Z</updated>
        <content type="html"><![CDATA[<h3 id="系统稳定性">系统稳定性</h3>
<h4 id="节点异常采集">节点异常采集</h4>
<blockquote>
<p>解决方案<a href="">NPD</a>，NPD可以收集节点相关的异常报错如内核错误、OOM等</p>
</blockquote>
<p><strong>部署方式</strong></p>
<pre><code>kubectl apply -f &quot;https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/node-problem-detector/npd.yaml&quot;
</code></pre>
<p><strong>测试</strong></p>
<pre><code>[root@lab1 ~]# kubectl  get events -w
0s    Warning   TaskHung   node/lab1   kernel: INFO: task docker:20744 blocked for more than 1200 seconds.
</code></pre>
<pre><code>[root@lab1 ~]# sudo sh -c &quot;echo 'kernel: INFO: task docker:20744 blocked for more than 1200 seconds.' &gt;&gt; /dev/kmsg&quot;
</code></pre>
<h4 id="节点资源信息集中监控">节点资源信息集中监控</h4>
<blockquote>
<p>采用weave试图，查看容器和节点的实时开销，支持远程排序过滤等功能</p>
</blockquote>
<p><strong>部署方式</strong></p>
<pre><code>kubectl apply -f &quot;https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d '\n')&amp;k8s-service-type=LoadBalancer&quot;
</code></pre>
<figure data-type="image" tabindex="1"><a href="https://imgchr.com/i/ljBWXq"><img src="https://s2.ax1x.com/2020/01/16/ljBWXq.md.png" alt="ljBWXq.md.png" loading="lazy"></a></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes 设置节点异常时候POD调度时间]]></title>
        <id>https://js2hlu.coding-pages.com/post/2019121701/</id>
        <link href="https://js2hlu.coding-pages.com/post/2019121701/">
        </link>
        <updated>2019-12-17T03:32:19.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>防止节点异常后，pod要很长时间才能在其他node上运行起来，导致业务故障时间过长。</p>
</blockquote>
<h3 id="配置controller-manager">配置controller manager</h3>
<p>编辑配置文件<code>/etc/kubernetes/manifests/kube-controller-manager.yaml</code></p>
<pre><code>--node-monitor-grace-period=10s \
--node-monitor-period=3s \
--node-startup-grace-period=20s \
--pod-eviction-timeout=10s \
</code></pre>
<h3 id="pod-调度过程">pod 调度过程</h3>
<p>kubernetes节点失效后pod的调度过程：</p>
<ol>
<li>
<p>Master每隔一段时间和node联系一次，判定node是否失联，这个时间周期配置项为 node-monitor-period ，默认5s</p>
</li>
<li>
<p>当node失联后一段时间后，kubernetes判定node为notready状态，这段时长的配置项为 node-monitor-grace-period ，默认40s</p>
</li>
<li>
<p>当node失联后一段时间后，kubernetes判定node为unhealthy，这段时长的配置项为 node-startup-grace-period ，默认1m0s</p>
</li>
<li>
<p>当node失联后一段时间后，kubernetes开始删除原node上的pod，这段时长配置项为 pod-eviction-timeout ，默认5m0s</p>
</li>
</ol>
<p>在应用中，想要缩短pod的重启时间，可以修改上述几个参数</p>
<p><strong>参数解释</strong></p>
<pre><code>--node-monitor-grace-period duration     Default: 40s
 	Amount of time which we allow running Node to be unresponsive before marking it unhealthy. Must be N times more than kubelet's nodeStatusUpdateFrequency, where N means number of retries allowed for kubelet to post node status.
--node-monitor-period duration     Default: 5s
 	The period for syncing NodeStatus in NodeController.
--node-startup-grace-period duration     Default: 1m0s
 	Amount of time which we allow starting Node to be unresponsive before marking it unhealthy.
--pod-eviction-timeout duration     Default: 5m0s
 	The grace period for deleting pods on failed nodes.
</code></pre>
<h3 id="参考文档">参考文档</h3>
<p>https://www.cnblogs.com/Qing-840/p/9818404.html</p>
<p>https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes master 更换ip（单节点）]]></title>
        <id>https://js2hlu.coding-pages.com/post/2019121102/</id>
        <link href="https://js2hlu.coding-pages.com/post/2019121102/">
        </link>
        <updated>2019-12-11T06:44:47.000Z</updated>
        <content type="html"><![CDATA[<h3 id="问题分析">问题分析</h3>
<p>master ip地址变更以后，我们首先应该检查以下内容：</p>
<ol>
<li>
<p><code>/etc/kubernetes/manifests</code>下面的config配置文件，替换里面对应的ip</p>
</li>
<li>
<p>相关的证书文件</p>
</li>
<li>
<p>客户端文件</p>
</li>
</ol>
<h3 id="解决步骤">解决步骤</h3>
<h4 id="准备config文件">准备config文件</h4>
<blockquote>
<p>如果环境能出国网则不用进行该步骤,此文件为kubeadm.config<br>
使用该文件时候注意替换相关的API地址和端口等信息</p>
</blockquote>
<pre><code>apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 100.64.139.62
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master-2
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
kind: ClusterConfiguration
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
kubernetesVersion: v1.16.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
</code></pre>
<h4 id="修改配置文件">修改配置文件</h4>
<pre><code>[root@k8s-master-2 kubernetes]# cd /etc/kubernetes
[root@k8s-master-2 kubernetes]# find . -type f |xargs grep 100.64.139.60 |awk '{print $1}' |sort |uniq
./admin.conf:
./controller-manager.conf:
./kubelet.conf:
./manifests/etcd.yaml:
./manifests/kube-apiserver.yaml:
./scheduler.conf:
</code></pre>
<p>其中几个conf文件为kubeadm自动生成的带证书的客户端配置文件，需要修改的为<code>etcd.yaml</code>,<code>kube-apiserver.yaml</code>两个配置文件。将里面对应的ip地址修改为新的ip地址。</p>
<h4 id="生成新证书">生成新证书</h4>
<p>备份原始证书，根据<code>find</code>命令的输出，以下相关的服务证书需要更换<code>kubelt api proxy</code></p>
<pre><code># 备份原始证书
mv /etc/kubernetes/pki/apiserver.key /etc/kubernetes/pki/apiserver.key.old
mv /etc/kubernetes/pki/apiserver.crt /etc/kubernetes/pki/apiserver.crt.old
mv /etc/kubernetes/pki/apiserver-kubelet-client.crt /etc/kubernetes/pki/apiserver-kubelet-client.crt.old
mv /etc/kubernetes/pki/apiserver-kubelet-client.key /etc/kubernetes/pki/apiserver-kubelet-client.key.old
mv /etc/kubernetes/pki/front-proxy-client.crt /etc/kubernetes/pki/front-proxy-client.crt.old
mv /etc/kubernetes/pki/front-proxy-client.key /etc/kubernetes/pki/front-proxy-client.key.old

# 生成新证书
kubeadm init  phase certs apiserver --config kubeadm.config
kubeadm init  phase certs apiserver-kubelet-client --config kubeadm.config
kubeadm init  phase certs front-proxy-client --config kubeadm.config
</code></pre>
<h4 id="生成新的客户端文件">生成新的客户端文件</h4>
<pre><code>kubeadm  init phase kubeconfig admin --config kubeadm.config
kubeadm  init phase kubeconfig controller-manager --config kubeadm.config
kubeadm  init phase kubeconfig kubelet --config kubeadm.config
kubeadm  init phase kubeconfig scheduler --config kubeadm.config
</code></pre>
<h4 id="查看证书过期时间">查看证书过期时间</h4>
<pre><code>[root@k8s-master-2 pki]# kubeadm  alpha  certs check-expiration
CERTIFICATE                EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
admin.conf                 Dec 10, 2020 05:31 UTC   364d            no
apiserver                  Dec 10, 2020 05:30 UTC   364d            no
apiserver-etcd-client      Dec 10, 2020 05:31 UTC   364d            no
apiserver-kubelet-client   Dec 10, 2020 05:30 UTC   364d            no
controller-manager.conf    Dec 10, 2020 05:31 UTC   364d            no
etcd-healthcheck-client    Dec 10, 2020 05:31 UTC   364d            no
etcd-peer                  Dec 10, 2020 05:31 UTC   364d            no
etcd-server                Dec 10, 2020 05:30 UTC   364d            no
front-proxy-client         Dec 10, 2020 05:30 UTC   364d            no
scheduler.conf             Dec 10, 2020 05:31 UTC   364d            no
</code></pre>
<h4 id="重启服务">重启服务</h4>
<pre><code>service docker restart 
service kubelet restart
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[etcd故障后恢复]]></title>
        <id>https://js2hlu.coding-pages.com/post/2019121101/</id>
        <link href="https://js2hlu.coding-pages.com/post/2019121101/">
        </link>
        <updated>2019-12-11T04:46:33.000Z</updated>
        <content type="html"><![CDATA[<h3 id="etcd故障描述">etcd故障描述</h3>
<p>etcd集群采用raft算法,所以当集群能够正常运行的时候一定是存活节点数大于总节点数的一半。当少于总结点数一半时，集群会处于不可用状态，这种情况下我们需要恢复集群。此处也可以使用备份进行新节点恢复。</p>
<h3 id="故障模拟">故障模拟</h3>
<ol>
<li>
<p>首先我们需要搭建一个正常的集群，搭建集群参考文档<a href="https://tomcat1009.coding.me/hexo/post/2019102501/">etcd测试集群搭建</a>。</p>
</li>
<li>
<p>根据上述文档完成集群搭建后，选择任意两个节点进行宕机操作</p>
</li>
</ol>
<pre><code>docker stop etcd2
docker stop etcd3
</code></pre>
<h3 id="恢复">恢复</h3>
<h4 id="etcd1">etcd1</h4>
<p>使用剩下的节点<code>etcd1</code>进行如下操作</p>
<pre><code># 备份数据库，注意需要备份到持久化目录中
ETCDCTL_API=3 /root/local/bin/etcdctl snapshot save snapshot.db

# 删除容器
docker rm -f etcd1

# 启动新容器，跟上参数--force-new-cluster，集群恢复后需要取消该选项
docker run --restart=always --net host -it --name etcd1 -d -v /var/etcd:/var/etcd -v /etc/localtime:/etc/localtime registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.3.10 etcd --name etcd-s1 --auto-compaction-retention=1 --max-request-bytes=33554432 --quota-backend-bytes=8589934592 --data-dir=/var/etcd/etcd-data --listen-client-urls http://0.0.0.0:2379 --listen-peer-urls http://0.0.0.0:2380 --initial-advertise-peer-urls http://172.16.10.101:2380 --advertise-client-urls http://172.16.10.101:2379,http://172.16.10.101:2380 -initial-cluster-token etcd-cluster -initial-cluster &quot;etcd-s1=http://172.16.10.101:2380,etcd-s2=http://172.16.10.102:2380,etcd-s3=http://172.16.10.103:2380&quot; -initial-cluster-state new --force-new-cluster

# 查看并添加节点，需要一个一个添加，添加后再启动相应的节点
ETCDCTL_API=3 etcdctl member list
ETCDCTL_API=3 etcdctl member add etcd-s2 --peer-urls=http://172.16.10.102:2380

# 当完成etcd2的启动后再执行etcd3节点的注册添加
ETCDCTL_API=3 etcdctl member add etcd-s3 --peer-urls=http://172.16.10.103:2380
</code></pre>
<h4 id="etcd2">etcd2</h4>
<p>启动<code>etcd2</code>,此处需要注意参数<code>existing</code></p>
<pre><code># 删除etcd2容器，并清理本地etcd数据目录
docker rm -f etcd2
rm -rf /var/etcd

# 启动etcd2新容器
docker run --restart=always --net host -it --name etcd2 -d -v /var/etcd:/var/etcd -v /etc/localtime:/etc/localtime registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.3.10 etcd --name etcd-s2  --auto-compaction-retention=1 --max-request-bytes=33554432 --quota-backend-bytes=8589934592 --data-dir=/var/etcd/etcd-data --listen-client-urls http://0.0.0.0:2379 --listen-peer-urls http://0.0.0.0:2380 --initial-advertise-peer-urls http://172.16.10.102:2380 --advertise-client-urls http://172.16.10.102:2379,http://172.16.10.102:2380 -initial-cluster-token etcd-cluster -initial-cluster &quot;etcd-s1=http://172.16.10.101:2380,etcd-s2=http://172.16.10.102:2380&quot; -initial-cluster-state existing
</code></pre>
<h4 id="etcd3">etcd3</h4>
<p>启动<code>etcd3</code>,此处需要注意参数<code>existing</code></p>
<pre><code># 删除etcd3容器，并清理本地etcd数据目录
docker rm -f etcd3
rm -rf /var/etcd

# 启动etcd3容器
docker run --restart=always --net host -it --name etcd3 -d -v /var/etcd:/var/etcd -v /etc/localtime:/etc/localtime registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.3.10 etcd --name etcd-s3  --auto-compaction-retention=1 --max-request-bytes=33554432 --quota-backend-bytes=8589934592 --data-dir=/var/etcd/etcd-data --listen-client-urls http://0.0.0.0:2379 --listen-peer-urls http://0.0.0.0:2380 --initial-advertise-peer-urls http://172.16.10.103:2380 --advertise-client-urls http://172.16.10.103:2379,http://172.16.10.103:2380 -initial-cluster-token etcd-cluster -initial-cluster &quot;etcd-s1=http://172.16.10.101:2380,etcd-s2=http://172.16.10.102:2380,etcd-s3=http://172.16.10.103:2380&quot; -initial-cluster-state existing
</code></pre>
<h3 id="参考文档">参考文档</h3>
<p>https://blog.csdn.net/yinlongfei_love/article/details/87722341</p>
<p>https://www.cnblogs.com/wangbin/p/9426644.html</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes 新增master节点]]></title>
        <id>https://js2hlu.coding-pages.com/post/2019112201/</id>
        <link href="https://js2hlu.coding-pages.com/post/2019112201/">
        </link>
        <updated>2019-11-22T02:25:27.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>本方法使用于1.15之后的版本(实验版本为1.16.3)</p>
</blockquote>
<h3 id="初始化集群">初始化集群</h3>
<pre><code>[root@k8s-master-2 ~]# kubeadm init --control-plane-endpoint 100.64.139.60 --pod-network-cidr=10.244.0.0/16 --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers --upload-certs
...
kubeadm join 100.64.139.60:6443 --token 9vbxyg.dc7mupnjuznb8839 \
    --discovery-token-ca-cert-hash sha256:92fb288714ae020deba3740fab23c04e13d40f833b0ce9d0e33b3cb248dd33a5 \
    --control-plane --certificate-key 3181c80884a40d3df623564940c550a01f26e994ac94a3843c87700444ed9bee

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
&quot;kubeadm init phase upload-certs --upload-certs&quot; to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 100.64.139.60:6443 --token 9vbxyg.dc7mupnjuznb8839 \
    --discovery-token-ca-cert-hash sha256:92fb288714ae020deba3740fab23c04e13d40f833b0ce9d0e33b3cb248dd33a5
</code></pre>
<h3 id="后续新加入">后续新加入</h3>
<p><strong>node</strong></p>
<pre><code>kubeadm token create --print-join-command
</code></pre>
<p><strong>master</strong></p>
<pre><code># 子命令
kubeadm token create --print-join-command
kubeadm init phase upload-certs --upload-certs
# 单个命令全部生成
echo &quot;$(kubeadm token create --print-join-command) --control-plane --certificate-key $(kubeadm init phase upload-certs --upload-certs|tail -n 1)&quot;
</code></pre>
<h3 id="参考文档">参考文档</h3>
<p>https://www.cnblogs.com/Dev0ps/p/10980516.html</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[etcd测试集群搭建（http）]]></title>
        <id>https://js2hlu.coding-pages.com/post/2019102501/</id>
        <link href="https://js2hlu.coding-pages.com/post/2019102501/">
        </link>
        <updated>2019-10-25T07:35:10.000Z</updated>
        <content type="html"><![CDATA[<h3 id="集群搭建">集群搭建</h3>
<h4 id="资源列表">资源列表</h4>
<table>
<thead>
<tr>
<th>角色</th>
<th>ip地址</th>
<th>端口</th>
</tr>
</thead>
<tbody>
<tr>
<td>etcd1</td>
<td>172.16.10.101</td>
<td>2379,2380</td>
</tr>
<tr>
<td>ectd2</td>
<td>172.16.10.102</td>
<td>2379,2380</td>
</tr>
<tr>
<td>Etcd3</td>
<td>172.16.10.103</td>
<td>2379,2380</td>
</tr>
</tbody>
</table>
<h4 id="etcd1">etcd1</h4>
<pre><code>rm -rf /var/etcd
mkdir -p /var/etcd
docker rm etcd1 -f
docker run --restart=always --net host -it --name etcd1 -d \
-v /var/etcd:/var/etcd \
-v /etc/localtime:/etc/localtime \
registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.3.10 \
etcd --name etcd-s1 \
--auto-compaction-retention=1 --max-request-bytes=33554432 --quota-backend-bytes=8589934592 \
--data-dir=/var/etcd/etcd-data \
--listen-client-urls http://0.0.0.0:2379 \
--listen-peer-urls http://0.0.0.0:2380 \
--initial-advertise-peer-urls http://172.16.10.101:2380 \
--advertise-client-urls http://172.16.10.101:2379,http://172.16.10.101:2380 \
-initial-cluster-token etcd-cluster \
-initial-cluster &quot;etcd-s1=http://172.16.10.101:2380,etcd-s2=http://172.16.10.102:2380,etcd-s3=http://172.16.10.103:2380&quot; \
-initial-cluster-state new
</code></pre>
<h4 id="etcd2">etcd2</h4>
<pre><code>rm -rf /var/etcd
mkdir -p /var/etcd
docker rm etcd2 -f
docker run --restart=always --net host -it --name etcd2 -d \
-v /var/etcd:/var/etcd \
-v /etc/localtime:/etc/localtime \
registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.3.10 \
etcd --name etcd-s2  \
--auto-compaction-retention=1 --max-request-bytes=33554432 --quota-backend-bytes=8589934592 \
--data-dir=/var/etcd/etcd-data \
--listen-client-urls http://0.0.0.0:2379 \
--listen-peer-urls http://0.0.0.0:2380 \
--initial-advertise-peer-urls http://172.16.10.102:2380 \
--advertise-client-urls http://172.16.10.102:2379,http://172.16.10.102:2380 \
-initial-cluster-token etcd-cluster \
-initial-cluster &quot;etcd-s1=http://172.16.10.101:2380,etcd-s2=http://172.16.10.102:2380,etcd-s3=http://172.16.10.103:2380&quot; \
-initial-cluster-state new
</code></pre>
<h4 id="etcd3">etcd3</h4>
<pre><code>rm -rf /var/etcd
mkdir -p /var/etcd
docker rm etcd3 -f
docker run --restart=always --net host -it --name etcd3 -d \
-v /var/etcd:/var/etcd \
-v /etc/localtime:/etc/localtime \
registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.3.10 \
etcd --name etcd-s3  \
--auto-compaction-retention=1 --max-request-bytes=33554432 --quota-backend-bytes=8589934592 \
--data-dir=/var/etcd/etcd-data \
--listen-client-urls http://0.0.0.0:2379 \
--listen-peer-urls http://0.0.0.0:2380 \
--initial-advertise-peer-urls http://172.16.10.103:2380 \
--advertise-client-urls http://172.16.10.103:2379,http://172.16.10.103:2380 \
-initial-cluster-token etcd-cluster \
-initial-cluster &quot;etcd-s1=http://172.16.10.101:2380,etcd-s2=http://172.16.10.102:2380,etcd-s3=http://172.16.10.103:2380&quot; \
-initial-cluster-state new
</code></pre>
<h3 id="检查">检查</h3>
<pre><code># 检查
ETCDCTL_API=3 etcdctl endpoint health
ETCDCTL_API=3 etcdctl member list
ETCDCTL_API=3 etcdctl put foo bar
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[使用kubeadm 新加入节点（原始token过期后）]]></title>
        <id>https://js2hlu.coding-pages.com/post/2019091201/</id>
        <link href="https://js2hlu.coding-pages.com/post/2019091201/">
        </link>
        <updated>2019-09-12T04:47:00.000Z</updated>
        <content type="html"><![CDATA[<h2 id="kubeadm-join">kubeadm join</h2>
<blockquote>
<p>kubeadm init 安装完成后你会得到以下的输出，使用join指令可以新增节点到集群,此token 有效期为24小时</p>
</blockquote>
<pre><code>You should now deploy a pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join 18.16.202.35:6443 --token zr8n5j.yfkanjio0lfsupc0 --discovery-token-ca-cert-hash sha256:380b775b7f9ea362d45e4400be92adc4f71d86793ba6aae091ddb53c489d218c
</code></pre>
<h2 id="kubeadm-token">kubeadm token</h2>
<blockquote>
<p>在新节点没有拿到证书以前，新节点和api server的通信是通过token和ca的签名完成的，具体的步骤如下</p>
</blockquote>
<pre><code># 生成token
[root@node1 flannel]# kubeadm  token create
kiyfhw.xiacqbch8o8fa8qj
[root@node1 flannel]# kubeadm  token list
TOKEN                     TTL         EXPIRES                     USAGES                   DESCRIPTION   EXTRA GROUPS
gvvqwk.hn56nlsgsv11mik6   &lt;invalid&gt;   2018-10-25T14:16:06+08:00   authentication,signing   &lt;none&gt;        system:bootstrappers:kubeadm:default-node-token
kiyfhw.xiacqbch8o8fa8qj   23h         2018-10-27T06:39:24+08:00   authentication,signing   &lt;none&gt;        system:bootstrappers:kubeadm:default-node-token
</code></pre>
<pre><code># 生成ca的sha256 hash值
[root@node1 flannel]# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
5417eb1b68bd4e7a4c82aded83abc55ec91bd601e45734d6aba85de8b1ebb057
</code></pre>
<pre><code># 组装join命令
kubeadm join 18.16.202.35:6443 --token kiyfhw.xiacqbch8o8fa8qj --discovery-token-ca-cert-hash sha256:5417eb1b68bd4e7a4c82aded83abc55ec91bd601e45734d6aba85de8b1ebb057
</code></pre>
<pre><code># 一步完成以上步骤
kubeadm token create --print-join-command
</code></pre>
<pre><code># 手动生成token，完成命令打印
token=$(kubeadm token generate)
kubeadm token create $token --print-join-command --ttl=0
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes1.15.3离线安装]]></title>
        <id>https://js2hlu.coding-pages.com/post/2019090901/</id>
        <link href="https://js2hlu.coding-pages.com/post/2019090901/">
        </link>
        <updated>2019-09-09T10:09:51.000Z</updated>
        <content type="html"><![CDATA[<h1 id="kubeadm-k8s集群离线安装">kubeadm k8s集群离线安装</h1>
<h2 id="下载软件包">下载软件包</h2>
<pre><code>yum install --downloadonly --downloaddir=./  kubeadm-1.15.3-0 kubelet-1.15.3-0 kubectl-1.15.3-0 docker-ce-18.06.3.ce-3.el7 keepalived ipset ipvsadm   
</code></pre>
<h2 id="下载镜像并打包">下载镜像并打包</h2>
<pre><code>kubeadm  config images pull --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers

for x in $(docker images |awk  '{print $1&quot;:&quot;$2}'|grep -v REP);do docker save -o $(echo $x|awk -F '/' '{print $3}').tar $x;done

</code></pre>
<h2 id="恢复镜像">恢复镜像</h2>
<pre><code>cd images;for x in `ls`;do docker load -i $x;done
</code></pre>
<h2 id="安装脚本">安装脚本</h2>
<p>软件包下载地址：</p>
<pre><code>链接:https://pan.baidu.com/s/1Vw0geL4Pn9_cq0rLxw7dRQ  密码:3v4g
</code></pre>
<pre><code># hostname
#hostnamectl set-hostname $HOSTNAME

# disable swap
swapoff -a

# disable selinux
sed -i &quot;s/SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config
setenforce 0

# stop and disable firewalld
systemctl stop firewalld ; systemctl disable firewalld

# add ipvs modules
cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4

# change node kernel
cat &lt;&lt;EOF &gt;  /etc/sysctl.d/ceph.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system

# install rpm
cd /root/local/rpm ; yum localinstall ./*

# load images
systemctl start docker 
systemctl enable docker kubelet
cd /root/local/images ; for x in `ls`;do docker load -i $x;done

# install kubernetes
# kubeadm init --apiserver-advertise-address 172.16.10.101 --pod-network-cidr=10.244.0.0/16 --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers
echo &quot;172.16.10.101 api.k8s.com&quot; &gt;&gt; /etc/hosts
kubeadm init --config /root/local/kubeadm.conf

# install client env
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# install addons
cd /root/local/yaml; kubectl apply -f .

# remove taint
#kubectl  taint node lab1 node-role.kubernetes.io/master:NoSchedule-
kubectl  get nodes |grep master |awk '{print $1}' |xargs -i kubectl  taint node {} node-role.kubernetes.io/master:NoSchedule-
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[etcd数据备份和恢复]]></title>
        <id>https://js2hlu.coding-pages.com/post/2019090503/</id>
        <link href="https://js2hlu.coding-pages.com/post/2019090503/">
        </link>
        <updated>2019-09-05T03:46:23.000Z</updated>
        <content type="html"><![CDATA[<h2 id="对于etcd-api-v3数据备份与恢复方法">对于etcd api v3数据备份与恢复方法</h2>
<pre><code> # export ETCDCTL_API=3
</code></pre>
<pre><code> # etcdctl --endpoints localhost:2379 snapshot save snapshot.db （备份）
</code></pre>
<pre><code> # etcdctl snapshot restore snapshot.db --name m3 --data-dir=/home/etcd_data （还原）
</code></pre>
<blockquote>
<p>恢复后的文件需要修改权限为 etcd:etcd<br>
–name:重新指定一个数据目录，可以不指定，默认为 default.etcd<br>
–data-dir：指定数据目录<br>
建议使用时不指定 name 但指定 data-dir，并将 data-dir 对应于 etcd 服务中配置的 data-dir</p>
</blockquote>
<h2 id="实践方法">实践方法</h2>
<p><strong>单机备份</strong></p>
<pre><code>[root@k8s-master1 ~]# etcdctl --endpoints 127.0.0.1:2379 snapshot save snashot.db
Snapshot saved at snashot.db
[root@k8s-master1 ~]# ll
-rw-r--r--   1 root root 3756064 Apr 18 10:38 snashot.db
[root@k8s-master1 ~]#
</code></pre>
<p><strong>集群备份</strong></p>
<pre><code>[root@k8s-master1 ~]# etcdctl --endpoints=&quot;https://192.168.32.129:2379,https://192.168.32.130:2379,192.168.32.128:2379&quot; --cacert=/etc/kubernetes/cert/ca.pem --key=/etc/etcd/cert/etcd-key.pem --cert=/etc/etcd/cert/etcd.pem  snapshot save snashot1.db
Snapshot saved at snashot1.db
[root@k8s-master1 ~]#
[root@k8s-master1 ~]# ll
-rw-r--r--   1 root root 3756064 Apr 18 10:53 snashot1.db
-rw-r--r--   1 root root 3756064 Apr 18 10:38 snashot.db
</code></pre>
<p><strong>数据恢复</strong></p>
<p>做下面的操作,请慎重,有可能造成集群崩溃数据丢失.请在实验环境测试.</p>
<p>执行命令:systemctl stop etcd<br>
所有节点的etcd服务全部停止.</p>
<p>执行命令:rm -rf /var/lib/etcd/<br>
所有节点删除etcd的数据</p>
<p><strong>恢复v3的数据</strong></p>
<pre><code>[root@k8s-master1 ~]#  etcdctl --name=k8s-master1 --endpoints=&quot;https://192.168.32.128:2379&quot; --cacert=/etc/kubernetes/cert/ca.pem --key=/etc/etcd/cert/etcd-key.pem --cert=/etc/etcd/cert/etcd.pem --initial-cluster-token=etcd-cluster-1 --initial-advertise-peer-urls=https://192.168.32.128:2380 --initial-cluster=k8s-master1=https://192.168.32.128:2380,k8s-master2=https://192.168.32.129:2380,k8s-master3=https://192.168.32.130:2380 --data-dir=/var/lib/etcd snapshot restore snashot1.db
2019-04-18 13:43:42.570882 I | mvcc: restore compact to 148651
2019-04-18 13:43:42.584194 I | etcdserver/membership: added member 4c99f52323a3e391 [https://192.168.32.129:2380] to cluster 2a0978507970d828
2019-04-18 13:43:42.584224 I | etcdserver/membership: added member 5a74b01f28ece933 [https://192.168.32.128:2380] to cluster 2a0978507970d828
2019-04-18 13:43:42.584234 I | etcdserver/membership: added member b29b94ace458096d [https://192.168.32.130:2380] to cluster 2a0978507970d828
</code></pre>
<pre><code>[root@k8s-master2 ~]# etcdctl --name=k8s-master2 --endpoints=&quot;https://192.168.32.129:2379&quot; --cacert=/etc/kubernetes/cert/ca.pem --key=/etc/etcd/cert/etcd-key.pem --cert=/etc/etcd/cert/etcd.pem --initial-cluster-token=etcd-cluster-1 --initial-advertise-peer-urls=https://192.168.32.129:2380 --initial-cluster=k8s-master1=https://192.168.32.128:2380,k8s-master2=https://192.168.32.129:2380,k8s-master3=https://192.168.32.130:2380 --data-dir=/var/lib/etcd snapshot restore snashot1.db
2019-04-18 13:43:56.313096 I | mvcc: restore compact to 148651
2019-04-18 13:43:56.324779 I | etcdserver/membership: added member 4c99f52323a3e391 [https://192.168.32.129:2380] to cluster 2a0978507970d828
2019-04-18 13:43:56.324806 I | etcdserver/membership: added member 5a74b01f28ece933 [https://192.168.32.128:2380] to cluster 2a0978507970d828
2019-04-18 13:43:56.324819 I | etcdserver/membership: added member b29b94ace458096d [https://192.168.32.130:2380] to cluster 2a0978507970d828
</code></pre>
<pre><code>[root@k8s-master3 ~]# etcdctl --name=k8s-master3 --endpoints=&quot;https://192.168.32.130:2379&quot; --cacert=/etc/kubernetes/cert/ca.pem --key=/etc/etcd/cert/etcd-key.pem --cert=/etc/etcd/cert/etcd.pem --initial-cluster-token=etcd-cluster-1 --initial-advertise-peer-urls=https://192.168.32.130:2380 --initial-cluster=k8s-master1=https://192.168.32.128:2380,k8s-master2=https://192.168.32.129:2380,k8s-master3=https://192.168.32.130:2380 --data-dir=/var/lib/etcd snapshot restore snashot1.db
2019-04-18 13:44:10.643115 I | mvcc: restore compact to 148651
2019-04-18 13:44:10.649920 I | etcdserver/membership: added member 4c99f52323a3e391 [https://192.168.32.129:2380] to cluster 2a0978507970d828
2019-04-18 13:44:10.649957 I | etcdserver/membership: added member 5a74b01f28ece933 [https://192.168.32.128:2380] to cluster 2a0978507970d828
2019-04-18 13:44:10.649973 I | etcdserver/membership: added member b29b94ace458096d [https://192.168.32.130:2380] to cluster 2a0978507970d828
</code></pre>
<p>服务起不来</p>
<pre><code>[root@k8s-master1 ~]# tail -n 30 /var/log/messages
Apr 18 13:46:41 k8s-master1 systemd: Starting Etcd Server...
Apr 18 13:46:41 k8s-master1 etcd: etcd Version: 3.3.7
Apr 18 13:46:41 k8s-master1 etcd: Git SHA: 56536de55
Apr 18 13:46:41 k8s-master1 etcd: Go Version: go1.9.6
Apr 18 13:46:41 k8s-master1 etcd: Go OS/Arch: linux/amd64
Apr 18 13:46:41 k8s-master1 etcd: setting maximum number of CPUs to 1, total number of available CPUs is 1
Apr 18 13:46:41 k8s-master1 etcd: error listing data dir: /var/lib/etcd
Apr 18 13:46:41 k8s-master1 systemd: etcd.service: main process exited, code=exited, status=1/FAILURE
Apr 18 13:46:41 k8s-master1 systemd: Failed to start Etcd Server.
Apr 18 13:46:41 k8s-master1 systemd: Unit etcd.service entered failed state.
Apr 18 13:46:41 k8s-master1 systemd: etcd.service failed.
Apr 18 13:46:41 k8s-master1 flanneld: timed out
Apr 18 13:46:41 k8s-master1 flanneld: E0418 13:46:41.858283   63943 main.go:349] Couldn't fetch network config: client: etcd cluster is unavailable or misconfigured; error #0: EOF
Apr 18 13:46:41 k8s-master1 flanneld: ; error #1: EOF
Apr 18 13:46:41 k8s-master1 flanneld: ; error #2: EOF
[root@k8s-master1 ~]#
</code></pre>
<p>修改数据目录权限,默认是root:root<br>
chown -R etcd:etcd /var/lib/etcd<br>
恢复正常.</p>
<pre><code>[root@k8s-master1 ~]# etcdctl member list
4c99f52323a3e391, started, k8s-master2, https://192.168.32.129:2380, https://192.168.32.129:2379
5a74b01f28ece933, started, k8s-master1, https://192.168.32.128:2380, https://192.168.32.128:2379
b29b94ace458096d, started, k8s-master3, https://192.168.32.130:2380, https://192.168.32.130:2379
[root@k8s-master1 ~]#
</code></pre>
<p>可以看到v3的数据恢复成功.</p>
<h2 id="参考地址">参考地址</h2>
<p>https://blog.csdn.net/liukuan73/article/details/78986652</p>
<p>https://blog.51cto.com/goome/2380854</p>
<p>https://yq.aliyun.com/articles/336781</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Centos7下的日志切割]]></title>
        <id>https://js2hlu.coding-pages.com/post/2019090502/</id>
        <link href="https://js2hlu.coding-pages.com/post/2019090502/">
        </link>
        <updated>2019-09-05T03:27:49.000Z</updated>
        <content type="html"><![CDATA[<h2 id="logrotate">logrotate</h2>
<p>/etc/logrotate.conf 是 Logrotate 工具的一个配置文件，这个工具用来自动切割系统日志，Logrotate 是基于 cron 来运行的，如下：</p>
<pre><code>[root@localhost ~]$ cat /etc/cron.daily/logrotate    # 每天运行
#!/bin/sh

/usr/sbin/logrotate /etc/logrotate.conf &gt;/dev/null 2&gt;&amp;1
EXITVALUE=$?
if [ $EXITVALUE != 0 ]; then
    /usr/bin/logger -t logrotate &quot;ALERT exited abnormally with [$EXITVALUE]&quot;
fi
exit 0
</code></pre>
<p>实际运行时，Logrotate 会调用配置文件 /etc/logrotate.conf ，默认的配置如下：</p>
<pre><code>[root@localhost ~]$ cat /etc/logrotate.conf 

weekly                      # 每周切割一次
rotate 4                    # 只保留四份文件
create                      # 切割后会创建一个新的文件
dateext                     # 指定切割文件的后缀名，这里以日期为后缀名
include /etc/logrotate.d    # 包含其他配置文件的目录

/var/log/wtmp {             # 对哪个文件进行切割
    monthly                 # 每个月切割一次
    create 0664 root utmp   # 指定创建的新文件的权限，属主，属组
        minsize 1M          # 文件容量超过这个值时才进行切割
    rotate 1                # 只保留一份文件
}

/var/log/btmp {
    missingok
    monthly
    create 0600 root utmp
    rotate 1
}
</code></pre>
<h2 id="参考链接">参考链接</h2>
<p>https://blog.51cto.com/linuxblind/1269458</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes里面的GC]]></title>
        <id>https://js2hlu.coding-pages.com/post/2019090501/</id>
        <link href="https://js2hlu.coding-pages.com/post/2019090501/">
        </link>
        <updated>2019-09-05T03:02:08.000Z</updated>
        <content type="html"><![CDATA[<h2 id="什么是gc">什么是GC</h2>
<p>GC 是 Garbage Collector 的简称。从功能层面上来说，它和编程语言当中的「GC」 基本上是一样的。它清理 Kubernetes 中「符合特定条件」的 Resource Object。</p>
<p>Kubelet的GC功能将清理未使用的image和container。Kubelet每分钟对container执行一次GC，每5分钟对image执行一次GC。不建议使用外部垃圾收集工具，因为这些工具可能破坏Kubelet。</p>
<h2 id="kubernetes里面的基本常识">kubernetes里面的基本常识</h2>
<ul>
<li>在 k8s 中，你可以认为万物皆资源，很多逻辑的操作对象都是 Resource Object。</li>
<li>Kubernetes 在不同的 Resource Objects 中维护一定的「从属关系」。内置的 Resource Objects 一般会默认在一个 Resource Object 和它的创建者之间建立一个「从属关系」。</li>
<li>你也可以利用<code>ObjectMeta.OwnerReferences</code>自由的去给两个 Resource Object 建立关系，前提是被建立关系的两个对象必须在一个 Namespace 下。</li>
<li>K8s 实现了一种「Cascading deletion」（级联删除）的机制，它利用已经建立的「从属关系」进行资源对象的清理工作。例如，当一个 dependent 资源的 owner 已经被删除或者不存在的时候，从某种角度就可以判定，这个 dependent 的对象已经是异常（无人管辖）的了，需要进行清理。而 「cascading deletion」则是被 k8s 中的一个 controller 组件实现的：<code>Garbage Collector</code></li>
<li>k8s 是通过 <code>Garbage Collector</code> 和 <code>ownerReference</code> 一起配合实现了「垃圾回收」的功能。</li>
</ul>
<h2 id="kubernetes的gc组成">kubernetes的gc组成</h2>
<figure data-type="image" tabindex="1"><img src="http://img.blog.csdn.net/20161225115013410?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvV2FsdG9uV2FuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="kubernetes GC architecture in v1.3" loading="lazy"></figure>
<p>一个 Garbage Collector 通常由三部分实现：</p>
<ul>
<li>Scanner： 它负责收集目前系统中已存在的 Resource，并且周期性的将这些资源对象放入一个队列中，等待处理（检测是否要对某一个Resource Object 进行 GC 操作）</li>
<li>Garbage Processor: Garbage Processor 由两部分组成
<ul>
<li>
<p>Dirty Queue： Scanner 会将周期性扫描到的 Resource Object 放入这个队列中等待处理</p>
</li>
<li>
<p>Worker：worker 负责从这个队列中取出元素进行处理</p>
<ul>
<li>
<p>检查 Object 的 metaData 部分，查看<code>ownerReference</code>字段是否为空</p>
<ul>
<li>
<p>如果为空，则本次处理结束</p>
</li>
<li>
<p>如果不为空，检测<code>ownerReference</code>字段内标识的 Owner Resource Object是否存在</p>
<ul>
<li>存在：则本次处理结束</li>
<li>不存在：删除这个 Object</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Propagator： Propagator 由三个部分构成
<ul>
<li>
<p>EventQueue：负责存储 k8s 中资源对象的事件（Eg：ADD，UPDATE，DELETE）</p>
</li>
<li>
<p>DAG(有向无环图)：负责存储 k8s 中所有资源对象的「owner-dependent」 关系</p>
</li>
<li>
<p>Worker：从 EventQueue 中，取出资源对象的事件，根据事件的类型会采取以下两种操作</p>
<ul>
<li>ADD/UPDATE: 将该事件对应的资源对象加入 DAG，且如果该对象有 owner 且 owner 不在 DAG 中，将它同时加入 Garbage Processor 的 Dirty Queue 中</li>
<li>DELETE：将该事件对应的资源对象从 DAG 中删除，并且将其「管辖」的对象（只向下寻找一级，如删除 Deployment，那么只操作 ReplicaSet ）加入 Garbage Processor 的 Dirty Queue 中</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>其实，在有了 Scanner 和 Garbage Processor 之后，Garbage Collector 就已经能够实现「垃圾回收」的功能了。但是有一个明显的问题：Scanner 的扫描频率设置多少好呢？太长了，k8s 内部就会积累过多的「废弃资源」；太短了，尤其是在集群内部资源对象较多的时候，频繁的拉取信息对 API-Server 也是一个不小的压力。</p>
<p>k8s 作为一个分布式的服务编排系统，其内部执行任何一项逻辑或者行为，都依赖一种机制：「事件驱动」。说的简单点，k8s 中一些看起来「自动」的行为，其实都是由一些神秘的「力量」在驱动着。而这个「力量」就是我们所说的「Event」。任意一个 Resource Object 发生变动的时候（新建，更新，删除），都会触发一个 k8s 的事件（Event），这个事件在 k8s 的内部是公开的，也就是说，我们可以在任意一个地方监听这些事件。</p>
<p>总的来说，无论是「事件的监听机制」还是「周期性访问 API-Server 批量获取 Resource Object 信息」，其目的都是为了能够掌握 Resource Object 的最新信息。两者是各有优势的：</p>
<ol>
<li>批量拉取：一次性拉取所有的 Resource Object，全面</li>
<li>监听 Resource 的 Event：实时性强， 且对 API—SERVER 不会造成太大的压力</li>
</ol>
<p>综上所述，在实现 Garbage Collector 的过程中，k8s 向其添加了一个「增强型」的组件：Propagator</p>
<p>在有了 Propagator 的加入之后，我们完全可以仅在 GC 开始运行的时候，让 Scanner 扫描一下系统中所有的 Object，然后将这些信息传递给 Propagator 和 Dirty Queue。只要 DAG 一建立起来之后，那么 Scanner 其实就没有再工作的必要了。「事件驱动」的机制提供了一种增量的方式让 GC 来监控 k8s 集群内部的资源对象变化情况。</p>
<h2 id="参考地址">参考地址</h2>
<p>https://mp.weixin.qq.com/s/6b5jdDkvmtywvcRa4MMjQA</p>
<p>https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go</p>
<p>https://yq.aliyun.com/articles/679728</p>
<p>https://zhuanlan.zhihu.com/p/50101300</p>
]]></content>
    </entry>
</feed>